{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==1.13.3 -q\n",
    "# !pip install transformers==4.11.3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    ")\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.6.0].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrieval:\n",
    "  def __init__(self, args, dataset, num_neg, tokenizer, p_encoder, q_encoder):\n",
    "\n",
    "    self.args = args\n",
    "    self.dataset = dataset\n",
    "    self.num_neg = num_neg\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.p_encoder = p_encoder\n",
    "    self.q_encoder = q_encoder\n",
    "\n",
    "    self.prepare_in_batch_negative(num_neg=num_neg)\n",
    "\n",
    "  def prepare_in_batch_negative(self, dataset=None, num_neg=2, tokenizer=None):\n",
    "    if dataset is None:\n",
    "      dataset = self.dataset\n",
    "    \n",
    "    if tokenizer is None:\n",
    "      tokenizer = self.tokenizer\n",
    "    \n",
    "    corpus = np.array(list(set([example for example in dataset['context']])))\n",
    "    p_with_neg = []\n",
    "\n",
    "    for c in dataset['context']:\n",
    "      while True:\n",
    "        neg_idxs = np.random.randint(len(corpus), size=num_neg)\n",
    "\n",
    "        if c not in corpus[neg_idxs]:\n",
    "          p_neg = corpus[neg_idxs]\n",
    "\n",
    "          p_with_neg.append(c)\n",
    "          p_with_neg.extend(p_neg)\n",
    "          break\n",
    "\n",
    "    q_seqs = tokenizer(\n",
    "      dataset['question'],\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_tensors = 'pt',\n",
    "      return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    p_seqs = tokenizer(\n",
    "      p_with_neg,\n",
    "      padding = 'max_length',\n",
    "      truncation=True,\n",
    "      return_tensors = 'pt',\n",
    "      return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    # print('dataset[\"question\"]: ', dataset['question'])\n",
    "    # print('p_with_neg: ', p_with_neg)\n",
    "\n",
    "    # print('q_seqs: ', q_seqs)\n",
    "    # print('p_seqs: ', p_seqs)\n",
    "\n",
    "    max_len = p_seqs['input_ids'].size(-1)\n",
    "    p_seqs['input_ids'] = p_seqs['input_ids'].view(-1, num_neg+1, max_len)\n",
    "    p_seqs['attention_mask'] = p_seqs['attention_mask'].view(-1, num_neg+1, max_len)\n",
    "    # print(len(p_with_neg))\n",
    "    # print(p_seqs['input_ids'].shape)\n",
    "    # print(p_seqs['attention_mask'].shape)\n",
    "    # print(q_seqs['input_ids'].shape)\n",
    "    # print(q_seqs['attention_mask'].shape)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "      p_seqs['input_ids'], p_seqs['attention_mask'],\n",
    "      q_seqs['input_ids'], q_seqs['attention_mask']\n",
    "    )\n",
    "\n",
    "    self.train_dataloader = DataLoader(\n",
    "      train_dataset,\n",
    "      shuffle=True,\n",
    "      batch_size = self.args.per_device_train_batch_size\n",
    "    )\n",
    "\n",
    "    valid_seqs = tokenizer(\n",
    "      dataset['context'],\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    passage_dataset = TensorDataset(\n",
    "      valid_seqs['input_ids'],\n",
    "      valid_seqs['attention_mask']\n",
    "    )\n",
    "\n",
    "    self.passage_dataloader = DataLoader(\n",
    "      passage_dataset,\n",
    "      batch_size = self.args.per_device_train_batch_size\n",
    "    )\n",
    "\n",
    "  def train(self, args=None):\n",
    "    if args is None:\n",
    "      args = self.args\n",
    "    batch_size = args.per_device_train_batch_size\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "      {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "      {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "      {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "      {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "      optimizer_grouped_parameters,\n",
    "      lr=args.learning_rate,\n",
    "      eps=args.adam_epsilon\n",
    "    )\n",
    "    t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps = args.warmup_steps,\n",
    "      num_training_steps = t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    self.p_encoder.zero_grad()\n",
    "    self.q_encoder.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_iterator = tqdm(range(int(args.num_train_epochs)), desc='Epoch')\n",
    "\n",
    "    for _ in train_iterator:\n",
    "\n",
    "      with tqdm(self.train_dataloader, unit='batch') as tepoch:\n",
    "        for batch in tepoch:\n",
    "          self.p_encoder.train()\n",
    "          self.q_encoder.train()\n",
    "\n",
    "          targets = torch.zeros(batch_size).long()\n",
    "          targets = targets.to(args.device)\n",
    "\n",
    "          print('targets: ', targets)\n",
    "          \n",
    "\n",
    "          p_inputs = {\n",
    "            \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "            \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "          }\n",
    "\n",
    "          q_inputs = {\n",
    "            \"input_ids\": batch[2].to(args.device),\n",
    "            \"attention_mask\": batch[3].to(args.device),\n",
    "          }\n",
    "\n",
    "          print('p_inputs: ', p_inputs)\n",
    "          print('q_inputs: ', q_inputs)\n",
    "\n",
    "          p_outputs = self.p_encoder(**p_inputs)\n",
    "          q_outputs = self.q_encoder(**q_inputs)\n",
    "\n",
    "          print('q_outputs.shape before: ', q_outputs.shape)\n",
    "          print('p_outputs.shape before: ', p_outputs.shape)\n",
    "\n",
    "          p_outputs = p_outputs.view(batch_size, -1, self.num_neg+1)\n",
    "          q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "          print('targets.shape: ', targets.shape)\n",
    "          print('q_outputs.shape: ', q_outputs.shape)\n",
    "          print('p_outputs.shape: ', p_outputs.shape)\n",
    "\n",
    "          sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()\n",
    "          print('sim_scores1: ',sim_scores)\n",
    "          sim_scores = sim_scores.view(batch_size, -1)\n",
    "          print('sim_scores2: ',sim_scores)\n",
    "          sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "          print('sim_scores3: ',sim_scores)\n",
    "\n",
    "          loss = F.nll_loss(sim_scores, targets)\n",
    "          tepoch.set_postfix(loss=f'{str(loss.item())}')\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "\n",
    "          self.p_encoder.zero_grad()\n",
    "          self.q_encoder.zero_grad()\n",
    "\n",
    "          global_step += 1\n",
    "\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "          del p_inputs, q_inputs\n",
    "\n",
    "        print('loss:', loss)\n",
    "        print('sim_scores:', sim_scores)\n",
    "        print('targets:', targets)\n",
    "        print('p_outputs:', p_outputs)\n",
    "        print('q_outputs:', q_outputs)\n",
    "\n",
    "  def get_relevant_doc(self, query, k=1, args=None, p_encoder=None, q_encoder=None):\n",
    "    if args is None:\n",
    "      args = self.args\n",
    "    \n",
    "    if p_encoder is None:\n",
    "      p_encoder = self.p_encoder\n",
    "    \n",
    "    if q_encoder is None:\n",
    "      q_encoder = self.q_encoder\n",
    "\n",
    "    with torch.no_grad():\n",
    "      p_encoder.eval()\n",
    "      q_encoder.eval()\n",
    "\n",
    "      q_seqs_val = self.tokenizer(\n",
    "        [query],\n",
    "        padding = 'max_length',\n",
    "        truncation= True,\n",
    "        return_tensors = 'pt',\n",
    "        return_token_type_ids=False,\n",
    "      ).to(args.device)\n",
    "      q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "\n",
    "      p_embs = []\n",
    "      for batch in self.passage_dataloader:\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        p_inputs = {\n",
    "          'input_ids': batch[0],\n",
    "          'attention_mask': batch[1]\n",
    "        }\n",
    "        p_emb = p_encoder(**p_inputs).to('cpu')\n",
    "        p_embs.append(p_emb)\n",
    "    \n",
    "    p_embs = torch.stack(\n",
    "      p_embs, dim = 0\n",
    "    ).view(len(self.passage_dataloader.dataset), -1)\n",
    "\n",
    "    dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "    rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "    return rank[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(RobertaPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.roberta = RobertaModel(config)\n",
    "    self.init_weights()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask=None):\n",
    "    outputs = self.roberta(\n",
    "      input_ids,\n",
    "      attention_mask = attention_mask\n",
    "    )\n",
    "\n",
    "    pooled_output = outputs[1]\n",
    "    return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79010ae3cf9474e9687bb3dab5f8525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbc99b76237483095e1562b0dda5bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/734k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaEncoder: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaEncoder were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaEncoder: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaEncoder were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_from_disk('../../data/train_dataset')['train']\n",
    "\n",
    "# num_sample = 1500\n",
    "# sample_idx = np.random.choice(range(len(train_dataset)),100)# len(train_dataset))#num_sample)\n",
    "sample_idx = np.random.choice(range(len(train_dataset)), len(train_dataset))#num_sample)\n",
    "train_dataset = train_dataset[sample_idx]\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = 'dense_retrieval',\n",
    "  evaluation_strategy = 'epoch',\n",
    "  learning_rate=3e-4,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=4,\n",
    "  num_train_epochs=20,\n",
    "  weight_decay=0.01\n",
    ")\n",
    "\n",
    "model_checkpoint = 'klue/roberta-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DenseRetrieval(\n",
    "  args=args,\n",
    "  dataset=train_dataset,\n",
    "  # num_neg=12,\n",
    "  num_neg=5,\n",
    "  tokenizer=tokenizer,\n",
    "  p_encoder=p_encoder,\n",
    "  q_encoder=q_encoder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a921224cbc7845dbbca772d0cc313f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe61c4eb8104c6da8b3b2292a6588d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 20561,  2181,  ...,     1,     1,     1],\n",
      "        [    0,  1469,  2251,  ...,     1,     1,     1],\n",
      "        [    0, 24612,  4472,  ...,  3839,  2470,     2],\n",
      "        ...,\n",
      "        [    0,  1878,  2067,  ...,     1,     1,     1],\n",
      "        [    0,  4305,  2170,  ...,    13,  1497,     2],\n",
      "        [    0,    27,  2429,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 12646,  2116,  ...,     1,     1,     1],\n",
      "        [    0,  9310,   837,  ...,     1,     1,     1],\n",
      "        [    0,  3890,  2052,  ...,     1,     1,     1],\n",
      "        [    0,  1381,  2479,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 0.7931, -0.4471, -2.9468,  2.2195, -0.7349, -0.1452],\n",
      "        [ 0.5339, -0.4004, -3.2641,  1.6185, -1.1767, -1.0744],\n",
      "        [ 1.1726, -1.1298, -3.0667,  1.1194, -0.6933, -0.1711],\n",
      "        [ 1.5221,  0.1390, -2.4593,  1.8634,  0.1788, -0.9123]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 0.7931, -0.4471, -2.9468,  2.2195, -0.7349, -0.1452],\n",
      "        [ 0.5339, -0.4004, -3.2641,  1.6185, -1.1767, -1.0744],\n",
      "        [ 1.1726, -1.1298, -3.0667,  1.1194, -0.6933, -0.1711],\n",
      "        [ 1.5221,  0.1390, -2.4593,  1.8634,  0.1788, -0.9123]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.8059, -3.0461, -5.5457, -0.3794, -3.3339, -2.7441],\n",
      "        [-1.5591, -2.4934, -5.3571, -0.4745, -3.2697, -3.1674],\n",
      "        [-0.9076, -3.2100, -5.1468, -0.9607, -2.7735, -2.2513],\n",
      "        [-1.1068, -2.4900, -5.0882, -0.7656, -2.4502, -3.5413]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 13327,  2258,  ...,     1,     1,     1],\n",
      "        [    0,  3633, 14223,  ...,     1,     1,     1],\n",
      "        [    0, 25551,  3698,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 12168, 13045,  ...,     1,     1,     1],\n",
      "        [    0,    30, 15321,  ...,     1,     1,     1],\n",
      "        [    0,  1432,  2331,  ...,  1504,  2259,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 13327,  2258,  ...,     1,     1,     1],\n",
      "        [    0,  6500,  2116,  ...,     1,     1,     1],\n",
      "        [    0,  1184, 16139,  ...,     1,     1,     1],\n",
      "        [    0,  3651,  2777,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[11.7435, -4.5082,  1.9364, -5.5649,  2.7281, -0.3681],\n",
      "        [11.5623, -3.3179,  0.2502, -5.1343,  2.4328, -0.1287],\n",
      "        [11.3961, -3.1229, -0.0263, -5.0777,  3.4223, -1.4219],\n",
      "        [12.0610, -3.2024, -0.9917, -6.6255,  1.7470, -1.0284]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[11.7435, -4.5082,  1.9364, -5.5649,  2.7281, -0.3681],\n",
      "        [11.5623, -3.3179,  0.2502, -5.1343,  2.4328, -0.1287],\n",
      "        [11.3961, -3.1229, -0.0263, -5.0777,  3.4223, -1.4219],\n",
      "        [12.0610, -3.2024, -0.9917, -6.6255,  1.7470, -1.0284]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.8214e-04, -1.6252e+01, -9.8073e+00, -1.7309e+01, -9.0156e+00,\n",
      "         -1.2112e+01],\n",
      "        [-1.2945e-04, -1.4880e+01, -1.1312e+01, -1.6697e+01, -9.1297e+00,\n",
      "         -1.1691e+01],\n",
      "        [-3.5852e-04, -1.4519e+01, -1.1423e+01, -1.6474e+01, -7.9742e+00,\n",
      "         -1.2818e+01],\n",
      "        [-3.7550e-05, -1.5263e+01, -1.3053e+01, -1.8686e+01, -1.0314e+01,\n",
      "         -1.3089e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 23072,  2104,  ...,     1,     1,     1],\n",
      "        [    0, 26496,  6977,  ...,     1,     1,     1],\n",
      "        [    0,    11, 11272,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 10605,  2069,  ...,     1,     1,     1],\n",
      "        [    0,  3665,  2079,  ...,     1,     1,     1],\n",
      "        [    0,  5125,  2440,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 23072,  2104,  ...,     1,     1,     1],\n",
      "        [    0,  8348,  2440,  ...,     1,     1,     1],\n",
      "        [    0, 16096,  2349,  ...,     1,     1,     1],\n",
      "        [    0, 12769,  2522,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[13.0644, -2.7059, -0.4762, -6.1067,  2.7869, -3.2185],\n",
      "        [12.2822, -3.6410, -0.5352, -5.6640,  2.4809, -0.5617],\n",
      "        [12.2918, -2.3167, -0.5647, -6.6361,  2.6047, -1.2628],\n",
      "        [10.2860, -3.6168,  0.2653, -5.7800,  2.6498, -0.3906]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[13.0644, -2.7059, -0.4762, -6.1067,  2.7869, -3.2185],\n",
      "        [12.2822, -3.6410, -0.5352, -5.6640,  2.4809, -0.5617],\n",
      "        [12.2918, -2.3167, -0.5647, -6.6361,  2.6047, -1.2628],\n",
      "        [10.2860, -3.6168,  0.2653, -5.7800,  2.6498, -0.3906]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-3.6001e-05, -1.5770e+01, -1.3541e+01, -1.9171e+01, -1.0278e+01,\n",
      "         -1.6283e+01],\n",
      "        [-6.0914e-05, -1.5923e+01, -1.2817e+01, -1.7946e+01, -9.8014e+00,\n",
      "         -1.2844e+01],\n",
      "        [-6.6517e-05, -1.4609e+01, -1.2857e+01, -1.8928e+01, -9.6871e+00,\n",
      "         -1.3555e+01],\n",
      "        [-5.5107e-04, -1.3903e+01, -1.0021e+01, -1.6067e+01, -7.6368e+00,\n",
      "         -1.0677e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1331,  2091,  ...,     1,     1,     1],\n",
      "        [    0,  4612,  2207,  ...,     1,     1,     1],\n",
      "        [    0, 21343,  2259,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  3926,  2171,  ...,    16,  3661,     2],\n",
      "        [    0,  5170,  4038,  ...,     1,     1,     1],\n",
      "        [    0,  1121,  2251,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[   0,  823, 2584,  ...,    1,    1,    1],\n",
      "        [   0, 1869, 7285,  ...,    1,    1,    1],\n",
      "        [   0, 1086, 2600,  ...,    1,    1,    1],\n",
      "        [   0, 3611, 2031,  ...,    1,    1,    1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[12.5273, -2.1943, -0.2148, -6.9590,  1.0521, -2.6993],\n",
      "        [13.6718, -1.8796, -0.2239, -6.0730,  2.8535, -2.0948],\n",
      "        [14.5195, -3.6659, -0.8289, -7.0253,  0.2675, -1.3581],\n",
      "        [12.4934, -2.1372, -0.7681, -5.2488,  0.8603, -1.5498]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[12.5273, -2.1943, -0.2148, -6.9590,  1.0521, -2.6993],\n",
      "        [13.6718, -1.8796, -0.2239, -6.0730,  2.8535, -2.0948],\n",
      "        [14.5195, -3.6659, -0.8289, -7.0253,  0.2675, -1.3581],\n",
      "        [12.4934, -2.1372, -0.7681, -5.2488,  0.8603, -1.5498]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.3947e-05, -1.4722e+01, -1.2742e+01, -1.9486e+01, -1.1475e+01,\n",
      "         -1.5227e+01],\n",
      "        [-2.1338e-05, -1.5551e+01, -1.3896e+01, -1.9745e+01, -1.0818e+01,\n",
      "         -1.5767e+01],\n",
      "        [-9.5367e-07, -1.8185e+01, -1.5348e+01, -2.1545e+01, -1.4252e+01,\n",
      "         -1.5878e+01],\n",
      "        [-1.1921e-05, -1.4631e+01, -1.3261e+01, -1.7742e+01, -1.1633e+01,\n",
      "         -1.4043e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1545,  2083,  ..., 13774,  2116,     2],\n",
      "        [    0,  7050, 26271,  ..., 27110,  1028,     2],\n",
      "        [    0,  3870,  2090,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 21384,  8047,  ...,  4792, 20195,     2],\n",
      "        [    0,  5004, 27379,  ...,     1,     1,     1],\n",
      "        [    0,  5994,  2440,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  4353,  2073,  ...,     1,     1,     1],\n",
      "        [    0,  7484,  3742,  ...,     1,     1,     1],\n",
      "        [    0,  1183, 11483,  ...,     1,     1,     1],\n",
      "        [    0,  6276,  9503,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[15.6810, -1.8039, -1.5269, -6.4763,  1.9032, -2.5531],\n",
      "        [14.7409, -2.3014, -1.9119, -6.9206, -0.1001, -2.5284],\n",
      "        [13.9754, -0.5853, -1.1293, -5.8704,  0.2536, -1.6627],\n",
      "        [15.4389, -2.0916, -1.5281, -6.7572, -0.4933, -2.1410]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[15.6810, -1.8039, -1.5269, -6.4763,  1.9032, -2.5531],\n",
      "        [14.7409, -2.3014, -1.9119, -6.9206, -0.1001, -2.5284],\n",
      "        [13.9754, -0.5853, -1.1293, -5.8704,  0.2536, -1.6627],\n",
      "        [15.4389, -2.0916, -1.5281, -6.7572, -0.4933, -2.1410]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.0729e-06, -1.7485e+01, -1.7208e+01, -2.2157e+01, -1.3778e+01,\n",
      "         -1.8234e+01],\n",
      "        [-4.7684e-07, -1.7042e+01, -1.6653e+01, -2.1662e+01, -1.4841e+01,\n",
      "         -1.7269e+01],\n",
      "        [-1.9073e-06, -1.4561e+01, -1.5105e+01, -1.9846e+01, -1.3722e+01,\n",
      "         -1.5638e+01],\n",
      "        [-1.1921e-07, -1.7531e+01, -1.6967e+01, -2.2196e+01, -1.5932e+01,\n",
      "         -1.7580e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1078,  2466,  ...,  1537,  2170,     2],\n",
      "        [    0,  3790,    16,  ...,    81,     3,     2],\n",
      "        [    0,  3696,  1504,  ...,  2138,  7298,     2],\n",
      "        ...,\n",
      "        [    0, 23305, 12377,  ...,     1,     1,     1],\n",
      "        [    0,   636,  2259,  ...,     1,     1,     1],\n",
      "        [    0, 13880,  2082,  ...,  2103,  2259,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  1078,  2466,  ...,     1,     1,     1],\n",
      "        [    0, 17612,  2259,  ...,     1,     1,     1],\n",
      "        [    0,  7919,   752,  ...,     1,     1,     1],\n",
      "        [    0,  4364,    24,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[15.2742, -1.8074, -2.4286, -6.8653, -0.4137, -3.5585],\n",
      "        [15.4665, -1.7612, -2.6407, -6.5746, -0.6688, -3.1420],\n",
      "        [14.8304, -1.5117, -3.9621, -7.6207, -1.4379, -3.0273],\n",
      "        [16.4465, -1.0924, -1.2304, -6.8445, -1.2311, -2.0509]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[15.2742, -1.8074, -2.4286, -6.8653, -0.4137, -3.5585],\n",
      "        [15.4665, -1.7612, -2.6407, -6.5746, -0.6688, -3.1420],\n",
      "        [14.8304, -1.5117, -3.9621, -7.6207, -1.4379, -3.0273],\n",
      "        [16.4465, -1.0924, -1.2304, -6.8445, -1.2311, -2.0509]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.1921e-07, -1.7082e+01, -1.7703e+01, -2.2140e+01, -1.5688e+01,\n",
      "         -1.8833e+01],\n",
      "        [-1.1921e-07, -1.7228e+01, -1.8107e+01, -2.2041e+01, -1.6135e+01,\n",
      "         -1.8608e+01],\n",
      "        [-2.3842e-07, -1.6342e+01, -1.8793e+01, -2.2451e+01, -1.6268e+01,\n",
      "         -1.7858e+01],\n",
      "        [ 0.0000e+00, -1.7539e+01, -1.7677e+01, -2.3291e+01, -1.7678e+01,\n",
      "         -1.8497e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,   636,  2079,  ...,  2391,   887,     2],\n",
      "        [    0, 10415,  2504,  ...,     1,     1,     1],\n",
      "        [    0, 15854, 13011,  ...,  2052, 19568,     2],\n",
      "        ...,\n",
      "        [    0,  7711,  5485,  ...,  2079,  4152,     2],\n",
      "        [    0,  1378,  2327,  ...,     1,     1,     1],\n",
      "        [    0,  4470,  2440,  ...,  3719,  6509,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  7790, 28330,  ...,     1,     1,     1],\n",
      "        [    0,  1915,  2942,  ...,     1,     1,     1],\n",
      "        [    0,  4838,  2444,  ...,     1,     1,     1],\n",
      "        [    0,  1459,  2130,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[15.0249, -1.0622, -3.5579, -6.4416, -1.6226, -2.8179],\n",
      "        [16.8676, -2.7526, -3.2899, -8.3184, -1.2472, -2.1282],\n",
      "        [11.8401, -0.6425, -2.2891, -4.6571, -2.7506, -2.9776],\n",
      "        [14.7299, -1.6042, -3.0551, -5.4148, -1.6152, -3.4837]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[15.0249, -1.0622, -3.5579, -6.4416, -1.6226, -2.8179],\n",
      "        [16.8676, -2.7526, -3.2899, -8.3184, -1.2472, -2.1282],\n",
      "        [11.8401, -0.6425, -2.2891, -4.6571, -2.7506, -2.9776],\n",
      "        [14.7299, -1.6042, -3.0551, -5.4148, -1.6152, -3.4837]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.1921e-07, -1.6087e+01, -1.8583e+01, -2.1467e+01, -1.6647e+01,\n",
      "         -1.7843e+01],\n",
      "        [ 0.0000e+00, -1.9620e+01, -2.0157e+01, -2.5186e+01, -1.8115e+01,\n",
      "         -1.8996e+01],\n",
      "        [-5.3644e-06, -1.2483e+01, -1.4129e+01, -1.6497e+01, -1.4591e+01,\n",
      "         -1.4818e+01],\n",
      "        [-2.3842e-07, -1.6334e+01, -1.7785e+01, -2.0145e+01, -1.6345e+01,\n",
      "         -1.8214e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  7907,  2196,  ...,  2259,  1517,     2],\n",
      "        [    0, 26144,  5260,  ...,     1,     1,     1],\n",
      "        [    0,   801,  2051,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  4892, 27135,  ...,  2079, 12882,     2],\n",
      "        [    0, 26891,  2440,  ...,  2259, 11942,     2],\n",
      "        [    0, 29670,  2079,  ...,     3,    13,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 21023,  2069,  ...,     1,     1,     1],\n",
      "        [    0,  9190,  2440,  ...,     1,     1,     1],\n",
      "        [    0,  1583,  2055,  ...,     1,     1,     1],\n",
      "        [    0,  7273,  2073,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[15.8544, -1.0161, -4.0880, -5.9259, -2.5920, -1.0606],\n",
      "        [15.7945, -0.7228, -2.6205, -6.7507, -3.1616, -2.3924],\n",
      "        [15.4440, -1.9116, -4.1749, -6.8044, -2.4618, -2.7091],\n",
      "        [16.1023, -0.9993, -3.4199, -6.0596, -2.3301, -3.2667]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[15.8544, -1.0161, -4.0880, -5.9259, -2.5920, -1.0606],\n",
      "        [15.7945, -0.7228, -2.6205, -6.7507, -3.1616, -2.3924],\n",
      "        [15.4440, -1.9116, -4.1749, -6.8044, -2.4618, -2.7091],\n",
      "        [16.1023, -0.9993, -3.4199, -6.0596, -2.3301, -3.2667]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-1.1921e-07, -1.6870e+01, -1.9942e+01, -2.1780e+01, -1.8446e+01,\n",
      "         -1.6915e+01],\n",
      "        [-1.1921e-07, -1.6517e+01, -1.8415e+01, -2.2545e+01, -1.8956e+01,\n",
      "         -1.8187e+01],\n",
      "        [ 0.0000e+00, -1.7356e+01, -1.9619e+01, -2.2248e+01, -1.7906e+01,\n",
      "         -1.8153e+01],\n",
      "        [ 0.0000e+00, -1.7102e+01, -1.9522e+01, -2.2162e+01, -1.8432e+01,\n",
      "         -1.9369e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 12270,  2403,  ...,     1,     1,     1],\n",
      "        [    0, 26023,  2259,  ...,     1,     1,     1],\n",
      "        [    0,  3718, 14223,  ...,    81,  2052,     2],\n",
      "        ...,\n",
      "        [    0, 21405,  4926,  ...,  2170, 28467,     2],\n",
      "        [    0,  3811,  2052,  ...,     1,     1,     1],\n",
      "        [    0,     6, 24260,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 12270,  2403,  ...,     1,     1,     1],\n",
      "        [    0, 17538, 22691,  ...,     1,     1,     1],\n",
      "        [    0, 28928,  2116,  ...,     1,     1,     1],\n",
      "        [    0, 26051,  2255,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[16.6815, -0.9053, -4.3553, -5.7580, -3.6615, -1.9814],\n",
      "        [16.6662, -1.9685, -5.1765, -7.0440, -3.5584, -2.5975],\n",
      "        [17.5010, -1.9835, -4.8615, -7.0332, -3.4234, -1.6063],\n",
      "        [18.1375, -1.4077, -4.8586, -7.7086, -4.8747, -3.8715]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[16.6815, -0.9053, -4.3553, -5.7580, -3.6615, -1.9814],\n",
      "        [16.6662, -1.9685, -5.1765, -7.0440, -3.5584, -2.5975],\n",
      "        [17.5010, -1.9835, -4.8615, -7.0332, -3.4234, -1.6063],\n",
      "        [18.1375, -1.4077, -4.8586, -7.7086, -4.8747, -3.8715]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -17.5868, -21.0368, -22.4395, -20.3430, -18.6629],\n",
      "        [  0.0000, -18.6347, -21.8427, -23.7101, -20.2246, -19.2636],\n",
      "        [  0.0000, -19.4845, -22.3625, -24.5342, -20.9244, -19.1073],\n",
      "        [  0.0000, -19.5452, -22.9961, -25.8461, -23.0122, -22.0090]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  5993,  5013,  ...,     1,     1,     1],\n",
      "        [    0,    32, 14221,  ...,     1,     1,     1],\n",
      "        [    0, 22660,  2326,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  5635,  2259,  ...,     1,     1,     1],\n",
      "        [    0,  4612,  6500,  ...,     1,     1,     1],\n",
      "        [    0,  5415,  2318,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 14834,  3929,  ...,     1,     1,     1],\n",
      "        [    0,  1793,  2181,  ...,     1,     1,     1],\n",
      "        [    0,  3629,  4171,  ...,     1,     1,     1],\n",
      "        [    0,  6362,  2104,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[18.7077, -2.0023, -3.9899, -7.3562, -4.2023, -3.1441],\n",
      "        [16.1859, -2.8718, -4.6172, -7.3554, -3.2244, -2.0511],\n",
      "        [15.9756, -1.7271, -3.7922, -6.7802, -3.5073, -2.0914],\n",
      "        [18.0464, -1.6342, -5.2371, -7.5303, -4.3158, -2.5897]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[18.7077, -2.0023, -3.9899, -7.3562, -4.2023, -3.1441],\n",
      "        [16.1859, -2.8718, -4.6172, -7.3554, -3.2244, -2.0511],\n",
      "        [15.9756, -1.7271, -3.7922, -6.7802, -3.5073, -2.0914],\n",
      "        [18.0464, -1.6342, -5.2371, -7.5303, -4.3158, -2.5897]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -20.7099, -22.6975, -26.0639, -22.9099, -21.8518],\n",
      "        [  0.0000, -19.0577, -20.8031, -23.5413, -19.4103, -18.2370],\n",
      "        [  0.0000, -17.7027, -19.7678, -22.7558, -19.4829, -18.0669],\n",
      "        [  0.0000, -19.6805, -23.2834, -25.5767, -22.3622, -20.6360]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 12570,  2128,  ...,     1,     1,     1],\n",
      "        [    0,  1235,  2093,  ...,     1,     1,     1],\n",
      "        [    0,  9936,  2050,  ..., 18952,  2012,     2],\n",
      "        ...,\n",
      "        [    0,  4510,  2079,  ...,     1,     1,     1],\n",
      "        [    0,    30,  9119,  ...,     1,     1,     1],\n",
      "        [    0,    32, 14221,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  3647, 10363,  ...,     1,     1,     1],\n",
      "        [    0,  5558,  2170,  ...,     1,     1,     1],\n",
      "        [    0,  5868, 27135,  ...,     1,     1,     1],\n",
      "        [    0, 11633,  2225,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[20.0091, -2.5349, -4.5624, -9.0804, -6.1759, -3.8086],\n",
      "        [14.6892, -2.3957, -5.1774, -6.5777, -4.7279, -1.2006],\n",
      "        [18.8974, -2.7754, -6.0993, -7.5004, -5.9628, -2.9190],\n",
      "        [18.2863, -1.4803, -6.1809, -8.4863, -4.6952, -2.2270]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[20.0091, -2.5349, -4.5624, -9.0804, -6.1759, -3.8086],\n",
      "        [14.6892, -2.3957, -5.1774, -6.5777, -4.7279, -1.2006],\n",
      "        [18.8974, -2.7754, -6.0993, -7.5004, -5.9628, -2.9190],\n",
      "        [18.2863, -1.4803, -6.1809, -8.4863, -4.6952, -2.2270]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[ 0.0000e+00, -2.2544e+01, -2.4571e+01, -2.9089e+01, -2.6185e+01,\n",
      "         -2.3818e+01],\n",
      "        [-1.1921e-07, -1.7085e+01, -1.9867e+01, -2.1267e+01, -1.9417e+01,\n",
      "         -1.5890e+01],\n",
      "        [ 0.0000e+00, -2.1673e+01, -2.4997e+01, -2.6398e+01, -2.4860e+01,\n",
      "         -2.1816e+01],\n",
      "        [ 0.0000e+00, -1.9767e+01, -2.4467e+01, -2.6773e+01, -2.2982e+01,\n",
      "         -2.0513e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 11310,  2248,  ...,     1,     1,     1],\n",
      "        [    0,     3, 22313,  ...,     1,     1,     1],\n",
      "        [    0, 22542,  2259,  ...,  2173,   647,     2],\n",
      "        ...,\n",
      "        [    0,  9175,  2440,  ...,     1,     1,     1],\n",
      "        [    0,  4517,  2897,  ...,  2600,  2189,     2],\n",
      "        [    0,  3932, 14223,  ...,  2069,  6936,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 23580,  1276,  ...,     1,     1,     1],\n",
      "        [    0,  7066, 17704,  ...,     1,     1,     1],\n",
      "        [    0,     3,   723,  ...,     1,     1,     1],\n",
      "        [    0,  4989, 27135,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[17.7060, -3.2633, -6.3940, -8.1802, -5.1761, -2.2728],\n",
      "        [18.9716, -1.4860, -5.4380, -6.6613, -5.0857, -2.6550],\n",
      "        [19.4300, -1.1989, -5.0454, -7.7155, -7.0822, -3.1446],\n",
      "        [17.8546, -0.9597, -5.5787, -7.5988, -4.4780, -3.3248]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[17.7060, -3.2633, -6.3940, -8.1802, -5.1761, -2.2728],\n",
      "        [18.9716, -1.4860, -5.4380, -6.6613, -5.0857, -2.6550],\n",
      "        [19.4300, -1.1989, -5.0454, -7.7155, -7.0822, -3.1446],\n",
      "        [17.8546, -0.9597, -5.5787, -7.5988, -4.4780, -3.3248]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -20.9693, -24.1001, -25.8862, -22.8822, -19.9789],\n",
      "        [  0.0000, -20.4576, -24.4096, -25.6330, -24.0573, -21.6267],\n",
      "        [  0.0000, -20.6289, -24.4754, -27.1455, -26.5122, -22.5746],\n",
      "        [  0.0000, -18.8142, -23.4333, -25.4533, -22.3326, -21.1794]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 15918,  2116,  ...,  2259,  4312,     2],\n",
      "        [    0,   133,  5267,  ...,  2051,  2523,     2],\n",
      "        [    0, 15401,  2082,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  7756,  2440,  ...,     1,     1,     1],\n",
      "        [    0, 14192,  2151,  ...,  2517,  2016,     2],\n",
      "        [    0, 10561,  2119,  ...,   766,  2104,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  6281, 16066,  ...,     1,     1,     1],\n",
      "        [    0,  6631, 14426,  ...,     1,     1,     1],\n",
      "        [    0,  7311,  2138,  ...,     1,     1,     1],\n",
      "        [    0,  9703,  2444,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[16.6074, -1.6597, -6.1120, -7.2817, -3.3468, -3.0830],\n",
      "        [22.0486, -1.8298, -7.3646, -8.9906, -5.4386, -4.0899],\n",
      "        [17.8658, -1.8741, -7.1653, -7.9686, -6.2772, -3.1763],\n",
      "        [19.0315, -1.8872, -7.7905, -9.0613, -5.3486, -2.6237]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[16.6074, -1.6597, -6.1120, -7.2817, -3.3468, -3.0830],\n",
      "        [22.0486, -1.8298, -7.3646, -8.9906, -5.4386, -4.0899],\n",
      "        [17.8658, -1.8741, -7.1653, -7.9686, -6.2772, -3.1763],\n",
      "        [19.0315, -1.8872, -7.7905, -9.0613, -5.3486, -2.6237]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -18.2672, -22.7195, -23.8891, -19.9542, -19.6905],\n",
      "        [  0.0000, -23.8784, -29.4132, -31.0392, -27.4872, -26.1385],\n",
      "        [  0.0000, -19.7399, -25.0311, -25.8344, -24.1430, -21.0422],\n",
      "        [  0.0000, -20.9187, -26.8219, -28.0928, -24.3801, -21.6552]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 16716,  2259,  ...,     1,     1,     1],\n",
      "        [    0, 13298,  2440,  ...,     1,     1,     1],\n",
      "        [    0,  4510, 17704,  ...,  2259,  3768,     2],\n",
      "        ...,\n",
      "        [    0, 23315,  2440,  ...,     1,     1,     1],\n",
      "        [    0, 22042,  3759,  ...,     1,     1,     1],\n",
      "        [    0, 11728,  2870,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,   856,  2310,  ...,     1,     1,     1],\n",
      "        [    0,  5211, 27135,  ...,     1,     1,     1],\n",
      "        [    0,  4655,  2079,  ...,     1,     1,     1],\n",
      "        [    0,  7414,  3195,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 19.9264,  -0.3157,  -5.8271,  -8.3207,  -7.0043,  -3.8801],\n",
      "        [ 21.2087,  -2.3803,  -7.1263, -10.0101,  -8.3986,  -5.1355],\n",
      "        [ 19.0919,  -1.0634,  -7.2079,  -7.8386,  -7.3001,  -3.2646],\n",
      "        [ 20.3895,  -1.6961,  -6.3321,  -8.0700,  -6.0183,  -2.9814]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.9264,  -0.3157,  -5.8271,  -8.3207,  -7.0043,  -3.8801],\n",
      "        [ 21.2087,  -2.3803,  -7.1263, -10.0101,  -8.3986,  -5.1355],\n",
      "        [ 19.0919,  -1.0634,  -7.2079,  -7.8386,  -7.3001,  -3.2646],\n",
      "        [ 20.3895,  -1.6961,  -6.3321,  -8.0700,  -6.0183,  -2.9814]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -20.2421, -25.7535, -28.2471, -26.9308, -23.8065],\n",
      "        [  0.0000, -23.5890, -28.3350, -31.2189, -29.6074, -26.3443],\n",
      "        [  0.0000, -20.1553, -26.2998, -26.9305, -26.3920, -22.3565],\n",
      "        [  0.0000, -22.0855, -26.7216, -28.4594, -26.4078, -23.3709]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1882,  2634,  ...,  2205, 13807,     2],\n",
      "        [    0,  4619,  4104,  ...,     1,     1,     1],\n",
      "        [    0, 18199,  2440,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 10009,  4977,  ...,     1,     1,     1],\n",
      "        [    0,  5852,  2905,  ...,     1,     1,     1],\n",
      "        [    0,  1376,  3779,  ...,   904,  2299,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 18494,  2570,  ...,     1,     1,     1],\n",
      "        [    0, 13317,  2088,  ...,     1,     1,     1],\n",
      "        [    0,  4746, 12561,  ...,     1,     1,     1],\n",
      "        [    0,  6592,  2470,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 19.4755,  -1.5480,  -5.9953,  -6.9341,  -7.9919,  -3.3943],\n",
      "        [ 20.2376,  -2.2910,  -7.5099,  -9.2874,  -7.1149,  -3.9422],\n",
      "        [ 20.3235,  -1.3436,  -7.4019, -10.2999,  -6.0730,  -3.4442],\n",
      "        [ 16.8723,  -0.4742,  -6.9173,  -5.4747,  -7.4853,  -3.4713]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.4755,  -1.5480,  -5.9953,  -6.9341,  -7.9919,  -3.3943],\n",
      "        [ 20.2376,  -2.2910,  -7.5099,  -9.2874,  -7.1149,  -3.9422],\n",
      "        [ 20.3235,  -1.3436,  -7.4019, -10.2999,  -6.0730,  -3.4442],\n",
      "        [ 16.8723,  -0.4742,  -6.9173,  -5.4747,  -7.4853,  -3.4713]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -21.0235, -25.4708, -26.4096, -27.4674, -22.8698],\n",
      "        [  0.0000, -22.5285, -27.7475, -29.5250, -27.3525, -24.1798],\n",
      "        [  0.0000, -21.6671, -27.7254, -30.6234, -26.3965, -23.7677],\n",
      "        [  0.0000, -17.3465, -23.7895, -22.3470, -24.3576, -20.3436]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  9240,  3702,  ...,  2118,  8613,     2],\n",
      "        [    0, 13880,  2082,  ...,  2103,  2259,     2],\n",
      "        [    0,  9936,  2050,  ..., 18952,  2012,     2],\n",
      "        ...,\n",
      "        [    0,  4074,  3802,  ...,  4078,  5744,     2],\n",
      "        [    0,  4377,  3303,  ...,  4650,  2097,     2],\n",
      "        [    0,  1013,  2115,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  4353,  2242,  ...,     1,     1,     1],\n",
      "        [    0, 21786,  2052,  ...,     1,     1,     1],\n",
      "        [    0, 11289,  2843,  ...,     1,     1,     1],\n",
      "        [    0,  6664, 12242,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[19.5985, -2.3648, -7.2209, -9.1718, -7.6475, -4.0742],\n",
      "        [22.2267, -1.5429, -8.7551, -9.5708, -8.5495, -3.3256],\n",
      "        [18.5181, -2.8274, -6.4430, -7.2772, -7.1193, -2.7877],\n",
      "        [20.4902, -1.6349, -7.3433, -8.5817, -6.8093, -4.0392]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[19.5985, -2.3648, -7.2209, -9.1718, -7.6475, -4.0742],\n",
      "        [22.2267, -1.5429, -8.7551, -9.5708, -8.5495, -3.3256],\n",
      "        [18.5181, -2.8274, -6.4430, -7.2772, -7.1193, -2.7877],\n",
      "        [20.4902, -1.6349, -7.3433, -8.5817, -6.8093, -4.0392]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -21.9632, -26.8194, -28.7703, -27.2460, -23.6727],\n",
      "        [  0.0000, -23.7696, -30.9817, -31.7975, -30.7761, -25.5522],\n",
      "        [  0.0000, -21.3455, -24.9611, -25.7953, -25.6374, -21.3057],\n",
      "        [  0.0000, -22.1251, -27.8336, -29.0719, -27.2996, -24.5294]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,   170,  5558,  ...,     1,     1,     1],\n",
      "        [    0,  9570,    12,  ...,     1,     1,     1],\n",
      "        [    0,  1377, 10363,  ...,  6205,  2371,     2],\n",
      "        ...,\n",
      "        [    0,  7785,  2302,  ...,  2051,   991,     2],\n",
      "        [    0,  1880, 28501,  ...,     1,     1,     1],\n",
      "        [    0, 25390,    17,  ..., 27135,  6797,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  4551,  4505,  ...,     1,     1,     1],\n",
      "        [    0,   594,  2209,  ...,     1,     1,     1],\n",
      "        [    0, 12646,  2116,  ...,     1,     1,     1],\n",
      "        [    0,  5379,  2079,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.1945,  -2.9048,  -7.6505,  -9.9728,  -8.1436,  -3.4080],\n",
      "        [ 19.4437,  -1.3146,  -7.1505, -10.0402,  -6.8922,  -2.8099],\n",
      "        [ 22.9650,  -2.6838,  -7.9190, -10.8817,  -7.7006,  -4.7667],\n",
      "        [ 20.0145,  -2.2295,  -9.3999,  -9.4234,  -7.0757,  -2.6144]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.1945,  -2.9048,  -7.6505,  -9.9728,  -8.1436,  -3.4080],\n",
      "        [ 19.4437,  -1.3146,  -7.1505, -10.0402,  -6.8922,  -2.8099],\n",
      "        [ 22.9650,  -2.6838,  -7.9190, -10.8817,  -7.7006,  -4.7667],\n",
      "        [ 20.0145,  -2.2295,  -9.3999,  -9.4234,  -7.0757,  -2.6144]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -25.0994, -29.8451, -32.1673, -30.3382, -25.6025],\n",
      "        [  0.0000, -20.7583, -26.5942, -29.4839, -26.3359, -22.2536],\n",
      "        [  0.0000, -25.6488, -30.8840, -33.8467, -30.6656, -27.7317],\n",
      "        [  0.0000, -22.2440, -29.4144, -29.4379, -27.0902, -22.6289]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  3626,  2615,  ...,     1,     1,     1],\n",
      "        [    0, 10687,  2223,  ...,     1,     1,     1],\n",
      "        [    0, 11400,  2073,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,     6, 24260,  ...,     1,     1,     1],\n",
      "        [    0,    31,  7709,  ...,  2147,    16,     2],\n",
      "        [    0,  1545,  2083,  ..., 24893,  2440,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 17372,  1443,  ...,     1,     1,     1],\n",
      "        [    0,  1761,  2241,  ...,     1,     1,     1],\n",
      "        [    0,  1552,  2515,  ...,     1,     1,     1],\n",
      "        [    0,  1545,    24,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 19.5708,  -0.7969,  -8.6724,  -7.4303,  -8.0606,  -3.0130],\n",
      "        [ 23.0229,  -1.2038,  -7.8886, -10.4397,  -8.3238,  -3.5217],\n",
      "        [ 23.2203,  -3.2553,  -8.9584,  -9.8034,  -8.9801,  -3.8853],\n",
      "        [ 21.2786,  -1.6312,  -6.8107,  -9.2985,  -7.1935,  -3.6275]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.5708,  -0.7969,  -8.6724,  -7.4303,  -8.0606,  -3.0130],\n",
      "        [ 23.0229,  -1.2038,  -7.8886, -10.4397,  -8.3238,  -3.5217],\n",
      "        [ 23.2203,  -3.2553,  -8.9584,  -9.8034,  -8.9801,  -3.8853],\n",
      "        [ 21.2786,  -1.6312,  -6.8107,  -9.2985,  -7.1935,  -3.6275]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -20.3677, -28.2432, -27.0011, -27.6314, -22.5838],\n",
      "        [  0.0000, -24.2267, -30.9115, -33.4626, -31.3468, -26.5446],\n",
      "        [  0.0000, -26.4757, -32.1787, -33.0238, -32.2005, -27.1056],\n",
      "        [  0.0000, -22.9099, -28.0894, -30.5772, -28.4722, -24.9062]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  5937,  2478,  ...,  2116, 20137,     2],\n",
      "        [    0,  4353,  2242,  ...,     1,     1,     1],\n",
      "        [    0,   578,  5667,  ...,  2155,   578,     2],\n",
      "        ...,\n",
      "        [    0,   609,  2179,  ...,     1,     1,     1],\n",
      "        [    0,  3927, 11769,  ...,     1,     1,     1],\n",
      "        [    0,  7586,  2287,  ...,  3638,  2052,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 31115, 24625,  ...,     1,     1,     1],\n",
      "        [    0,  1316,  2260,  ...,     1,     1,     1],\n",
      "        [    0, 29875,  2334,  ...,     1,     1,     1],\n",
      "        [    0,  6385, 14640,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.5980,  -3.4093,  -9.0004,  -9.4204,  -8.6274,  -4.0136],\n",
      "        [ 20.5285,  -0.6100,  -7.4650,  -8.4129,  -8.5831,  -4.8877],\n",
      "        [ 19.9719,  -0.4648,  -6.3067, -10.4986,  -8.4384,  -5.3451],\n",
      "        [ 21.3316,  -0.6885,  -9.3278, -10.0788,  -7.9583,  -3.9953]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.5980,  -3.4093,  -9.0004,  -9.4204,  -8.6274,  -4.0136],\n",
      "        [ 20.5285,  -0.6100,  -7.4650,  -8.4129,  -8.5831,  -4.8877],\n",
      "        [ 19.9719,  -0.4648,  -6.3067, -10.4986,  -8.4384,  -5.3451],\n",
      "        [ 21.3316,  -0.6885,  -9.3278, -10.0788,  -7.9583,  -3.9953]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -25.0073, -30.5985, -31.0185, -30.2254, -25.6116],\n",
      "        [  0.0000, -21.1385, -27.9935, -28.9414, -29.1116, -25.4162],\n",
      "        [  0.0000, -20.4368, -26.2787, -30.4705, -28.4104, -25.3170],\n",
      "        [  0.0000, -22.0201, -30.6594, -31.4104, -29.2899, -25.3269]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  8702,  2063,  ...,     1,     1,     1],\n",
      "        [    0,   842,  2136,  ...,  3757,  2470,     2],\n",
      "        [    0, 14113, 23232,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  5034,  2073,  ...,     1,     1,     1],\n",
      "        [    0,  9270,  4175,  ...,     1,     1,     1],\n",
      "        [    0, 10954, 17438,  ...,    13,  1497,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 13312,  2406,  ...,     1,     1,     1],\n",
      "        [    0, 19772,  2227,  ...,     1,     1,     1],\n",
      "        [    0, 14328,  2114,  ...,     1,     1,     1],\n",
      "        [    0,  1156,  2079,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 20.0606,  -1.6002,  -8.5644,  -9.1121,  -9.5781,  -3.5265],\n",
      "        [ 20.6401,   0.3438,  -8.2871,  -7.8578,  -7.7971,  -2.3564],\n",
      "        [ 21.0203,  -0.6645,  -8.1912,  -9.9211, -10.0333,  -3.7069],\n",
      "        [ 22.5757,  -2.6844,  -8.9543,  -9.1407,  -8.7099,  -4.7271]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 20.0606,  -1.6002,  -8.5644,  -9.1121,  -9.5781,  -3.5265],\n",
      "        [ 20.6401,   0.3438,  -8.2871,  -7.8578,  -7.7971,  -2.3564],\n",
      "        [ 21.0203,  -0.6645,  -8.1912,  -9.9211, -10.0333,  -3.7069],\n",
      "        [ 22.5757,  -2.6844,  -8.9543,  -9.1407,  -8.7099,  -4.7271]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -21.6607, -28.6249, -29.1726, -29.6386, -23.5871],\n",
      "        [  0.0000, -20.2963, -28.9272, -28.4979, -28.4372, -22.9966],\n",
      "        [  0.0000, -21.6848, -29.2115, -30.9414, -31.0537, -24.7272],\n",
      "        [  0.0000, -25.2601, -31.5301, -31.7165, -31.2856, -27.3028]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1521, 16570,  ...,     1,     1,     1],\n",
      "        [    0, 24631, 10065,  ...,     1,     1,     1],\n",
      "        [    0,  1478,  2215,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 18095,  2021,  ..., 11287,  5049,     2],\n",
      "        [    0,  9420,  1585,  ...,  2824,  3606,     2],\n",
      "        [    0,  6125,  4014,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 18468,  2137,  ...,     1,     1,     1],\n",
      "        [    0,  4369,  2240,  ...,     1,     1,     1],\n",
      "        [    0,  3771,  1307,  ...,     1,     1,     1],\n",
      "        [    0, 31115, 24625,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.3867,  -2.6703,  -8.8951,  -8.1473,  -9.2334,  -3.6620],\n",
      "        [ 18.7773,  -0.8637,  -8.6498,  -8.4799,  -9.2344,  -3.2451],\n",
      "        [ 19.5303,  -2.4680,  -9.2555,  -8.5352, -10.1097,  -3.9490],\n",
      "        [ 21.8808,  -2.0652,  -9.9263, -11.0092,  -7.7165,  -3.8706]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.3867,  -2.6703,  -8.8951,  -8.1473,  -9.2334,  -3.6620],\n",
      "        [ 18.7773,  -0.8637,  -8.6498,  -8.4799,  -9.2344,  -3.2451],\n",
      "        [ 19.5303,  -2.4680,  -9.2555,  -8.5352, -10.1097,  -3.9490],\n",
      "        [ 21.8808,  -2.0652,  -9.9263, -11.0092,  -7.7165,  -3.8706]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -25.0569, -31.2817, -30.5340, -31.6200, -26.0487],\n",
      "        [  0.0000, -19.6410, -27.4272, -27.2572, -28.0117, -22.0224],\n",
      "        [  0.0000, -21.9984, -28.7858, -28.0656, -29.6400, -23.4794],\n",
      "        [  0.0000, -23.9460, -31.8072, -32.8901, -29.5973, -25.7514]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 13257, 19499,  ...,  2307,    16,     2],\n",
      "        [    0,  1672,  2174,  ...,     1,     1,     1],\n",
      "        [    0, 14478,  3797,  ..., 31072,  2145,     2],\n",
      "        ...,\n",
      "        [    0, 13801,  2248,  ...,    26,  2429,     2],\n",
      "        [    0, 12864,  2259,  ...,     1,     1,     1],\n",
      "        [    0,  6500,  2116,  ...,  1512,  2259,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  3967, 27135,  ...,     1,     1,     1],\n",
      "        [    0,  6621,  2079,  ...,     1,     1,     1],\n",
      "        [    0,  1905,  2704,  ...,     1,     1,     1],\n",
      "        [    0,  1697,  2210,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.7945,  -3.1839,  -9.2058,  -8.4604,  -8.4411,  -5.1620],\n",
      "        [ 20.1979,  -1.1348,  -8.8554,  -8.9141,  -8.8569,  -3.6190],\n",
      "        [ 19.4439,  -2.2208,  -8.8283,  -8.1538,  -8.8949,  -3.9371],\n",
      "        [ 20.1895,  -1.7209,  -8.5990, -10.5108,  -9.3405,  -5.5601]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.7945,  -3.1839,  -9.2058,  -8.4604,  -8.4411,  -5.1620],\n",
      "        [ 20.1979,  -1.1348,  -8.8554,  -8.9141,  -8.8569,  -3.6190],\n",
      "        [ 19.4439,  -2.2208,  -8.8283,  -8.1538,  -8.8949,  -3.9371],\n",
      "        [ 20.1895,  -1.7209,  -8.5990, -10.5108,  -9.3405,  -5.5601]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -24.9784, -31.0003, -30.2549, -30.2355, -26.9565],\n",
      "        [  0.0000, -21.3326, -29.0533, -29.1120, -29.0548, -23.8168],\n",
      "        [  0.0000, -21.6647, -28.2722, -27.5977, -28.3387, -23.3810],\n",
      "        [  0.0000, -21.9104, -28.7885, -30.7003, -29.5300, -25.7497]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  9175,  2440,  ...,  4571,  7488,     2],\n",
      "        [    0, 15140,  2440,  ...,  1513,  2062,     2],\n",
      "        [    0,  6282,  3657,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 16362,  8309,  ...,     1,     1,     1],\n",
      "        [    0, 27799,  2265,  ..., 30001,  2879,     2],\n",
      "        [    0,  8348,  2440,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,    28,  2211,  ...,     1,     1,     1],\n",
      "        [    0,   842,  2870,  ...,     1,     1,     1],\n",
      "        [    0,  5809,  2256,  ...,     1,     1,     1],\n",
      "        [    0, 18815,  2898,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.5067,   0.0714,  -9.4607,  -9.2553,  -9.5117,  -4.9562],\n",
      "        [ 21.8369,  -1.6544,  -9.5243,  -9.8448, -10.3978,  -4.0046],\n",
      "        [ 21.0510,  -1.4697,  -8.2617, -10.1793, -10.2932,  -4.1640],\n",
      "        [ 21.1772,  -3.5350,  -8.8132,  -8.0379,  -9.0989,  -3.3084]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.5067,   0.0714,  -9.4607,  -9.2553,  -9.5117,  -4.9562],\n",
      "        [ 21.8369,  -1.6544,  -9.5243,  -9.8448, -10.3978,  -4.0046],\n",
      "        [ 21.0510,  -1.4697,  -8.2617, -10.1793, -10.2932,  -4.1640],\n",
      "        [ 21.1772,  -3.5350,  -8.8132,  -8.0379,  -9.0989,  -3.3084]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -21.4353, -30.9675, -30.7620, -31.0184, -26.4630],\n",
      "        [  0.0000, -23.4913, -31.3612, -31.6817, -32.2347, -25.8416],\n",
      "        [  0.0000, -22.5207, -29.3127, -31.2303, -31.3442, -25.2149],\n",
      "        [  0.0000, -24.7122, -29.9904, -29.2151, -30.2761, -24.4855]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  7274,  2731,  ...,     1,     1,     1],\n",
      "        [    0,  5686,  2079,  ...,     1,     1,     1],\n",
      "        [    0,  4892, 27135,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  1036,  3043,  ...,  2205,  2307,     2],\n",
      "        [    0,  1552,  2305,  ...,     1,     1,     1],\n",
      "        [    0, 20561,  2181,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 11158,  2052,  ...,     1,     1,     1],\n",
      "        [    0,  6079,  5182,  ...,     1,     1,     1],\n",
      "        [    0, 27752,  6113,  ...,     1,     1,     1],\n",
      "        [    0,  1327,  2096,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.6448,  -2.2348,  -8.5494, -10.7384, -10.7193,  -4.4543],\n",
      "        [ 19.3390,  -1.7055,  -8.1859,  -8.8284,  -9.1927,  -4.6776],\n",
      "        [ 21.5267,  -1.1608, -10.4041,  -9.6837, -10.5775,  -4.8581],\n",
      "        [ 19.5868,  -3.1401,  -9.0382,  -8.2774,  -9.2477,  -3.4514]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.6448,  -2.2348,  -8.5494, -10.7384, -10.7193,  -4.4543],\n",
      "        [ 19.3390,  -1.7055,  -8.1859,  -8.8284,  -9.1927,  -4.6776],\n",
      "        [ 21.5267,  -1.1608, -10.4041,  -9.6837, -10.5775,  -4.8581],\n",
      "        [ 19.5868,  -3.1401,  -9.0382,  -8.2774,  -9.2477,  -3.4514]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -23.8797, -30.1943, -32.3832, -32.3641, -26.0992],\n",
      "        [  0.0000, -21.0445, -27.5249, -28.1675, -28.5318, -24.0167],\n",
      "        [  0.0000, -22.6875, -31.9308, -31.2104, -32.1042, -26.3848],\n",
      "        [  0.0000, -22.7269, -28.6250, -27.8642, -28.8346, -23.0382]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  4057,  2079,  ...,   636, 24686,     2],\n",
      "        [    0,   170, 22003,  ...,  2140,  2434,     2],\n",
      "        [    0,    25,   100,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 26891,  2440,  ...,  2259, 11942,     2],\n",
      "        [    0, 20855,  2259,  ...,     1,     1,     1],\n",
      "        [    0, 16106,  2302,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,   776,  2170,  ...,     1,     1,     1],\n",
      "        [    0, 23166,  4168,  ...,     1,     1,     1],\n",
      "        [    0,  4074,  3802,  ...,     1,     1,     1],\n",
      "        [    0,  3629, 27135,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 23.0435,  -2.0708,  -9.6616,  -9.9560, -10.9415,  -4.0214],\n",
      "        [ 21.0480,  -1.9303, -10.6692,  -9.5469, -10.2941,  -3.9274],\n",
      "        [ 21.0177,  -2.1499, -10.1680,  -8.9251, -10.6368,  -2.5826],\n",
      "        [ 22.4171,  -1.4525, -10.3826, -10.1145,  -9.3122,  -4.4056]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 23.0435,  -2.0708,  -9.6616,  -9.9560, -10.9415,  -4.0214],\n",
      "        [ 21.0480,  -1.9303, -10.6692,  -9.5469, -10.2941,  -3.9274],\n",
      "        [ 21.0177,  -2.1499, -10.1680,  -8.9251, -10.6368,  -2.5826],\n",
      "        [ 22.4171,  -1.4525, -10.3826, -10.1145,  -9.3122,  -4.4056]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -25.1144, -32.7051, -32.9995, -33.9850, -27.0649],\n",
      "        [  0.0000, -22.9783, -31.7172, -30.5949, -31.3420, -24.9754],\n",
      "        [  0.0000, -23.1676, -31.1856, -29.9428, -31.6545, -23.6002],\n",
      "        [  0.0000, -23.8696, -32.7997, -32.5316, -31.7293, -26.8227]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 11561,  5801,  ...,  2170, 11561,     2],\n",
      "        [    0, 17087,  2079,  ...,     1,     1,     1],\n",
      "        [    0,   848,  2465,  ...,  2170,  4996,     2],\n",
      "        ...,\n",
      "        [    0, 17292,  2583,  ...,     1,     1,     1],\n",
      "        [    0,  4543,  9627,  ...,     1,     1,     1],\n",
      "        [    0,  4392,  3740,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 11561,  3912,  ...,     1,     1,     1],\n",
      "        [    0,  7046,  2138,  ...,     1,     1,     1],\n",
      "        [    0,  1583,  2328,  ...,     1,     1,     1],\n",
      "        [    0, 14047, 20244,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.9845,  -0.3621,  -8.9427, -10.4963,  -9.8386,  -4.7011],\n",
      "        [ 21.3388,  -2.5691,  -9.5512,  -9.7422, -12.6039,  -4.1993],\n",
      "        [ 21.8017,  -2.0317,  -9.0329,  -8.9907, -10.3493,  -3.7242],\n",
      "        [ 19.0390,  -0.9438,  -9.3975,  -7.1172,  -9.1319,  -3.8134]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.9845,  -0.3621,  -8.9427, -10.4963,  -9.8386,  -4.7011],\n",
      "        [ 21.3388,  -2.5691,  -9.5512,  -9.7422, -12.6039,  -4.1993],\n",
      "        [ 21.8017,  -2.0317,  -9.0329,  -8.9907, -10.3493,  -3.7242],\n",
      "        [ 19.0390,  -0.9438,  -9.3975,  -7.1172,  -9.1319,  -3.8134]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -23.3466, -31.9271, -33.4808, -32.8230, -27.6855],\n",
      "        [  0.0000, -23.9078, -30.8900, -31.0810, -33.9427, -25.5381],\n",
      "        [  0.0000, -23.8334, -30.8346, -30.7924, -32.1510, -25.5259],\n",
      "        [  0.0000, -19.9829, -28.4365, -26.1563, -28.1709, -22.8524]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1504,  2322,  ...,     1,     1,     1],\n",
      "        [    0, 19214, 16579,  ...,     1,     1,     1],\n",
      "        [    0, 26447,  2100,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    31,  5001,  ...,  2453,  2753,     2],\n",
      "        [    0,   801,  2051,  ...,     1,     1,     1],\n",
      "        [    0, 18608,  4926,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  1797,  1002,  ...,     1,     1,     1],\n",
      "        [    0,  6440,  9831,  ...,     1,     1,     1],\n",
      "        [    0, 25015,  2116,  ...,     1,     1,     1],\n",
      "        [    0,  4510,  2116,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 19.1005,  -2.0925,  -8.7221,  -8.5179, -12.0563,  -3.4705],\n",
      "        [ 21.6042,  -1.7303,  -8.4487,  -8.0657, -11.3859,  -2.2652],\n",
      "        [ 20.5066,  -1.8951,  -8.3744, -10.9005,  -9.5563,  -3.2587],\n",
      "        [ 19.9305,  -1.9768,  -8.2251,  -8.8430,  -9.7071,  -4.4796]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.1005,  -2.0925,  -8.7221,  -8.5179, -12.0563,  -3.4705],\n",
      "        [ 21.6042,  -1.7303,  -8.4487,  -8.0657, -11.3859,  -2.2652],\n",
      "        [ 20.5066,  -1.8951,  -8.3744, -10.9005,  -9.5563,  -3.2587],\n",
      "        [ 19.9305,  -1.9768,  -8.2251,  -8.8430,  -9.7071,  -4.4796]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -21.1929, -27.8226, -27.6183, -31.1567, -22.5710],\n",
      "        [  0.0000, -23.3345, -30.0530, -29.6700, -32.9901, -23.8695],\n",
      "        [  0.0000, -22.4018, -28.8810, -31.4071, -30.0629, -23.7653],\n",
      "        [  0.0000, -21.9074, -28.1556, -28.7735, -29.6376, -24.4101]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  6584,  3708,  ...,  3881,  2460,     2],\n",
      "        [    0,  3760,   772,  ...,     1,     1,     1],\n",
      "        [    0, 20647,  2440,  ...,  2062,    18,     2],\n",
      "        ...,\n",
      "        [    0, 12990, 27135,  ...,     1,     1,     1],\n",
      "        [    0,  1378,  2286,  ...,     1,     1,     1],\n",
      "        [    0, 13252,  2440,  ...,  2522, 18032,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  3738,  3704,  ...,     1,     1,     1],\n",
      "        [    0, 21108,  2116,  ...,     1,     1,     1],\n",
      "        [    0, 16773,  2154,  ...,     1,     1,     1],\n",
      "        [    0, 17087,  2079,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.3147,  -2.5620,  -9.2466, -11.0062,  -9.5493,  -4.0354],\n",
      "        [ 18.7601,  -0.1696,  -8.4678,  -6.4940, -10.7135,  -1.8448],\n",
      "        [ 19.0266,  -2.0295,  -7.6714,  -8.1152, -10.2432,  -3.5881],\n",
      "        [ 22.4040,  -2.2177, -10.5936,  -9.8608, -10.1265,  -5.0580]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.3147,  -2.5620,  -9.2466, -11.0062,  -9.5493,  -4.0354],\n",
      "        [ 18.7601,  -0.1696,  -8.4678,  -6.4940, -10.7135,  -1.8448],\n",
      "        [ 19.0266,  -2.0295,  -7.6714,  -8.1152, -10.2432,  -3.5881],\n",
      "        [ 22.4040,  -2.2177, -10.5936,  -9.8608, -10.1265,  -5.0580]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -24.8767, -31.5613, -33.3208, -31.8640, -26.3500],\n",
      "        [  0.0000, -18.9296, -27.2279, -25.2540, -29.4736, -20.6048],\n",
      "        [  0.0000, -21.0562, -26.6980, -27.1418, -29.2698, -22.6147],\n",
      "        [  0.0000, -24.6217, -32.9976, -32.2648, -32.5305, -27.4621]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 11593,  2653,  ...,     1,     1,     1],\n",
      "        [    0,  1641,     3,  ...,     3,    81,     2],\n",
      "        [    0,  3727, 14925,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 19381,  2259,  ...,     1,     1,     1],\n",
      "        [    0,  4826, 16194,  ..., 31221,  4001,     2],\n",
      "        [    0,  8995,  2079,  ...,  2073,  1402,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 11593,  2653,  ...,     1,     1,     1],\n",
      "        [    0,   648,  4120,  ...,     1,     1,     1],\n",
      "        [    0,  1819,  2391,  ...,     1,     1,     1],\n",
      "        [    0, 28928,  2116,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.1571,  -3.0075,  -9.2107,  -9.4987,  -9.2059,  -4.3287],\n",
      "        [ 20.8579,  -1.9344,  -9.7126,  -9.4143, -10.0730,  -4.1921],\n",
      "        [ 21.4141,  -2.1531, -11.6363, -10.1869, -10.7460,  -2.6458],\n",
      "        [ 20.0757,  -1.9212,  -9.0831,  -9.0908,  -9.9237,  -4.0202]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.1571,  -3.0075,  -9.2107,  -9.4987,  -9.2059,  -4.3287],\n",
      "        [ 20.8579,  -1.9344,  -9.7126,  -9.4143, -10.0730,  -4.1921],\n",
      "        [ 21.4141,  -2.1531, -11.6363, -10.1869, -10.7460,  -2.6458],\n",
      "        [ 20.0757,  -1.9212,  -9.0831,  -9.0908,  -9.9237,  -4.0202]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -24.1646, -30.3678, -30.6558, -30.3630, -25.4858],\n",
      "        [  0.0000, -22.7924, -30.5705, -30.2723, -30.9310, -25.0501],\n",
      "        [  0.0000, -23.5672, -33.0504, -31.6010, -32.1601, -24.0599],\n",
      "        [  0.0000, -21.9969, -29.1588, -29.1665, -29.9994, -24.0959]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,   587,  2254,  ...,     1,     1,     1],\n",
      "        [    0,   788, 27464,  ..., 14424, 10309,     2],\n",
      "        [    0,  4510,  2079,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 15321,  1235,  ...,     1,     1,     1],\n",
      "        [    0,  6370,  2287,  ..., 14208,  2138,     2],\n",
      "        [    0,  1504,  4171,  ..., 11187, 28765,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[   0, 6264, 2242,  ...,    1,    1,    1],\n",
      "        [   0, 1808, 7702,  ...,    1,    1,    1],\n",
      "        [   0, 8264, 2079,  ...,    1,    1,    1],\n",
      "        [   0, 1324, 2316,  ...,    1,    1,    1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.6293,  -1.4329,  -9.4128,  -8.6355,  -8.8768,  -2.6613],\n",
      "        [ 19.1874,  -1.3316,  -7.6627,  -8.8115,  -9.8936,  -5.1905],\n",
      "        [ 21.0061,  -1.7198,  -9.8834,  -8.7420,  -9.5514,  -5.7673],\n",
      "        [ 20.5556,  -2.6237,  -8.2679,  -8.3596, -10.1559,  -4.4438]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.6293,  -1.4329,  -9.4128,  -8.6355,  -8.8768,  -2.6613],\n",
      "        [ 19.1874,  -1.3316,  -7.6627,  -8.8115,  -9.8936,  -5.1905],\n",
      "        [ 21.0061,  -1.7198,  -9.8834,  -8.7420,  -9.5514,  -5.7673],\n",
      "        [ 20.5556,  -2.6237,  -8.2679,  -8.3596, -10.1559,  -4.4438]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -23.0622, -31.0421, -30.2647, -30.5061, -24.2906],\n",
      "        [  0.0000, -20.5190, -26.8501, -27.9989, -29.0811, -24.3779],\n",
      "        [  0.0000, -22.7259, -30.8894, -29.7481, -30.5574, -26.7733],\n",
      "        [  0.0000, -23.1793, -28.8235, -28.9152, -30.7114, -24.9993]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,   993,  2119,  ...,     1,     1,     1],\n",
      "        [    0,  7274,  2731,  ...,     1,     1,     1],\n",
      "        [    0,  3666,  6233,  ...,  2259, 11132,     2],\n",
      "        ...,\n",
      "        [    0, 17783,  2079,  ..., 25640,  2116,     2],\n",
      "        [    0,  3666,  2069,  ...,     1,     1,     1],\n",
      "        [    0, 10833,  2083,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  1500,  2444,  ...,     1,     1,     1],\n",
      "        [    0,  1002, 30962,  ...,     1,     1,     1],\n",
      "        [    0,  3804,  1083,  ...,     1,     1,     1],\n",
      "        [    0, 15523,  2386,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.3153,  -1.2288, -10.4920,  -9.0514,  -9.3814,  -4.7588],\n",
      "        [ 23.2623,  -1.1756,  -9.9728,  -9.0057, -11.1800,  -3.6227],\n",
      "        [ 21.2246,  -0.9977,  -9.7213,  -9.4952,  -9.9515,  -2.4588],\n",
      "        [ 23.2455,  -2.2831, -10.7871, -10.1287,  -9.6592,  -4.7377]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.3153,  -1.2288, -10.4920,  -9.0514,  -9.3814,  -4.7588],\n",
      "        [ 23.2623,  -1.1756,  -9.9728,  -9.0057, -11.1800,  -3.6227],\n",
      "        [ 21.2246,  -0.9977,  -9.7213,  -9.4952,  -9.9515,  -2.4588],\n",
      "        [ 23.2455,  -2.2831, -10.7871, -10.1287,  -9.6592,  -4.7377]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -23.5441, -32.8073, -31.3667, -31.6967, -27.0742],\n",
      "        [  0.0000, -24.4379, -33.2351, -32.2680, -34.4423, -26.8849],\n",
      "        [  0.0000, -22.2223, -30.9460, -30.7199, -31.1761, -23.6835],\n",
      "        [  0.0000, -25.5286, -34.0326, -33.3742, -32.9048, -27.9832]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  5370,  2052,  ...,     1,     1,     1],\n",
      "        [    0, 19070,  3874,  ...,     1,     1,     1],\n",
      "        [    0,  5686,  2079,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  3719,    16,  ...,  2359,  2088,     2],\n",
      "        [    0, 13252,  2440,  ...,  2522, 18032,     2],\n",
      "        [    0,  9936,  2050,  ...,  2720,    12,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  4499, 21602,  ...,     1,     1,     1],\n",
      "        [    0, 14361,  2345,  ...,     1,     1,     1],\n",
      "        [    0,  1797,  2052,  ...,     1,     1,     1],\n",
      "        [    0,  3638,  2079,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.5672,  -1.5894, -10.3553,  -8.8562,  -9.9760,  -5.3571],\n",
      "        [ 21.4335,  -1.3987, -11.1057,  -9.9846, -10.5509,  -5.3060],\n",
      "        [ 20.6373,  -3.0376, -10.3381,  -9.6494, -10.2476,  -5.0586],\n",
      "        [ 22.9716,  -1.4670,  -9.0200, -10.3608, -11.0171,  -5.3462]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.5672,  -1.5894, -10.3553,  -8.8562,  -9.9760,  -5.3571],\n",
      "        [ 21.4335,  -1.3987, -11.1057,  -9.9846, -10.5509,  -5.3060],\n",
      "        [ 20.6373,  -3.0376, -10.3381,  -9.6494, -10.2476,  -5.0586],\n",
      "        [ 22.9716,  -1.4670,  -9.0200, -10.3608, -11.0171,  -5.3462]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -23.1566, -31.9225, -30.4234, -31.5432, -26.9243],\n",
      "        [  0.0000, -22.8322, -32.5392, -31.4180, -31.9844, -26.7395],\n",
      "        [  0.0000, -23.6749, -30.9754, -30.2867, -30.8849, -25.6959],\n",
      "        [  0.0000, -24.4386, -31.9916, -33.3324, -33.9887, -28.3178]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1934,  2304,  ...,     1,     1,     1],\n",
      "        [    0,  3817,  6775,  ...,  2259,  4135,     2],\n",
      "        [    0, 24893,  2440,  ...,  6233,  4263,     2],\n",
      "        ...,\n",
      "        [    0,  1421,  2137,  ...,  2073, 13901,     2],\n",
      "        [    0,     3,  4831,  ...,     1,     1,     1],\n",
      "        [    0,   974,  2130,  ...,  2069,  1850,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[   0, 1934, 2304,  ...,    1,    1,    1],\n",
      "        [   0, 1816, 5822,  ...,    1,    1,    1],\n",
      "        [   0, 5791, 2440,  ...,    1,    1,    1],\n",
      "        [   0, 7757, 2170,  ...,    1,    1,    1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 17.9045,  -1.2179,  -7.7731,  -6.4405, -10.0259,  -4.3380],\n",
      "        [ 21.7187,  -2.2855, -10.7054, -10.9708, -11.4866,  -3.5602],\n",
      "        [ 22.9666,  -2.4189,  -9.5351, -10.6276, -12.2419,  -4.6680],\n",
      "        [ 21.2515,  -2.8826, -10.8914,  -8.8156, -10.4147,  -5.2708]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 17.9045,  -1.2179,  -7.7731,  -6.4405, -10.0259,  -4.3380],\n",
      "        [ 21.7187,  -2.2855, -10.7054, -10.9708, -11.4866,  -3.5602],\n",
      "        [ 22.9666,  -2.4189,  -9.5351, -10.6276, -12.2419,  -4.6680],\n",
      "        [ 21.2515,  -2.8826, -10.8914,  -8.8156, -10.4147,  -5.2708]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -19.1224, -25.6776, -24.3450, -27.9304, -22.2425],\n",
      "        [  0.0000, -24.0042, -32.4241, -32.6895, -33.2053, -25.2789],\n",
      "        [  0.0000, -25.3855, -32.5017, -33.5942, -35.2085, -27.6346],\n",
      "        [  0.0000, -24.1341, -32.1429, -30.0671, -31.6662, -26.5223]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  7051,  2079,  ...,     1,     1,     1],\n",
      "        [    0,  4892, 27135,  ...,  2079, 12882,     2],\n",
      "        [    0,  1781,  2603,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  8099,  4904,  ...,     1,     1,     1],\n",
      "        [    0,  8920,  5635,  ...,     1,     1,     1],\n",
      "        [    0, 21824,  2063,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  7051,  2079,  ...,     1,     1,     1],\n",
      "        [    0,  3907, 15259,  ...,     1,     1,     1],\n",
      "        [    0, 11590,  5194,  ...,     1,     1,     1],\n",
      "        [    0,  1316,  2626,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.7287,  -2.6955, -10.6518,  -8.3333, -10.7467,  -5.0642],\n",
      "        [ 21.9848,  -0.1786, -10.9322, -10.1143, -13.0492,  -4.2068],\n",
      "        [ 22.4297,  -2.1080,  -9.2583, -10.0860, -11.1316,  -4.9439],\n",
      "        [ 22.0815,  -2.4216, -10.6583, -10.4968, -11.3330,  -2.6739]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.7287,  -2.6955, -10.6518,  -8.3333, -10.7467,  -5.0642],\n",
      "        [ 21.9848,  -0.1786, -10.9322, -10.1143, -13.0492,  -4.2068],\n",
      "        [ 22.4297,  -2.1080,  -9.2583, -10.0860, -11.1316,  -4.9439],\n",
      "        [ 22.0815,  -2.4216, -10.6583, -10.4968, -11.3330,  -2.6739]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -25.4243, -33.3805, -31.0620, -33.4754, -27.7929],\n",
      "        [  0.0000, -22.1633, -32.9170, -32.0990, -35.0340, -26.1916],\n",
      "        [  0.0000, -24.5376, -31.6880, -32.5156, -33.5613, -27.3736],\n",
      "        [  0.0000, -24.5031, -32.7397, -32.5783, -33.4144, -24.7553]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  6739,  5804,  ...,     1,     1,     1],\n",
      "        [    0,  1959,  2316,  ...,     1,     1,     1],\n",
      "        [    0,  4353,  2242,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  3665,  5262,  ...,     1,     1,     1],\n",
      "        [    0, 10170,  4342,  ...,  2397,  2522,     2],\n",
      "        [    0,  5676,  8665,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 10152,  2504,  ...,     1,     1,     1],\n",
      "        [    0, 21777,  2057,  ...,     1,     1,     1],\n",
      "        [    0,  1490,  2300,  ...,     1,     1,     1],\n",
      "        [    0,  4057,  2079,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.8228,  -1.3899, -11.0177,  -9.5462, -10.7819,  -4.2403],\n",
      "        [ 21.8871,  -1.5409,  -9.8786,  -9.6832, -11.4813,  -2.7281],\n",
      "        [ 20.7886,  -1.6894, -10.5761, -10.7053,  -9.9545,  -3.8030],\n",
      "        [ 22.8381,  -1.0503, -10.5601, -10.0164, -11.3776,  -4.9549]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.8228,  -1.3899, -11.0177,  -9.5462, -10.7819,  -4.2403],\n",
      "        [ 21.8871,  -1.5409,  -9.8786,  -9.6832, -11.4813,  -2.7281],\n",
      "        [ 20.7886,  -1.6894, -10.5761, -10.7053,  -9.9545,  -3.8030],\n",
      "        [ 22.8381,  -1.0503, -10.5601, -10.0164, -11.3776,  -4.9549]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -24.2127, -33.8405, -32.3690, -33.6047, -27.0631],\n",
      "        [  0.0000, -23.4281, -31.7657, -31.5703, -33.3685, -24.6152],\n",
      "        [  0.0000, -22.4780, -31.3648, -31.4939, -30.7431, -24.5916],\n",
      "        [  0.0000, -23.8884, -33.3982, -32.8545, -34.2157, -27.7930]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  1002, 18298,  ...,  3973,  2371,     2],\n",
      "        [    0,  3666,  2145,  ...,     1,     1,     1],\n",
      "        [    0,  1076,    17,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 11801,  2079,  ...,    18,     3,     2],\n",
      "        [    0,  5361,  2489,  ...,     1,     1,     1],\n",
      "        [    0, 13801,  2236,  ..., 11558, 13147,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 30341,  2116,  ...,     1,     1,     1],\n",
      "        [    0,  1469,  2664,  ...,     1,     1,     1],\n",
      "        [    0,  6393,  2270,  ...,     1,     1,     1],\n",
      "        [    0,  1969, 21091,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 21.5097,  -1.3860, -10.0200,  -9.1716, -10.5828,  -3.7839],\n",
      "        [ 21.5186,  -1.2884,  -8.9377,  -9.2515, -12.1496,  -4.6143],\n",
      "        [ 21.6208,  -0.9018,  -8.8081, -10.2321, -11.3282,  -5.1347],\n",
      "        [ 21.4746,  -0.0448, -11.6281,  -9.1787,  -9.6705,  -3.4839]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.5097,  -1.3860, -10.0200,  -9.1716, -10.5828,  -3.7839],\n",
      "        [ 21.5186,  -1.2884,  -8.9377,  -9.2515, -12.1496,  -4.6143],\n",
      "        [ 21.6208,  -0.9018,  -8.8081, -10.2321, -11.3282,  -5.1347],\n",
      "        [ 21.4746,  -0.0448, -11.6281,  -9.1787,  -9.6705,  -3.4839]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -22.8957, -31.5297, -30.6812, -32.0924, -25.2935],\n",
      "        [  0.0000, -22.8070, -30.4563, -30.7700, -33.6682, -26.1329],\n",
      "        [  0.0000, -22.5225, -30.4289, -31.8529, -32.9489, -26.7554],\n",
      "        [  0.0000, -21.5194, -33.1027, -30.6534, -31.1451, -24.9585]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0,  4543,  2079,  ...,  2079,  4451,     2],\n",
      "        [    0,  6447,  2305,  ...,     1,     1,     1],\n",
      "        [    0, 12730,  2304,  ...,  2287,  2015,     2],\n",
      "        ...,\n",
      "        [    0, 15661,  2521,  ...,     1,     1,     1],\n",
      "        [    0, 11195,  2156,  ...,     1,     1,     1],\n",
      "        [    0,   776,  2165,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0,  4543,  4451,  ...,     1,     1,     1],\n",
      "        [    0,  1284,  2052,  ...,     1,     1,     1],\n",
      "        [    0, 20969,  2015,  ...,     1,     1,     1],\n",
      "        [    0, 27131,  2127,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "q_outputs.shape before:  torch.Size([4, 768])\n",
      "p_outputs.shape before:  torch.Size([24, 768])\n",
      "targets.shape:  torch.Size([4])\n",
      "q_outputs.shape:  torch.Size([4, 1, 768])\n",
      "p_outputs.shape:  torch.Size([4, 768, 6])\n",
      "sim_scores1:  tensor([[ 22.0145,  -1.8869, -11.2625,  -9.3697, -11.5345,  -4.1551],\n",
      "        [ 23.0506,  -2.8151,  -9.9630, -10.6009, -11.3593,  -3.8541],\n",
      "        [ 22.9923,  -2.4226, -10.9691, -10.0969, -12.9289,  -5.9574],\n",
      "        [ 22.2813,  -2.1256, -10.9152,  -9.2724, -10.9507,  -4.5798]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.0145,  -1.8869, -11.2625,  -9.3697, -11.5345,  -4.1551],\n",
      "        [ 23.0506,  -2.8151,  -9.9630, -10.6009, -11.3593,  -3.8541],\n",
      "        [ 22.9923,  -2.4226, -10.9691, -10.0969, -12.9289,  -5.9574],\n",
      "        [ 22.2813,  -2.1256, -10.9152,  -9.2724, -10.9507,  -4.5798]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -23.9013, -33.2769, -31.3842, -33.5490, -26.1696],\n",
      "        [  0.0000, -25.8656, -33.0135, -33.6514, -34.4098, -26.9047],\n",
      "        [  0.0000, -25.4149, -33.9614, -33.0892, -35.9212, -28.9496],\n",
      "        [  0.0000, -24.4070, -33.1965, -31.5538, -33.2321, -26.8611]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "targets:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_inputs:  {'input_ids': tensor([[    0, 20368,  2440,  ...,     1,     1,     1],\n",
      "        [    0,  4950,  1943,  ...,     1,     1,     1],\n",
      "        [    0,    21,  2042,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  6241,  2440,  ...,     1,     1,     1],\n",
      "        [    0, 25602, 22718,  ...,     1,     1,     1],\n",
      "        [    0,  8729,  7506,  ...,  2073,  4230,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "q_inputs:  {'input_ids': tensor([[    0, 30129,  2225,  ...,     1,     1,     1],\n",
      "        [    0,   648,  2427,  ...,     1,     1,     1],\n",
      "        [    0,  3666,  3739,  ...,     1,     1,     1],\n",
      "        [    0,  1086,  2247,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13211/1239205259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13211/2491405680.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    151\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q_inputs: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m           \u001b[0mp_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mp_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m           \u001b[0mq_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mq_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13211/3412069747.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     outputs = self.roberta(\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         )\n\u001b[0;32m--> 848\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 )\n\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    170\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \"\"\"\n\u001b[0;32m-> 2048\u001b[0;31m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0m\u001b[1;32m   2049\u001b[0m                             torch.backends.cudnn.enabled)\n\u001b[1;32m   2050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"해바라기는 무슨꽃일까?\"\n",
    "# query = '대한민국의 대통령은 누구인가?'\n",
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search Query] 해바라기는 무슨꽃일까?\n",
      "\n",
      "Top-1th Passage (Index 2346)\n",
      "('마지막 유격전은 적의 수송대를 공격하는 작전이었다. 생도들은 정보원으로부터 북한군의 UN군의 서울 공격에 대비하여 마을 사람들을 화물차에 '\n",
      " '싣고 북으로 올라갈 것이라는 소식을 듣게 되고, 구출 작전을 구상하게 된다. 당시 유격대가 보유한 장비는 개인별 소총 1자루와 실탄 '\n",
      " '10여 발이 전부였지만, 국가와 국민을 지키겠다는 신념 하나로 전투에 임하게 된다. 전투는 야간에 시작되었다. 생도들은 적이 통과할 '\n",
      " '내곡리 마을 주변에 매복하고 적군을 기다렸다. 23시경 적군의 수송대가 내곡리 마을을 통과하려던 찰나, 생도들은 소총을 쏘며 습격을 '\n",
      " '감행하였다. 이때 적군의 혼란을 틈타 조영달 생도가 주민들에게 대피하라고 외친 덕에 많은 주민들이 구출될 수 있었다. 가지고 있던 장비를 '\n",
      " '모두 소모한 생도들은 불암산의 기지로 복귀하고자 하였으나 적의 흉탄에 남은 생도 모두가 장렬히 전사하였다. 이로써 국가와 국민을 위해 '\n",
      " '수도 서울과 육사를 방어하고자 항쟁한 생도들의 찬란한 유격전도 막을 내렸다. 때는 서울 수복 1주일 전이였다.')\n",
      "Top-2th Passage (Index 106)\n",
      "('괘불이란 야외에서 큰 법회나 의식을 열 때 쓰이는 대형불화를 말하며, 이 불화는 보살 형태의 단독상을 화면 전체에 꽉 차게 그려 넣은 '\n",
      " '것이다. \\\\n\\\\n보살상은 양 손으로 꽃가지를 받치고 서 있는 모습으로 상체를 크게 묘사한 반면 하체는 짧게 나타냈다. 머리에는 '\n",
      " '산(山) 모양의 화려한 장식이 달린 보관(寶冠)을 쓰고 있으며 네모진 얼굴을 하고 있다. 양쪽 어깨를 감싼 옷은 다양한 무늬로 장식되어 '\n",
      " '있고, 광배(光背)는 머리 광배와 몸광배를 구분하여 큼직하게 그렸다. 몸광배 안에는 꽃무늬, 구름무늬 등을 그려 공간을 채우고 있는데 '\n",
      " '옷의 화려한 무늬들과 어우러져 부처님 세계의 정경을 보는 것처럼 느껴진다. 광배 위쪽으로는 구름이 감싸고 있고 화면의 가장 윗부분은 흰 '\n",
      " '광선이 소용돌이를 이루면서 걸쳐져 있다. \\\\n\\\\n전체적인 비례가 잘 이루어지지는 않았으나 묵중하고 중후한 느낌을 보여주며 다양한 '\n",
      " '문양과 장식, 밝고 선명한 원색과 중간색을 적절히 사용하여 화려하고 밝은 분위기를 느끼게 한다. 참여한 많은 인물들과 함께 법주사 창건 '\n",
      " '배경과 당시의 불교 사상을 파악할 수 있는 글이 남아 있어 더욱 중요하게 여겨지는 작품이다.')\n",
      "Top-3th Passage (Index 3804)\n",
      "(\"열대우림 '장마전선'은 벵골만과 서북태평양에서 동아시아 몬순의 하위시스템으로 조성된다. '장마전선'의 북진 움직임은 아열대 능선이 발달한 \"\n",
      " \"데 영향을 받는다. 이 북쪽으로 이동하는 준정전선은 남한에서 '장마'라고 불리며, 주요 강수 기간을 나타낸다. 창마전선'은 한반도를 \"\n",
      " '통과하는 데 약 4~5주가 걸린다. 이러한 느린 움직임은 매년 6월말과 7월에 한반도 전체에 많은 양의 여름 강우량을 발생시킨다. 최근 '\n",
      " \"들어 '창마전선'은 7월 말부터 8월 초까지 다양한 규모의 폭풍우와 함께 폭우가 쏟아지면서 한반도를 통과하는 데 3주도 채 걸리지 않는 \"\n",
      " \"등 빠르게 움직이는 경향을 보였다. '창마' 이후 더 극한의 날씨와 국지적인 폭우가 발생하고 있다는 뜻이다. 잠열 방출에 의해 강하게 \"\n",
      " \"변형된 바로크린 교란에서 비롯된 초여름의 '창마' 비의 역학관계는 여전히 제대로 파악되지 않고 있다. 가을 창마로 부를 수 있는 또 다른 \"\n",
      " \"'창마' 유형도 있다. 이는 물론 기상청의 공식 용어는 아니다. 그러나 최근의 기후 변화로 인해 '낙하 창마'라는 용어가 생겨났다. \"\n",
      " \"'낙하 창마'는 보통 8월 말에서 9월 초에 시작한다. 한반도에서 북태평양고기압이 완전히 끝난 뒤 '폭포창마'도 끝났다. 최근의 이 \"\n",
      " \"'폭포창마'는 보통의 '창마'보다 훨씬 더 큰 피해를 가져오는데, 왜냐하면 '폭포창마'는 단기간에 극도의 폭우가 집중적으로 쏟아지기 \"\n",
      " '때문이다. 장마 순환 변화가 없을 경우 강수량이 증가할 것으로 예상되지만 비교적 완만한 이동이나 시기 변화는 동중국인, 한국, 일본 '\n",
      " '기후에 큰 영향을 미칠 수 있다.')\n",
      "Top-4th Passage (Index 3634)\n",
      "('금동미륵보살반가사유상(金銅彌勒菩薩半跏思惟像)은 현재 경기도 용인의 호암미술관에 전시되어 있는 높이 11.1cm의 작은 '\n",
      " '금동불이다.\\\\n\\\\n머리에는 산봉우리 모양의 삼산관(三山冠)을 쓰고 있는데, 봉우리가 거의 동일선상에 있어서 주목된다. 머리카락은 '\n",
      " '2가닥으로 길게 늘어져 양 어깨를 덮고 있으며 얼굴은 몸에 비해 큰 편으로 고개를 약간 숙인 채 미소를 머금고 있다. 목 부분이 표현되지 '\n",
      " '않아 약간은 투박한 느낌을 주고 있다.\\\\n\\\\n상체에는 옷을 걸치고 있지 않으며 치마가 가슴 부근까지 올라와 있다. 옷자락은 굵은 '\n",
      " '주름으로 표현되어 대좌(臺座) 아래까지 넓게 퍼져 있으며 앞면은 물론 불상의 뒷면에까지 표현되어 있다. 일반적인 반가상(半跏像)의 '\n",
      " '형식대로 오른쪽 다리를 왼쪽 다리 위에 포개고 오른쪽 팔꿈치를 오른쪽 무릎에 대어 오른손을 볼에 가볍게 대고 있는 모습이다. 두 팔목에는 '\n",
      " '두꺼운 팔찌가 있고 늘어뜨린 왼발 밑에는 연꽃무늬가 새겨진 발받침대가 따로 마련되어 있다.\\\\n\\\\n조각수법이 뛰어나지 않고 표현기법도 '\n",
      " '그다지 세련되지 않았으나 고식(古式)의 반가상 양식을 지니고 있다. 작은 몸에 비해 얼굴이 크고 허리가 굵으며 상체를 강조한 점 등은 '\n",
      " '중국의 제나라, 주나라의 양식과 통하므로, 6세기 후반경의 작품으로 추정된다. 특히 경상남도에서 출토되었다는 점을 감안할 때 신라시대에 '\n",
      " '만들어진 작품일 가능성이 높다.')\n",
      "Top-5th Passage (Index 2111)\n",
      "('구상에서 온도가 다른 두 기단이 만날 때 그 경계를 이루는 전선면(전이층)은 수평에서 약간 기울어진 사면이 된다. 그 경사는 지표면에 '\n",
      " '가까운 부분을 제외하면 1/50에서 1/200 사이에 있다. 일반적으로 한랭전선의 경사는 온난전선보다 크다. 전선 부분은 기온의 수평 '\n",
      " '경사가 크고, 그러한 경향은 대류권 전반에 두루 영향을 미치고 있으므로 전선 부근에서는 상공으로 올라갈수록 바람이 강하며, 권계면 '\n",
      " '부근에서 최대가 된다. 이것이 바로 한대전선 제트기류이다.\\\\n\\\\n전선이 통과했을 때 기상 요소의 변화를 자기 기록으로 측정해 보면, '\n",
      " '첫째, 한랭전선이 통과하면 온도는 급격히 저하하고, 온난전선이 통과하면 온도가 상승한다. 그러나 한랭전선일수록 급격한 변화는 나타나지 '\n",
      " '않는다.\\\\n\\\\n둘째, 한랭전선이 통과하면 풍향이 남쪽으로 치우쳐 있다가 시계 방향으로 급격히 북쪽 또는 서쪽으로 변화한다. 단 '\n",
      " '한랭전선이 통과하기 직전에 시계반대방향으로 변화하는 경우가 있다. 온난전선이 통과하면 풍향은 시계방향으로 북쪽에서 남쪽으로 '\n",
      " '바뀐다.\\\\n\\\\n셋째, 온난전선과 한랭전선 모두 기압은 전선 부분에서 극소치를 나타낸다. 즉 전선은 기압골을 수반한다. 그 때문에 전선 '\n",
      " '부분에서 등압선은 V자형으로 구부러진다.\\\\n\\\\n넷째, 온난전선을 따라 생기는 구름은 층상(層狀)으로, 가장 낮은 곳부터 순서대로 '\n",
      " '난층운·고층운·권층운이 나타나며, 난층운부터는 연속적으로 강수가 있다. 한랭전선을 따라 생기는 구름은 적운 또는 적란운으로, 소나기나 '\n",
      " '뇌우를 동반한다.')\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Search Query] {query}\\n\")\n",
    "\n",
    "indices = results.tolist()\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"Top-{i + 1}th Passage (Index {idx})\")\n",
    "    pprint(retriever.dataset[\"context\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[    0, 21847,  2181,  ...,  3655, 21847,     2],\n",
       "          [    0,  7785,  2302,  ...,  2051,   991,     2],\n",
       "          [    0,  1478,  2878,  ...,  2172,  2446,     2]],\n",
       " \n",
       "         [[    0,  3719, 10695,  ...,     1,     1,     1],\n",
       "          [    0,  1176,  2489,  ...,  2507,  2062,     2],\n",
       "          [    0,  5865,  2079,  ...,  1513,  2259,     2]],\n",
       " \n",
       "         [[    0, 15884,  2160,  ...,  2112, 14019,     2],\n",
       "          [    0,  5352,  2504,  ...,     1,     1,     1],\n",
       "          [    0,  1183, 11483,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0,   812,  3657,  ...,     1,     1,     1],\n",
       "          [    0,  1504, 16550,  ...,     1,     1,     1],\n",
       "          [    0,  5034,  2073,  ...,     1,     1,     1]]]),\n",
       " tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " tensor([[    0, 21847,  2181,  ...,     1,     1,     1],\n",
       "         [    0, 19128,    23,  ...,     1,     1,     1],\n",
       "         [    0, 27752,  6113,  ...,     1,     1,     1],\n",
       "         [    0,  1176,  2278,  ...,     1,     1,     1]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(retriever.train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_scores1:  tensor([[ 2.0670, -1.8882, -0.4755],\n",
      "        [ 1.1081, -0.2872, -0.5352],\n",
      "        [ 1.2624, -0.2268, -1.5149],\n",
      "        [ 2.0739, -1.5962, -0.9872]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 2.0670, -1.8882, -0.4755],\n",
      "        [ 1.1081, -0.2872, -0.5352],\n",
      "        [ 1.2624, -0.2268, -1.5149],\n",
      "        [ 2.0739, -1.5962, -0.9872]], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[-0.0933, -4.0485, -2.6358],\n",
      "        [-0.3654, -1.7607, -2.0087],\n",
      "        [-0.2529, -1.7421, -3.0301],\n",
      "        [-0.0698, -3.7400, -3.1309]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 18.9575, -13.0442,  -6.3324],\n",
      "        [ 18.9219, -13.7115,  -7.6625],\n",
      "        [ 18.6899, -14.1833,  -6.6999],\n",
      "        [ 19.7926, -14.9210,  -6.0559]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 18.9575, -13.0442,  -6.3324],\n",
      "        [ 18.9219, -13.7115,  -7.6625],\n",
      "        [ 18.6899, -14.1833,  -6.6999],\n",
      "        [ 19.7926, -14.9210,  -6.0559]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -32.0017, -25.2900],\n",
      "        [  0.0000, -32.6333, -26.5844],\n",
      "        [  0.0000, -32.8732, -25.3898],\n",
      "        [  0.0000, -34.7136, -25.8485]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.7016, -15.9050,  -7.9630],\n",
      "        [ 18.6390, -16.8143,  -6.2301],\n",
      "        [ 18.5716, -15.4357,  -5.1185],\n",
      "        [ 18.6988, -16.4790,  -6.3037]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.7016, -15.9050,  -7.9630],\n",
      "        [ 18.6390, -16.8143,  -6.2301],\n",
      "        [ 18.5716, -15.4357,  -5.1185],\n",
      "        [ 18.6988, -16.4790,  -6.3037]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -35.6066, -27.6647],\n",
      "        [  0.0000, -35.4534, -24.8691],\n",
      "        [  0.0000, -34.0072, -23.6900],\n",
      "        [  0.0000, -35.1778, -25.0025]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.3020, -16.3367,  -6.9249],\n",
      "        [ 18.0863, -15.6936,  -6.4714],\n",
      "        [ 20.8831, -15.9750,  -6.4057],\n",
      "        [ 17.5353, -15.9932,  -5.3757]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.3020, -16.3367,  -6.9249],\n",
      "        [ 18.0863, -15.6936,  -6.4714],\n",
      "        [ 20.8831, -15.9750,  -6.4057],\n",
      "        [ 17.5353, -15.9932,  -5.3757]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -35.6387, -26.2269],\n",
      "        [  0.0000, -33.7799, -24.5577],\n",
      "        [  0.0000, -36.8581, -27.2888],\n",
      "        [  0.0000, -33.5285, -22.9110]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.4612, -18.5311,  -7.0692],\n",
      "        [ 17.4514, -16.1776,  -5.9871],\n",
      "        [ 17.8875, -18.1035,  -5.6846],\n",
      "        [ 19.6938, -18.1141,  -7.2004]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.4612, -18.5311,  -7.0692],\n",
      "        [ 17.4514, -16.1776,  -5.9871],\n",
      "        [ 17.8875, -18.1035,  -5.6846],\n",
      "        [ 19.6938, -18.1141,  -7.2004]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -37.9922, -26.5303],\n",
      "        [  0.0000, -33.6291, -23.4386],\n",
      "        [  0.0000, -35.9910, -23.5721],\n",
      "        [  0.0000, -37.8079, -26.8942]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 21.0263, -17.2546,  -7.3807],\n",
      "        [ 19.7196, -16.0824,  -5.2096],\n",
      "        [ 18.5708, -17.6870,  -6.0913],\n",
      "        [ 16.7673, -17.2788,  -5.5635]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.0263, -17.2546,  -7.3807],\n",
      "        [ 19.7196, -16.0824,  -5.2096],\n",
      "        [ 18.5708, -17.6870,  -6.0913],\n",
      "        [ 16.7673, -17.2788,  -5.5635]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -38.2809, -28.4070],\n",
      "        [  0.0000, -35.8020, -24.9292],\n",
      "        [  0.0000, -36.2579, -24.6622],\n",
      "        [  0.0000, -34.0461, -22.3308]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 18.3619, -17.5218,  -6.9513],\n",
      "        [ 17.7334, -18.8895,  -6.5805],\n",
      "        [ 19.0399, -18.0639,  -6.1909],\n",
      "        [ 16.6032, -16.8530,  -7.2458]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 18.3619, -17.5218,  -6.9513],\n",
      "        [ 17.7334, -18.8895,  -6.5805],\n",
      "        [ 19.0399, -18.0639,  -6.1909],\n",
      "        [ 16.6032, -16.8530,  -7.2458]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -35.8837, -25.3132],\n",
      "        [  0.0000, -36.6229, -24.3139],\n",
      "        [  0.0000, -37.1038, -25.2308],\n",
      "        [  0.0000, -33.4562, -23.8490]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.6403, -19.4150,  -8.1556],\n",
      "        [ 20.7628, -20.4260,  -6.9605],\n",
      "        [ 20.4725, -19.5822,  -6.1974],\n",
      "        [ 20.1756, -19.9032,  -7.3601]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.6403, -19.4150,  -8.1556],\n",
      "        [ 20.7628, -20.4260,  -6.9605],\n",
      "        [ 20.4725, -19.5822,  -6.1974],\n",
      "        [ 20.1756, -19.9032,  -7.3601]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -39.0552, -27.7959],\n",
      "        [  0.0000, -41.1888, -27.7232],\n",
      "        [  0.0000, -40.0548, -26.6699],\n",
      "        [  0.0000, -40.0787, -27.5357]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 20.6039, -20.3562,  -7.5317],\n",
      "        [ 18.2517, -18.4895,  -7.2396],\n",
      "        [ 19.2696, -18.7270,  -5.3625],\n",
      "        [ 18.6063, -20.9600,  -7.1433]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 20.6039, -20.3562,  -7.5317],\n",
      "        [ 18.2517, -18.4895,  -7.2396],\n",
      "        [ 19.2696, -18.7270,  -5.3625],\n",
      "        [ 18.6063, -20.9600,  -7.1433]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -40.9601, -28.1357],\n",
      "        [  0.0000, -36.7412, -25.4912],\n",
      "        [  0.0000, -37.9966, -24.6322],\n",
      "        [  0.0000, -39.5663, -25.7496]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.1420, -18.6040,  -6.0182],\n",
      "        [ 19.4384, -20.3461,  -7.5667],\n",
      "        [ 19.9583, -20.1868,  -5.9409],\n",
      "        [ 21.3782, -20.0411,  -7.1013]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.1420, -18.6040,  -6.0182],\n",
      "        [ 19.4384, -20.3461,  -7.5667],\n",
      "        [ 19.9583, -20.1868,  -5.9409],\n",
      "        [ 21.3782, -20.0411,  -7.1013]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -37.7459, -25.1601],\n",
      "        [  0.0000, -39.7845, -27.0051],\n",
      "        [  0.0000, -40.1451, -25.8991],\n",
      "        [  0.0000, -41.4194, -28.4796]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.6004, -20.2478,  -7.4371],\n",
      "        [ 21.7364, -21.1459,  -8.1965],\n",
      "        [ 19.3092, -19.3821,  -6.4901],\n",
      "        [ 21.4933, -19.5955,  -7.5651]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.6004, -20.2478,  -7.4371],\n",
      "        [ 21.7364, -21.1459,  -8.1965],\n",
      "        [ 19.3092, -19.3821,  -6.4901],\n",
      "        [ 21.4933, -19.5955,  -7.5651]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -39.8482, -27.0375],\n",
      "        [  0.0000, -42.8823, -29.9330],\n",
      "        [  0.0000, -38.6913, -25.7992],\n",
      "        [  0.0000, -41.0887, -29.0584]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 20.7627, -18.8483,  -7.5668],\n",
      "        [ 20.6127, -20.5880,  -7.3777],\n",
      "        [ 17.6649, -19.9797,  -6.5389],\n",
      "        [ 20.9623, -21.4547,  -7.2272]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 20.7627, -18.8483,  -7.5668],\n",
      "        [ 20.6127, -20.5880,  -7.3777],\n",
      "        [ 17.6649, -19.9797,  -6.5389],\n",
      "        [ 20.9623, -21.4547,  -7.2272]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -39.6110, -28.3295],\n",
      "        [  0.0000, -41.2007, -27.9905],\n",
      "        [  0.0000, -37.6446, -24.2039],\n",
      "        [  0.0000, -42.4171, -28.1895]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 21.2274, -21.3646,  -7.2595],\n",
      "        [ 20.4602, -19.8530,  -7.7108],\n",
      "        [ 21.4854, -20.9368,  -7.7451],\n",
      "        [ 21.9013, -20.5813,  -7.5967]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.2274, -21.3646,  -7.2595],\n",
      "        [ 20.4602, -19.8530,  -7.7108],\n",
      "        [ 21.4854, -20.9368,  -7.7451],\n",
      "        [ 21.9013, -20.5813,  -7.5967]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -42.5920, -28.4869],\n",
      "        [  0.0000, -40.3132, -28.1710],\n",
      "        [  0.0000, -42.4222, -29.2305],\n",
      "        [  0.0000, -42.4826, -29.4980]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 20.1489, -22.7956,  -6.4710],\n",
      "        [ 20.8912, -21.5647,  -7.8131],\n",
      "        [ 21.0385, -21.7423,  -6.8502],\n",
      "        [ 20.4188, -21.9876,  -7.7137]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 20.1489, -22.7956,  -6.4710],\n",
      "        [ 20.8912, -21.5647,  -7.8131],\n",
      "        [ 21.0385, -21.7423,  -6.8502],\n",
      "        [ 20.4188, -21.9876,  -7.7137]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -42.9445, -26.6199],\n",
      "        [  0.0000, -42.4559, -28.7044],\n",
      "        [  0.0000, -42.7808, -27.8887],\n",
      "        [  0.0000, -42.4063, -28.1324]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 20.3652, -20.7907,  -7.3486],\n",
      "        [ 20.2258, -21.4324,  -8.1898],\n",
      "        [ 22.0525, -22.6062,  -8.8879],\n",
      "        [ 19.7080, -20.6045,  -6.6445]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 20.3652, -20.7907,  -7.3486],\n",
      "        [ 20.2258, -21.4324,  -8.1898],\n",
      "        [ 22.0525, -22.6062,  -8.8879],\n",
      "        [ 19.7080, -20.6045,  -6.6445]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -41.1559, -27.7138],\n",
      "        [  0.0000, -41.6581, -28.4155],\n",
      "        [  0.0000, -44.6587, -30.9405],\n",
      "        [  0.0000, -40.3125, -26.3525]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 22.4861, -23.1768,  -8.7276],\n",
      "        [ 20.8697, -19.7706,  -7.4301],\n",
      "        [ 22.5807, -20.7921,  -7.6226],\n",
      "        [ 20.5334, -20.4921,  -8.2179]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.4861, -23.1768,  -8.7276],\n",
      "        [ 20.8697, -19.7706,  -7.4301],\n",
      "        [ 22.5807, -20.7921,  -7.6226],\n",
      "        [ 20.5334, -20.4921,  -8.2179]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -45.6629, -31.2138],\n",
      "        [  0.0000, -40.6404, -28.2998],\n",
      "        [  0.0000, -43.3728, -30.2033],\n",
      "        [  0.0000, -41.0255, -28.7513]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 19.9769, -22.5745,  -8.8266],\n",
      "        [ 21.8273, -20.9343,  -7.7861],\n",
      "        [ 20.9987, -21.2951,  -8.9782],\n",
      "        [ 20.3307, -21.7390,  -9.4534]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 19.9769, -22.5745,  -8.8266],\n",
      "        [ 21.8273, -20.9343,  -7.7861],\n",
      "        [ 20.9987, -21.2951,  -8.9782],\n",
      "        [ 20.3307, -21.7390,  -9.4534]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -42.5514, -28.8035],\n",
      "        [  0.0000, -42.7615, -29.6134],\n",
      "        [  0.0000, -42.2938, -29.9769],\n",
      "        [  0.0000, -42.0697, -29.7841]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 22.1752, -21.7726,  -8.8740],\n",
      "        [ 21.3944, -20.5539,  -8.4125],\n",
      "        [ 21.2713, -22.4065,  -8.4852],\n",
      "        [ 21.8581, -20.9206,  -8.7656]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.1752, -21.7726,  -8.8740],\n",
      "        [ 21.3944, -20.5539,  -8.4125],\n",
      "        [ 21.2713, -22.4065,  -8.4852],\n",
      "        [ 21.8581, -20.9206,  -8.7656]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -43.9478, -31.0493],\n",
      "        [  0.0000, -41.9482, -29.8069],\n",
      "        [  0.0000, -43.6779, -29.7565],\n",
      "        [  0.0000, -42.7787, -30.6237]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 20.8886, -22.0446,  -8.5062],\n",
      "        [ 22.2826, -22.1557,  -7.3194],\n",
      "        [ 21.3736, -22.4655,  -7.9215],\n",
      "        [ 22.0496, -22.2193,  -8.4336]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 20.8886, -22.0446,  -8.5062],\n",
      "        [ 22.2826, -22.1557,  -7.3194],\n",
      "        [ 21.3736, -22.4655,  -7.9215],\n",
      "        [ 22.0496, -22.2193,  -8.4336]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -42.9332, -29.3947],\n",
      "        [  0.0000, -44.4383, -29.6019],\n",
      "        [  0.0000, -43.8391, -29.2951],\n",
      "        [  0.0000, -44.2689, -30.4832]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 21.3588, -22.2370,  -6.8472],\n",
      "        [ 19.6908, -21.9729,  -6.7540],\n",
      "        [ 22.0681, -22.8626,  -8.4110],\n",
      "        [ 23.0115, -23.2496,  -8.9440]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.3588, -22.2370,  -6.8472],\n",
      "        [ 19.6908, -21.9729,  -6.7540],\n",
      "        [ 22.0681, -22.8626,  -8.4110],\n",
      "        [ 23.0115, -23.2496,  -8.9440]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -43.5958, -28.2060],\n",
      "        [  0.0000, -41.6637, -26.4449],\n",
      "        [  0.0000, -44.9307, -30.4791],\n",
      "        [  0.0000, -46.2611, -31.9556]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 23.3233, -21.0999,  -8.8561],\n",
      "        [ 20.8003, -21.8411,  -7.2726],\n",
      "        [ 20.2321, -21.5142,  -6.7270],\n",
      "        [ 21.0954, -24.0800,  -8.5347]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 23.3233, -21.0999,  -8.8561],\n",
      "        [ 20.8003, -21.8411,  -7.2726],\n",
      "        [ 20.2321, -21.5142,  -6.7270],\n",
      "        [ 21.0954, -24.0800,  -8.5347]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -44.4232, -32.1794],\n",
      "        [  0.0000, -42.6414, -28.0729],\n",
      "        [  0.0000, -41.7463, -26.9591],\n",
      "        [  0.0000, -45.1754, -29.6301]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 21.5006, -22.2052,  -8.2979],\n",
      "        [ 21.1089, -22.8062,  -7.6997],\n",
      "        [ 20.6536, -21.2667,  -8.6762],\n",
      "        [ 21.8066, -23.0844,  -8.1231]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.5006, -22.2052,  -8.2979],\n",
      "        [ 21.1089, -22.8062,  -7.6997],\n",
      "        [ 20.6536, -21.2667,  -8.6762],\n",
      "        [ 21.8066, -23.0844,  -8.1231]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -43.7057, -29.7985],\n",
      "        [  0.0000, -43.9151, -28.8086],\n",
      "        [  0.0000, -41.9203, -29.3298],\n",
      "        [  0.0000, -44.8910, -29.9297]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 18.4972, -22.1835,  -6.9772],\n",
      "        [ 21.6914, -21.1846,  -9.2879],\n",
      "        [ 20.9040, -22.9383,  -8.7836],\n",
      "        [ 21.8061, -23.1066,  -8.4577]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 18.4972, -22.1835,  -6.9772],\n",
      "        [ 21.6914, -21.1846,  -9.2879],\n",
      "        [ 20.9040, -22.9383,  -8.7836],\n",
      "        [ 21.8061, -23.1066,  -8.4577]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -40.6807, -25.4745],\n",
      "        [  0.0000, -42.8761, -30.9793],\n",
      "        [  0.0000, -43.8423, -29.6877],\n",
      "        [  0.0000, -44.9127, -30.2638]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 21.5885, -22.1243,  -7.6789],\n",
      "        [ 21.6998, -22.9157,  -7.4915],\n",
      "        [ 21.5375, -20.8942,  -7.2593],\n",
      "        [ 22.6453, -23.8574,  -7.7890]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 21.5885, -22.1243,  -7.6789],\n",
      "        [ 21.6998, -22.9157,  -7.4915],\n",
      "        [ 21.5375, -20.8942,  -7.2593],\n",
      "        [ 22.6453, -23.8574,  -7.7890]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -43.7128, -29.2674],\n",
      "        [  0.0000, -44.6155, -29.1913],\n",
      "        [  0.0000, -42.4317, -28.7968],\n",
      "        [  0.0000, -46.5027, -30.4342]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "sim_scores1:  tensor([[ 22.3031, -21.2729,  -8.4275],\n",
      "        [ 22.1473, -21.8015,  -9.0392],\n",
      "        [ 20.1393, -20.6704,  -8.0232],\n",
      "        [ 18.1470, -19.8829,  -7.3621]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "sim_scores2:  tensor([[ 22.3031, -21.2729,  -8.4275],\n",
      "        [ 22.1473, -21.8015,  -9.0392],\n",
      "        [ 20.1393, -20.6704,  -8.0232],\n",
      "        [ 18.1470, -19.8829,  -7.3621]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "sim_scores3:  tensor([[  0.0000, -43.5760, -30.7306],\n",
      "        [  0.0000, -43.9488, -31.1865],\n",
      "        [  0.0000, -40.8096, -28.1624],\n",
      "        [  0.0000, -38.0299, -25.5091]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "loss: tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sim_scores: tensor([[  0.0000, -43.5760, -30.7306],\n",
      "        [  0.0000, -43.9488, -31.1865],\n",
      "        [  0.0000, -40.8096, -28.1624],\n",
      "        [  0.0000, -38.0299, -25.5091]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "targets: tensor([0, 0, 0, 0], device='cuda:0')\n",
      "p_outputs: tensor([[[-0.2828,  0.2189,  0.3590],\n",
      "         [-0.4554, -0.1614,  0.1250],\n",
      "         [ 0.0780,  0.0511, -0.1944],\n",
      "         ...,\n",
      "         [ 0.4404,  0.2783,  0.3426],\n",
      "         [-0.3670,  0.0518, -0.1551],\n",
      "         [ 0.6381, -0.3617, -0.2321]],\n",
      "\n",
      "        [[-0.2635,  0.3162,  0.3221],\n",
      "         [-0.3746, -0.2633,  0.0335],\n",
      "         [ 0.2125,  0.0111, -0.2120],\n",
      "         ...,\n",
      "         [ 0.5239,  0.1418,  0.3787],\n",
      "         [-0.3427,  0.0350, -0.0782],\n",
      "         [ 0.5831, -0.3120, -0.3704]],\n",
      "\n",
      "        [[-0.1935,  0.2067,  0.2202],\n",
      "         [-0.3663, -0.2757,  0.0735],\n",
      "         [ 0.0890,  0.0329, -0.2128],\n",
      "         ...,\n",
      "         [ 0.4545,  0.2334,  0.3461],\n",
      "         [-0.2655,  0.0806, -0.2142],\n",
      "         [ 0.5106, -0.3753, -0.3272]],\n",
      "\n",
      "        [[-0.3044,  0.1907,  0.2350],\n",
      "         [-0.4767, -0.2779,  0.0664],\n",
      "         [-0.1426,  0.2462, -0.2211],\n",
      "         ...,\n",
      "         [ 0.4860,  0.3116,  0.3844],\n",
      "         [-0.4297, -0.1095, -0.1866],\n",
      "         [ 0.5486, -0.2870, -0.4021]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "q_outputs: tensor([[[ 0.1808,  0.1537,  0.0327,  ...,  0.2288, -0.3302,  0.5529]],\n",
      "\n",
      "        [[ 0.0656,  0.2280,  0.0654,  ...,  0.1543, -0.4032,  0.6550]],\n",
      "\n",
      "        [[ 0.0651,  0.2539, -0.0007,  ...,  0.1649, -0.3408,  0.5014]],\n",
      "\n",
      "        [[ 0.1571,  0.2728,  0.1050,  ...,  0.1525, -0.4152,  0.5329]]\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "print(\"sim_scores1:  tensor([[ 2.0670, -1.8882, -0.4755],\\n        [ 1.1081, -0.2872, -0.5352],\\n        [ 1.2624, -0.2268, -1.5149],\\n        [ 2.0739, -1.5962, -0.9872]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 2.0670, -1.8882, -0.4755],\\n        [ 1.1081, -0.2872, -0.5352],\\n        [ 1.2624, -0.2268, -1.5149],\\n        [ 2.0739, -1.5962, -0.9872]], device='cuda:0', grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[-0.0933, -4.0485, -2.6358],\\n        [-0.3654, -1.7607, -2.0087],\\n        [-0.2529, -1.7421, -3.0301],\\n        [-0.0698, -3.7400, -3.1309]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 18.9575, -13.0442,  -6.3324],\\n        [ 18.9219, -13.7115,  -7.6625],\\n        [ 18.6899, -14.1833,  -6.6999],\\n        [ 19.7926, -14.9210,  -6.0559]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 18.9575, -13.0442,  -6.3324],\\n        [ 18.9219, -13.7115,  -7.6625],\\n        [ 18.6899, -14.1833,  -6.6999],\\n        [ 19.7926, -14.9210,  -6.0559]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -32.0017, -25.2900],\\n        [  0.0000, -32.6333, -26.5844],\\n        [  0.0000, -32.8732, -25.3898],\\n        [  0.0000, -34.7136, -25.8485]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.7016, -15.9050,  -7.9630],\\n        [ 18.6390, -16.8143,  -6.2301],\\n        [ 18.5716, -15.4357,  -5.1185],\\n        [ 18.6988, -16.4790,  -6.3037]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.7016, -15.9050,  -7.9630],\\n        [ 18.6390, -16.8143,  -6.2301],\\n        [ 18.5716, -15.4357,  -5.1185],\\n        [ 18.6988, -16.4790,  -6.3037]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -35.6066, -27.6647],\\n        [  0.0000, -35.4534, -24.8691],\\n        [  0.0000, -34.0072, -23.6900],\\n        [  0.0000, -35.1778, -25.0025]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.3020, -16.3367,  -6.9249],\\n        [ 18.0863, -15.6936,  -6.4714],\\n        [ 20.8831, -15.9750,  -6.4057],\\n        [ 17.5353, -15.9932,  -5.3757]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.3020, -16.3367,  -6.9249],\\n        [ 18.0863, -15.6936,  -6.4714],\\n        [ 20.8831, -15.9750,  -6.4057],\\n        [ 17.5353, -15.9932,  -5.3757]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -35.6387, -26.2269],\\n        [  0.0000, -33.7799, -24.5577],\\n        [  0.0000, -36.8581, -27.2888],\\n        [  0.0000, -33.5285, -22.9110]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.4612, -18.5311,  -7.0692],\\n        [ 17.4514, -16.1776,  -5.9871],\\n        [ 17.8875, -18.1035,  -5.6846],\\n        [ 19.6938, -18.1141,  -7.2004]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.4612, -18.5311,  -7.0692],\\n        [ 17.4514, -16.1776,  -5.9871],\\n        [ 17.8875, -18.1035,  -5.6846],\\n        [ 19.6938, -18.1141,  -7.2004]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -37.9922, -26.5303],\\n        [  0.0000, -33.6291, -23.4386],\\n        [  0.0000, -35.9910, -23.5721],\\n        [  0.0000, -37.8079, -26.8942]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.0263, -17.2546,  -7.3807],\\n        [ 19.7196, -16.0824,  -5.2096],\\n        [ 18.5708, -17.6870,  -6.0913],\\n        [ 16.7673, -17.2788,  -5.5635]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.0263, -17.2546,  -7.3807],\\n        [ 19.7196, -16.0824,  -5.2096],\\n        [ 18.5708, -17.6870,  -6.0913],\\n        [ 16.7673, -17.2788,  -5.5635]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -38.2809, -28.4070],\\n        [  0.0000, -35.8020, -24.9292],\\n        [  0.0000, -36.2579, -24.6622],\\n        [  0.0000, -34.0461, -22.3308]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 18.3619, -17.5218,  -6.9513],\\n        [ 17.7334, -18.8895,  -6.5805],\\n        [ 19.0399, -18.0639,  -6.1909],\\n        [ 16.6032, -16.8530,  -7.2458]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 18.3619, -17.5218,  -6.9513],\\n        [ 17.7334, -18.8895,  -6.5805],\\n        [ 19.0399, -18.0639,  -6.1909],\\n        [ 16.6032, -16.8530,  -7.2458]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -35.8837, -25.3132],\\n        [  0.0000, -36.6229, -24.3139],\\n        [  0.0000, -37.1038, -25.2308],\\n        [  0.0000, -33.4562, -23.8490]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.6403, -19.4150,  -8.1556],\\n        [ 20.7628, -20.4260,  -6.9605],\\n        [ 20.4725, -19.5822,  -6.1974],\\n        [ 20.1756, -19.9032,  -7.3601]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.6403, -19.4150,  -8.1556],\\n        [ 20.7628, -20.4260,  -6.9605],\\n        [ 20.4725, -19.5822,  -6.1974],\\n        [ 20.1756, -19.9032,  -7.3601]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -39.0552, -27.7959],\\n        [  0.0000, -41.1888, -27.7232],\\n        [  0.0000, -40.0548, -26.6699],\\n        [  0.0000, -40.0787, -27.5357]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.6039, -20.3562,  -7.5317],\\n        [ 18.2517, -18.4895,  -7.2396],\\n        [ 19.2696, -18.7270,  -5.3625],\\n        [ 18.6063, -20.9600,  -7.1433]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.6039, -20.3562,  -7.5317],\\n        [ 18.2517, -18.4895,  -7.2396],\\n        [ 19.2696, -18.7270,  -5.3625],\\n        [ 18.6063, -20.9600,  -7.1433]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -40.9601, -28.1357],\\n        [  0.0000, -36.7412, -25.4912],\\n        [  0.0000, -37.9966, -24.6322],\\n        [  0.0000, -39.5663, -25.7496]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.1420, -18.6040,  -6.0182],\\n        [ 19.4384, -20.3461,  -7.5667],\\n        [ 19.9583, -20.1868,  -5.9409],\\n        [ 21.3782, -20.0411,  -7.1013]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.1420, -18.6040,  -6.0182],\\n        [ 19.4384, -20.3461,  -7.5667],\\n        [ 19.9583, -20.1868,  -5.9409],\\n        [ 21.3782, -20.0411,  -7.1013]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -37.7459, -25.1601],\\n        [  0.0000, -39.7845, -27.0051],\\n        [  0.0000, -40.1451, -25.8991],\\n        [  0.0000, -41.4194, -28.4796]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.6004, -20.2478,  -7.4371],\\n        [ 21.7364, -21.1459,  -8.1965],\\n        [ 19.3092, -19.3821,  -6.4901],\\n        [ 21.4933, -19.5955,  -7.5651]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.6004, -20.2478,  -7.4371],\\n        [ 21.7364, -21.1459,  -8.1965],\\n        [ 19.3092, -19.3821,  -6.4901],\\n        [ 21.4933, -19.5955,  -7.5651]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -39.8482, -27.0375],\\n        [  0.0000, -42.8823, -29.9330],\\n        [  0.0000, -38.6913, -25.7992],\\n        [  0.0000, -41.0887, -29.0584]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.7627, -18.8483,  -7.5668],\\n        [ 20.6127, -20.5880,  -7.3777],\\n        [ 17.6649, -19.9797,  -6.5389],\\n        [ 20.9623, -21.4547,  -7.2272]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.7627, -18.8483,  -7.5668],\\n        [ 20.6127, -20.5880,  -7.3777],\\n        [ 17.6649, -19.9797,  -6.5389],\\n        [ 20.9623, -21.4547,  -7.2272]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -39.6110, -28.3295],\\n        [  0.0000, -41.2007, -27.9905],\\n        [  0.0000, -37.6446, -24.2039],\\n        [  0.0000, -42.4171, -28.1895]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.2274, -21.3646,  -7.2595],\\n        [ 20.4602, -19.8530,  -7.7108],\\n        [ 21.4854, -20.9368,  -7.7451],\\n        [ 21.9013, -20.5813,  -7.5967]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.2274, -21.3646,  -7.2595],\\n        [ 20.4602, -19.8530,  -7.7108],\\n        [ 21.4854, -20.9368,  -7.7451],\\n        [ 21.9013, -20.5813,  -7.5967]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.5920, -28.4869],\\n        [  0.0000, -40.3132, -28.1710],\\n        [  0.0000, -42.4222, -29.2305],\\n        [  0.0000, -42.4826, -29.4980]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.1489, -22.7956,  -6.4710],\\n        [ 20.8912, -21.5647,  -7.8131],\\n        [ 21.0385, -21.7423,  -6.8502],\\n        [ 20.4188, -21.9876,  -7.7137]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.1489, -22.7956,  -6.4710],\\n        [ 20.8912, -21.5647,  -7.8131],\\n        [ 21.0385, -21.7423,  -6.8502],\\n        [ 20.4188, -21.9876,  -7.7137]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.9445, -26.6199],\\n        [  0.0000, -42.4559, -28.7044],\\n        [  0.0000, -42.7808, -27.8887],\\n        [  0.0000, -42.4063, -28.1324]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.3652, -20.7907,  -7.3486],\\n        [ 20.2258, -21.4324,  -8.1898],\\n        [ 22.0525, -22.6062,  -8.8879],\\n        [ 19.7080, -20.6045,  -6.6445]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.3652, -20.7907,  -7.3486],\\n        [ 20.2258, -21.4324,  -8.1898],\\n        [ 22.0525, -22.6062,  -8.8879],\\n        [ 19.7080, -20.6045,  -6.6445]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -41.1559, -27.7138],\\n        [  0.0000, -41.6581, -28.4155],\\n        [  0.0000, -44.6587, -30.9405],\\n        [  0.0000, -40.3125, -26.3525]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 22.4861, -23.1768,  -8.7276],\\n        [ 20.8697, -19.7706,  -7.4301],\\n        [ 22.5807, -20.7921,  -7.6226],\\n        [ 20.5334, -20.4921,  -8.2179]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 22.4861, -23.1768,  -8.7276],\\n        [ 20.8697, -19.7706,  -7.4301],\\n        [ 22.5807, -20.7921,  -7.6226],\\n        [ 20.5334, -20.4921,  -8.2179]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -45.6629, -31.2138],\\n        [  0.0000, -40.6404, -28.2998],\\n        [  0.0000, -43.3728, -30.2033],\\n        [  0.0000, -41.0255, -28.7513]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.9769, -22.5745,  -8.8266],\\n        [ 21.8273, -20.9343,  -7.7861],\\n        [ 20.9987, -21.2951,  -8.9782],\\n        [ 20.3307, -21.7390,  -9.4534]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.9769, -22.5745,  -8.8266],\\n        [ 21.8273, -20.9343,  -7.7861],\\n        [ 20.9987, -21.2951,  -8.9782],\\n        [ 20.3307, -21.7390,  -9.4534]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.5514, -28.8035],\\n        [  0.0000, -42.7615, -29.6134],\\n        [  0.0000, -42.2938, -29.9769],\\n        [  0.0000, -42.0697, -29.7841]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 22.1752, -21.7726,  -8.8740],\\n        [ 21.3944, -20.5539,  -8.4125],\\n        [ 21.2713, -22.4065,  -8.4852],\\n        [ 21.8581, -20.9206,  -8.7656]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 22.1752, -21.7726,  -8.8740],\\n        [ 21.3944, -20.5539,  -8.4125],\\n        [ 21.2713, -22.4065,  -8.4852],\\n        [ 21.8581, -20.9206,  -8.7656]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.9478, -31.0493],\\n        [  0.0000, -41.9482, -29.8069],\\n        [  0.0000, -43.6779, -29.7565],\\n        [  0.0000, -42.7787, -30.6237]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.8886, -22.0446,  -8.5062],\\n        [ 22.2826, -22.1557,  -7.3194],\\n        [ 21.3736, -22.4655,  -7.9215],\\n        [ 22.0496, -22.2193,  -8.4336]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.8886, -22.0446,  -8.5062],\\n        [ 22.2826, -22.1557,  -7.3194],\\n        [ 21.3736, -22.4655,  -7.9215],\\n        [ 22.0496, -22.2193,  -8.4336]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.9332, -29.3947],\\n        [  0.0000, -44.4383, -29.6019],\\n        [  0.0000, -43.8391, -29.2951],\\n        [  0.0000, -44.2689, -30.4832]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.3588, -22.2370,  -6.8472],\\n        [ 19.6908, -21.9729,  -6.7540],\\n        [ 22.0681, -22.8626,  -8.4110],\\n        [ 23.0115, -23.2496,  -8.9440]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.3588, -22.2370,  -6.8472],\\n        [ 19.6908, -21.9729,  -6.7540],\\n        [ 22.0681, -22.8626,  -8.4110],\\n        [ 23.0115, -23.2496,  -8.9440]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.5958, -28.2060],\\n        [  0.0000, -41.6637, -26.4449],\\n        [  0.0000, -44.9307, -30.4791],\\n        [  0.0000, -46.2611, -31.9556]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 23.3233, -21.0999,  -8.8561],\\n        [ 20.8003, -21.8411,  -7.2726],\\n        [ 20.2321, -21.5142,  -6.7270],\\n        [ 21.0954, -24.0800,  -8.5347]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 23.3233, -21.0999,  -8.8561],\\n        [ 20.8003, -21.8411,  -7.2726],\\n        [ 20.2321, -21.5142,  -6.7270],\\n        [ 21.0954, -24.0800,  -8.5347]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -44.4232, -32.1794],\\n        [  0.0000, -42.6414, -28.0729],\\n        [  0.0000, -41.7463, -26.9591],\\n        [  0.0000, -45.1754, -29.6301]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.5006, -22.2052,  -8.2979],\\n        [ 21.1089, -22.8062,  -7.6997],\\n        [ 20.6536, -21.2667,  -8.6762],\\n        [ 21.8066, -23.0844,  -8.1231]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.5006, -22.2052,  -8.2979],\\n        [ 21.1089, -22.8062,  -7.6997],\\n        [ 20.6536, -21.2667,  -8.6762],\\n        [ 21.8066, -23.0844,  -8.1231]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.7057, -29.7985],\\n        [  0.0000, -43.9151, -28.8086],\\n        [  0.0000, -41.9203, -29.3298],\\n        [  0.0000, -44.8910, -29.9297]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 18.4972, -22.1835,  -6.9772],\\n        [ 21.6914, -21.1846,  -9.2879],\\n        [ 20.9040, -22.9383,  -8.7836],\\n        [ 21.8061, -23.1066,  -8.4577]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 18.4972, -22.1835,  -6.9772],\\n        [ 21.6914, -21.1846,  -9.2879],\\n        [ 20.9040, -22.9383,  -8.7836],\\n        [ 21.8061, -23.1066,  -8.4577]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -40.6807, -25.4745],\\n        [  0.0000, -42.8761, -30.9793],\\n        [  0.0000, -43.8423, -29.6877],\\n        [  0.0000, -44.9127, -30.2638]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.5885, -22.1243,  -7.6789],\\n        [ 21.6998, -22.9157,  -7.4915],\\n        [ 21.5375, -20.8942,  -7.2593],\\n        [ 22.6453, -23.8574,  -7.7890]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.5885, -22.1243,  -7.6789],\\n        [ 21.6998, -22.9157,  -7.4915],\\n        [ 21.5375, -20.8942,  -7.2593],\\n        [ 22.6453, -23.8574,  -7.7890]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.7128, -29.2674],\\n        [  0.0000, -44.6155, -29.1913],\\n        [  0.0000, -42.4317, -28.7968],\\n        [  0.0000, -46.5027, -30.4342]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 22.3031, -21.2729,  -8.4275],\\n        [ 22.1473, -21.8015,  -9.0392],\\n        [ 20.1393, -20.6704,  -8.0232],\\n        [ 18.1470, -19.8829,  -7.3621]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 22.3031, -21.2729,  -8.4275],\\n        [ 22.1473, -21.8015,  -9.0392],\\n        [ 20.1393, -20.6704,  -8.0232],\\n        [ 18.1470, -19.8829,  -7.3621]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.5760, -30.7306],\\n        [  0.0000, -43.9488, -31.1865],\\n        [  0.0000, -40.8096, -28.1624],\\n        [  0.0000, -38.0299, -25.5091]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nloss: tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\\nsim_scores: tensor([[  0.0000, -43.5760, -30.7306],\\n        [  0.0000, -43.9488, -31.1865],\\n        [  0.0000, -40.8096, -28.1624],\\n        [  0.0000, -38.0299, -25.5091]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\ntargets: tensor([0, 0, 0, 0], device='cuda:0')\\np_outputs: tensor([[[-0.2828,  0.2189,  0.3590],\\n         [-0.4554, -0.1614,  0.1250],\\n         [ 0.0780,  0.0511, -0.1944],\\n         ...,\\n         [ 0.4404,  0.2783,  0.3426],\\n         [-0.3670,  0.0518, -0.1551],\\n         [ 0.6381, -0.3617, -0.2321]],\\n\\n        [[-0.2635,  0.3162,  0.3221],\\n         [-0.3746, -0.2633,  0.0335],\\n         [ 0.2125,  0.0111, -0.2120],\\n         ...,\\n         [ 0.5239,  0.1418,  0.3787],\\n         [-0.3427,  0.0350, -0.0782],\\n         [ 0.5831, -0.3120, -0.3704]],\\n\\n        [[-0.1935,  0.2067,  0.2202],\\n         [-0.3663, -0.2757,  0.0735],\\n         [ 0.0890,  0.0329, -0.2128],\\n         ...,\\n         [ 0.4545,  0.2334,  0.3461],\\n         [-0.2655,  0.0806, -0.2142],\\n         [ 0.5106, -0.3753, -0.3272]],\\n\\n        [[-0.3044,  0.1907,  0.2350],\\n         [-0.4767, -0.2779,  0.0664],\\n         [-0.1426,  0.2462, -0.2211],\\n         ...,\\n         [ 0.4860,  0.3116,  0.3844],\\n         [-0.4297, -0.1095, -0.1866],\\n         [ 0.5486, -0.2870, -0.4021]]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nq_outputs: tensor([[[ 0.1808,  0.1537,  0.0327,  ...,  0.2288, -0.3302,  0.5529]],\\n\\n        [[ 0.0656,  0.2280,  0.0654,  ...,  0.1543, -0.4032,  0.6550]],\\n\\n        [[ 0.0651,  0.2539, -0.0007,  ...,  0.1649, -0.3408,  0.5014]],\\n\\n        [[ 0.1571,  0.2728,  0.1050,  ...,  0.1525, -0.4152,  0.5329]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
