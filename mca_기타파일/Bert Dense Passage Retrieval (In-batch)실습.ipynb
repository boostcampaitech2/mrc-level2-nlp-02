{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDdziEN_VCt"
   },
   "source": [
    "# 5강) BERT를 활용한 Dense Passage Retrieval 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NWluWk3_VCu"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5421,
     "status": "ok",
     "timestamp": 1616574100645,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "eGqFS4EEBF_Z",
    "outputId": "b5b5af1d-0d0d-4197-a717-d2fe3ca2528f"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYUkp06Y_VCv"
   },
   "source": [
    "## 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMrZa4uql_nx"
   },
   "source": [
    "KorQuAD train 데이터셋을 학습 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6098,
     "status": "ok",
     "timestamp": 1616574101330,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "4IUxepuj_VCv",
    "outputId": "6c681d71-4e21-4062-9807-1d975fc901e8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# dataset = load_dataset(\"squad_kor_v1\")\n",
    "dataset = load_from_disk('../../data/train_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtECqpB_VCx"
   },
   "source": [
    "## 토크나이저 준비 - Huggingface 제공 tokenizer 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Fu2WaqpUB8"
   },
   "source": [
    "BERT를 encoder로 사용하므로, hugginface에서 제공하는 \"bert-base-multilingual-cased\" tokenizer를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AoB8BHGDmVIK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "# model_checkpoint = \"klue/roberta-base\"\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8395,
     "status": "ok",
     "timestamp": 1616574103635,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WPxRvMjdvh4y",
    "outputId": "069dd4b9-760a-450b-ea59-096510005e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 9195,
     "status": "ok",
     "timestamp": 1616574104440,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "0U7sn3jsu44O",
    "outputId": "afb5de1c-c5d4-420c-d1f1-f69f56c44cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 미국 상의원 또는 미국 상원 ( United States Senate ) 은 양원제인 미국 의회의 상원이다. [UNK] n [UNK] n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1 / 3씩 상원의원을 새로 선출하여 연방에 보낸다. [UNK] n [UNK] n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할 ( 하원의 법안을 거부할 권한 등 ) 을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션 ( 공공건강보험기관 ) 의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다. 날짜 = 2017 - 02 - 05 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(dataset['train'][0]['context'], padding=\"max_length\", truncation=True)\n",
    "tokenizer.decode(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpoTleVJjp5x"
   },
   "source": [
    "## Dense encoder (BERT) 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nrxmtmfkRVb"
   },
   "source": [
    "HuggingFace BERT를 활용하여 question encoder, passage encoder 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6b215ZfJ_EOc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-bKwkxTpoje"
   },
   "source": [
    "1) Training Dataset 준비하기 (question, passage pairs)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E_FQ1kcazxge"
   },
   "outputs": [],
   "source": [
    "# Use subset (128 example) of original training dataset \n",
    "# sample_idx = np.random.choice(range(len(dataset['train'])), 128)\n",
    "# training_dataset = dataset['train'][sample_idx]\n",
    "\n",
    "training_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NJZWx1b-613e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "# q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt', return_token_type_ids=False)\n",
    "# p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt', return_token_type_ids=False)\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bAplp66Pkayy"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMvVH1e3h99"
   },
   "source": [
    "2) BERT encoder 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW7Oc7Zd9kkm"
   },
   "source": [
    "BertEncoder 모델 정의 후, question encoder, passage encoder에 pre-trained weight 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaPreTrainedModel, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oKKkTlh_l5VL"
   },
   "outputs": [],
   "source": [
    "# class BertEncoder(RobertaPreTrainedModel):\n",
    "class BertEncoder(BertPreTrainedModel):  \n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    # self.bert = RobertaModel(config)\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids #roberta시 주석\n",
    "                          )\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24450,
     "status": "ok",
     "timestamp": 1616574119714,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "wnO1b30SomBP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "998e38f2-0564-4956-bd43-878798158805",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "# p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "# q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "p_encoder = BertEncoder.from_pretrained('../encoders/p_encoder')\n",
    "q_encoder = BertEncoder.from_pretrained('../encoders/q_encoder')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  p_encoder.cuda()\n",
    "  q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in iter(p_encoder.named_parameters()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Dgo8U997HD"
   },
   "source": [
    "Train function 정의 후, 두개의 encoder fine-tuning 하기 (In-batch negative 활용) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VAb7NpUc8YRo"
   },
   "outputs": [],
   "source": [
    "def train(args, dataset, p_model, q_model):\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "  ]\n",
    "\n",
    "  optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    eps=args.adam_epsilon\n",
    "  )\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "# tt\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2] # roberta시 주석\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5] # roberta시 주석\n",
    "                  }\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "      \n",
    "      # print('q_outputs: ',q_outputs)\n",
    "      # print('p_outputs: ',p_outputs)\n",
    "      # print('sim_scores: ',sim_scores)\n",
    "        \n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    "\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "      \n",
    "      \n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    "      print(loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "  return p_model, q_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ICSJoJrUDGZ5"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96513,
     "status": "ok",
     "timestamp": 1616574191784,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "E8a7ww3WgsaZ",
    "outputId": "8f98f6cf-8a44-4e4a-928b-836a1b27a772"
   },
   "outputs": [],
   "source": [
    "p_encoder, q_encoder = tqdm(train(args, train_dataset, p_encoder, q_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.save_pretrained('encoders/p_encoder')\n",
    "q_encoder.save_pretrained('encoders/q_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder = BertEncoder.from_pretrained('../encoders/p_encoder').to('cuda')\n",
    "q_encoder = BertEncoder.from_pretrained('../encoders/q_encoder').to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOw-k7Ln85t"
   },
   "source": [
    "## Dense Embedding을 활용하여 passage retrieval 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96984,
     "status": "ok",
     "timestamp": 1616574192258,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "NouB9uBcTaws",
    "outputId": "f2446b6c-3008-4350-be54-140b194a1a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확증편향이라는 단어를 처음으로 인용한 것은 누구인가?\n",
      "인지심리학의 발달과 함께 확증 편향은 인간의 인지 특징 가운데 하나로 지목되었다. 확증 편향이라는 용어를 처음으로 사용한 사람은 피터 케스카트 왓슨으로 1960년 초기 연구에서 실험 참가자들에게 (2, 4, 6)과 같이 수 세개를 연이어 제시하는 활동을 제시하고 그 결과를 해석하며 개념을 정립하였다 실험자는 \"어떠한 것이든 증가하는 수열\"을 대보라고 요구했지만 참가자들은 그러한 수열을 찾는 것이 힘들어지면 \"가운데 수가 첫 수와 마지막 수의 평균값\" 같은 것으로 규칙을 바꾸고자 했다. 참가자들은 주어진 규칙을 분석하는 것보다 자신이 이미 가정하고 있는 규칙에 맞는 예만 제시하려고 하는 경향을 보였다. \\n\\n왓슨은 자신의 가설이 반증될 수도 있다는 점을 인정하면서 \"확증 편향\"의 개념을 제시했다. 왓슨은 4개의 카드로 이루어진 왓슨 선택 문제를 구상하고 확증 편향의 작동 경향을 분석하였다. 문제는 A이면 B이다 라는 규칙에 따라 부분적으로 주어진 정보를 바탕으로 질문을 하고 가려진 카드의 답을 맞추는 것이었다. 논리적 문제 해결에 익숙한 사람들은 어렵지 않게 문제를 풀었지만, 상당수가 주어진 정보나 규칙을 무시하여 오답을 내었다. 이 경우에도 주어진 정보 보다는 자신이 갖고 있는 심증이 판단에 더 큰 영향을 미쳤다. \\n\\n1987년 클라프만과 하는 왓슨의 실험이 정확히 확증 편향을 보여주는 것은 아니며 추정된 가설을 계속 유지하려는 성향을 보일 뿐이라고 비판하였다. 둘은 이 실험을 \"긍정적 시험 전략\"으로 규정하 의사 결정의 발견적 방법 가운데 하나로 설명하였다. 일종의 오류 확인 알고리즘을 통해 문제를 빠르게 해결하려는 방법으로 본 것이다. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_corpus = list(set([example['context'] for example in dataset['validation']]))#[:10]\n",
    "sample_idx = random.choice(range(len(dataset['validation'])))\n",
    "query = dataset['validation'][sample_idx]['question']\n",
    "ground_truth = dataset['validation'][sample_idx]['context']\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "  valid_corpus.append(ground_truth)\n",
    "\n",
    "print(query)\n",
    "print(ground_truth, '\\n\\n')\n",
    "\n",
    "# valid_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05D8GzFrJhHO"
   },
   "source": [
    "앞서 학습한 passage encoder, question encoder을 이용해 dense embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ba-hH3NQOEWJ"
   },
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "  return tuple(t.cuda() for t in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97547,
     "status": "ok",
     "timestamp": 1616574192826,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "YufA_ayPJBRg",
    "outputId": "fa600009-393a-4f93-871e-bff93675d05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([235, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  p_encoder.eval()\n",
    "  q_encoder.eval()\n",
    "\n",
    "  q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "  # q_seqs_val = tokenizer(['해바라기는 무슨 꽃일까?'], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "  q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)\n",
    "\n",
    "  p_embs = []\n",
    "  for p in valid_corpus:\n",
    "    p = tokenizer(p, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "    p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "    p_embs.append(p_emb)\n",
    "\n",
    "p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "\n",
    "print(p_embs.size(), q_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOHHak7WS1ko"
   },
   "source": [
    "생성된 embedding에 dot product를 수행 => Document들의 similarity ranking을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97546,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "xn5Cx5JkKZJB",
    "outputId": "eb232f4f-bdae-474d-b630-a13d0b563364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 235])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.size())\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "# print(dot_prod_scores)\n",
    "# print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2Oiv8MKVS6"
   },
   "source": [
    "Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97544,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WaStRXYdJ-wI",
    "outputId": "bbf42d40-dfae-49ff-bf12-139452b5849f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 확증편향이라는 단어를 처음으로 인용한 것은 누구인가? \n",
      "\n",
      "[Ground truth passage]\n",
      "인지심리학의 발달과 함께 확증 편향은 인간의 인지 특징 가운데 하나로 지목되었다. 확증 편향이라는 용어를 처음으로 사용한 사람은 피터 케스카트 왓슨으로 1960년 초기 연구에서 실험 참가자들에게 (2, 4, 6)과 같이 수 세개를 연이어 제시하는 활동을 제시하고 그 결과를 해석하며 개념을 정립하였다 실험자는 \"어떠한 것이든 증가하는 수열\"을 대보라고 요구했지만 참가자들은 그러한 수열을 찾는 것이 힘들어지면 \"가운데 수가 첫 수와 마지막 수의 평균값\" 같은 것으로 규칙을 바꾸고자 했다. 참가자들은 주어진 규칙을 분석하는 것보다 자신이 이미 가정하고 있는 규칙에 맞는 예만 제시하려고 하는 경향을 보였다. \\n\\n왓슨은 자신의 가설이 반증될 수도 있다는 점을 인정하면서 \"확증 편향\"의 개념을 제시했다. 왓슨은 4개의 카드로 이루어진 왓슨 선택 문제를 구상하고 확증 편향의 작동 경향을 분석하였다. 문제는 A이면 B이다 라는 규칙에 따라 부분적으로 주어진 정보를 바탕으로 질문을 하고 가려진 카드의 답을 맞추는 것이었다. 논리적 문제 해결에 익숙한 사람들은 어렵지 않게 문제를 풀었지만, 상당수가 주어진 정보나 규칙을 무시하여 오답을 내었다. 이 경우에도 주어진 정보 보다는 자신이 갖고 있는 심증이 판단에 더 큰 영향을 미쳤다. \\n\\n1987년 클라프만과 하는 왓슨의 실험이 정확히 확증 편향을 보여주는 것은 아니며 추정된 가설을 계속 유지하려는 성향을 보일 뿐이라고 비판하였다. 둘은 이 실험을 \"긍정적 시험 전략\"으로 규정하 의사 결정의 발견적 방법 가운데 하나로 설명하였다. 일종의 오류 확인 알고리즘을 통해 문제를 빠르게 해결하려는 방법으로 본 것이다. \n",
      "\n",
      "Top-1 passage with score 256.5248\n",
      "인지심리학의 발달과 함께 확증 편향은 인간의 인지 특징 가운데 하나로 지목되었다. 확증 편향이라는 용어를 처음으로 사용한 사람은 피터 케스카트 왓슨으로 1960년 초기 연구에서 실험 참가자들에게 (2, 4, 6)과 같이 수 세개를 연이어 제시하는 활동을 제시하고 그 결과를 해석하며 개념을 정립하였다 실험자는 \"어떠한 것이든 증가하는 수열\"을 대보라고 요구했지만 참가자들은 그러한 수열을 찾는 것이 힘들어지면 \"가운데 수가 첫 수와 마지막 수의 평균값\" 같은 것으로 규칙을 바꾸고자 했다. 참가자들은 주어진 규칙을 분석하는 것보다 자신이 이미 가정하고 있는 규칙에 맞는 예만 제시하려고 하는 경향을 보였다. \\n\\n왓슨은 자신의 가설이 반증될 수도 있다는 점을 인정하면서 \"확증 편향\"의 개념을 제시했다. 왓슨은 4개의 카드로 이루어진 왓슨 선택 문제를 구상하고 확증 편향의 작동 경향을 분석하였다. 문제는 A이면 B이다 라는 규칙에 따라 부분적으로 주어진 정보를 바탕으로 질문을 하고 가려진 카드의 답을 맞추는 것이었다. 논리적 문제 해결에 익숙한 사람들은 어렵지 않게 문제를 풀었지만, 상당수가 주어진 정보나 규칙을 무시하여 오답을 내었다. 이 경우에도 주어진 정보 보다는 자신이 갖고 있는 심증이 판단에 더 큰 영향을 미쳤다. \\n\\n1987년 클라프만과 하는 왓슨의 실험이 정확히 확증 편향을 보여주는 것은 아니며 추정된 가설을 계속 유지하려는 성향을 보일 뿐이라고 비판하였다. 둘은 이 실험을 \"긍정적 시험 전략\"으로 규정하 의사 결정의 발견적 방법 가운데 하나로 설명하였다. 일종의 오류 확인 알고리즘을 통해 문제를 빠르게 해결하려는 방법으로 본 것이다.\n",
      "Top-2 passage with score 256.2302\n",
      "여의도는 원래 불모지로, 군사 훈련장으로 쓰이기도 했다. 일제는 표면상으로는 민간항로 개설에 사용한다는 명분을 내세웠으나, 실제로는 중국 대륙 침략의 교두보로서 한반도에 유사시 사용할 비행장이 필요했기 때문에 여의도에 비행장을 만들었다. 이것이 한국 최초의 비행장이다. \\n\\n1916년 개장 당시, 여의도 비행장에는 활주로와 격납고만 있었다. 당시에는 무선시설이나 각종 계기가 발달하지 못했고, 비행기가 뜨고 내릴 수만 있으면 족했기 때문에 다른 부대 시설이 필요하지 않았다.\\n\\n개장 초기에 여의도 비행장은 매우 한산했다. 1913년 용산 연병장에서 일본이 개최한 순회 공개 비행대회에서 처음으로 비행기를 접한 주민들의 비행기에 대한 호기심은 대단했으나, 여의도 비행장에서 비행기를 볼 수 있는 경우는 드물었기 때문에 구경 나왔다가 실망하기 일쑤였다.\\n\\n* 1917년 5월 세계적인 곡예 비행사인 미국인 아트 스미스가 곡예 비행을 선보였는데, 순회 공개 비행대회 때 일본 해군 기술장교 나라하라가 겨우 30초 정도 공중 비행에 매우 신기해 했던 조선인들에게 곡예 비행에 대한 경이감은 대단했다. 당시 서울 시민 20만여 명 가운데 5만여 명이 아트 스미스의 비행을 보기 위해 여의도 비행장에 모였다고 한다.\\n* 1922년 12월 10일, 한국 최초의 비행사로 알려진 안창남이 이곳에서 시범비행을 보였다.\\n\\n\\n한국전쟁이 끝나고 1954년 4월 26일 여의도 국제공항이 정식 개항하였다. 하지만 홍수에 취약했던 여의도의 국제공항 기능은 1961년 김포로 옮겨갔다. . 1964년 4월에는 국내 항공노선까지 김포공항으로 이전함에 따라 여의도 민간 항공 시대는 완전히 끝이 났다.\\n\\n1970년대에 여의도를 개발하면서 당시 대통령 박정희의 지시로 기존의 활주로 자리에 5·16 광장(여의도 광장)을 만든 것은 유사시에 활주로로 쓸 수 있게 하기 위해서였다고 한다.\n",
      "Top-3 passage with score 255.7465\n",
      "1778년 초, 설리번은 로드아일랜드로 전출되어 그곳의 부대와 민병대를 지휘하게 되었다. 그것은 프랑스의 참전이후 난공불락으로 여겨지는 뉴포트를 장악한 영국군을 프랑스 해군과 연계하여 공격하거나 포위하려는 의도였다. 그러나 데스탱 장군이 이끄는 함대가 폭우로 손실되어 그 계획은 취소되었다. 손실된 배와 하우 제독이 이끄는 영국 함대의 도착으로 데스탱은 보스톤으로 철수하였다. 그때 뉴포트의 영국군 요새에 있던 영국군이 출동하였고, 설리번도 8월에 일어난 영양가 없었던 로드아일랜드 전투에서 교전을 벌인 후 퇴각했다.\\n\\n난공불락으로 보였던 요새 공략에 실패하고, 작전이 붕괴된 방식으로 인해 프랑스와 미합중국 간의 관계가 불편해지게 되었다. 설리반은 데스탱 장군에게 편지를 써서 그가 목격했던 것을 ‘프랑스의 명예를 손상시키는’이라는 표현을 써가며 배반적이고, 비겁한 짓이라고 항의했다. 이 작전 실패로 두 연합국 간의 국제 분쟁에 불이 붙었으며, 1년 뒤 사바나 포위전에서 영국 요새에 대한 또 다른 실패를 불러왔다. 설리번은 이 실패로 그의 이력에 큰 타격을 입지는 않았으며, 그는 조심스런 캐나다 침공을 위한 지휘관으로 오르내리고 있었다.\n",
      "Top-4 passage with score 255.5330\n",
      "러셀은 1890년 케임브리지 대학교의 트리니티 칼리지에 장학생으로 들어갔다. 그는 자신보다 어린 조지 에드워드 무어와 아는 사이가 되었으며, 화이트헤드에게 비밀 동아리 케임브리지 아포슬스를 추천받으며 영향을 받는다.\\n\\n러셀은 수학과 철학에서 두각을 나타내어, 1893년 학교를 최우등 졸업생으로 졸업하고 1895년 선임연구원(Fellow)이 된다. \\n\\n러셀은 17세에 처음으로 퀘이커 교도였던 앨리스 페어살 스미스와 만났으며, 그녀의 가족과도 친밀한 관계가 되었다. 그녀의 가족들은 러셀이 존 러셀 백작의 손자라는 것을 알고 있었으며, 이후 러셀과 같이 대륙을 돌며 여행하기를 즐겼다. 그들과 함께 러셀은 1889년 파리 박람회에 참가해 당시 갓 건축된 에펠 탑에 올라 본다. \\n\\n이후 그는 청교도적인 가치관을 가졌던 앨리스와 사랑에 빠지며, 할머니의 반대를 무릅쓰고 1894년 12월 13일 결혼했다. 그러나 둘은 1901년 함께 자전거를 타다가 러셀이 앨리스에게 자신이 그녀를 더 이상 사랑하지 않는다는 것을 고백하며 파경에 빠진다. 러셀은 앨리스의 어머니가 잔인하게 그를 조종하려고 하였기에 싫어하기도 하였으며, 이후 1921년까지 별거하며 형식적으로만 부부로 남아있다가 이혼했다. 이 기간동안 러셀은 오톨린 모렐과 배우 콘스턴스 말레슨 등 여러 사람들과 열애 관계에 빠졌다.\n",
      "Top-5 passage with score 255.3938\n",
      "1888년, 교사로 재직하던 조지 새포트 파커(George Safford Parker)는 부업으로 만년필을 판매했다. 그런데 당시의 만년필들은 잉크가 자주 새는 문제점이 있어 고객들은 늘 불만이었고,결국 그는 자신이 직접 만년필을 만들기로 결심한다. 계속되는 연구와 실패를 거듭한 끝에 1894년, 잉크 유출방지 기술인 ‘Lucky Curve’ 시스템을 발명하여 특허를 받게 된다. 이것이 오늘날 파커의 탄생인 것이다.\\n\\n필기는 역사를 발전시키는 중요한 수단이 되었으며 파커는 그 중심에 서 있었다. 20세기 초 미국경제가 급성장하는 동안, 만년필은 단순한 필기구에서 나아가 개인의 스타일을 대변해 주는 액세서리가 되었다. 파커 역시 금과 은은 물론 진주 등으로 세공한 만년필로 트렌드를 이끌었다. 그 당시 읽고 쓸 수 있었던 사람은 지식인층으로 한정되어 만년필의 소유 자체가 교양 있는 사람임을 보여주는 상징이었다.\\n그 후 제 1차 세계 대전으로 미국은 경제 위기를 맞이하게 되지만 반면 필기구 산업은 전성기를 맞게 된다. 가족들에게 편지를 쓰기 위해 펜이 필요했던 군인들에게 파커가 펜을 제공하였기 때문이다. 전쟁을 거치며 큰 성장을 이룩한 파커는 끊임없는 연구를 통해 파커 듀오폴드, 파커 51 등 그 당시를 대표하는 아이콘적인 모델을 선보이며 그 명성을 이어가게 된다.\\n\\n파커 펜은 100여 년 동안 경제계, 정계 그리고 예술계의 리더들로부터 사랑받아왔다. 셜록 홈즈의 작가 아서 코난 도일경은 1920년대에 “내가 평생 찾던 펜은 바로 파커”라고 할 만큼 파커 펜을 애용했고, 마거릿 대처 전 영국수상은 1988년 파커의 영국 공장을 방문하고 “어린시절, 파커 펜을 갖는 것은 나의 꿈이었으며, 지금도 모든 서명에 G7 정상회담 때 받은 듀오폴드 펜을 사용한다.” 라고 밝힌 바 있다. 또한 파커 펜은 역사적으로 중요한 순간에도 사용되었다. 맥아더 장군은 태평양 전쟁 당시 일본의 항복문서에 서명할 때 그의 20년 된 파커 듀오폴드 펜을 사용했다. 1990년 6월 부시 대통령과 고르바쵸프 대통령은 백악관에서 파커 펜으로 자유무역 협정을 체결하여 사실상 냉전을 종식 시켰고, 영국 왕실은 파커를 공식 지정 펜으로 사용하고 있다.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(\"Top-%d passage with score %.4f\" % (i+1, dot_prod_scores.squeeze()[rank[i]]))\n",
    "  print(valid_corpus[rank[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBaKYpdoXDcW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MRC Practice 5 - Dense Passage Retrieval (In-batch)",
   "provenance": [
    {
     "file_id": "1c9Vr7z_LBG2l9K4lVb40pu7Kk22hXQCp",
     "timestamp": 1614240569955
    },
    {
     "file_id": "1Q7iAXm_kwF_NHfOEGdViMCiPHnqoZlXe",
     "timestamp": 1613491158162
    }
   ]
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
