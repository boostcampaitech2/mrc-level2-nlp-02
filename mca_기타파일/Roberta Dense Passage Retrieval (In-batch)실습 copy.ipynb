{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDdziEN_VCt"
   },
   "source": [
    "# 5강) BERT를 활용한 Dense Passage Retrieval 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NWluWk3_VCu"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5421,
     "status": "ok",
     "timestamp": 1616574100645,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "eGqFS4EEBF_Z",
    "outputId": "b5b5af1d-0d0d-4197-a717-d2fe3ca2528f"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYUkp06Y_VCv"
   },
   "source": [
    "## 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMrZa4uql_nx"
   },
   "source": [
    "KorQuAD train 데이터셋을 학습 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6098,
     "status": "ok",
     "timestamp": 1616574101330,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "4IUxepuj_VCv",
    "outputId": "6c681d71-4e21-4062-9807-1d975fc901e8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# dataset = load_dataset(\"squad_kor_v1\")\n",
    "dataset = load_from_disk('../../data/train_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtECqpB_VCx"
   },
   "source": [
    "## 토크나이저 준비 - Huggingface 제공 tokenizer 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Fu2WaqpUB8"
   },
   "source": [
    "BERT를 encoder로 사용하므로, hugginface에서 제공하는 \"bert-base-multilingual-cased\" tokenizer를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AoB8BHGDmVIK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "# model_checkpoint = \"klue/roberta-base\"\n",
    "\n",
    "model_checkpoint = 'Huffon/sentence-klue-roberta-base'\n",
    "\n",
    "# model_checkpoint = \"klue/bert-base\"\n",
    "# model_checkpoint = \"xlm-roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8395,
     "status": "ok",
     "timestamp": 1616574103635,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WPxRvMjdvh4y",
    "outputId": "069dd4b9-760a-450b-ea59-096510005e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='Huffon/sentence-klue-roberta-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 9195,
     "status": "ok",
     "timestamp": 1616574104440,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "0U7sn3jsu44O",
    "outputId": "afb5de1c-c5d4-420c-d1f1-f69f56c44cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 미국 상의원 또는 미국 상원 ( United States Senate ) 은 양원제인 미국 의회의 상원이다. [UNK] n [UNK] n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1 / 3씩 상원의원을 새로 선출하여 연방에 보낸다. [UNK] n [UNK] n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할 ( 하원의 법안을 거부할 권한 등 ) 을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션 ( 공공건강보험기관 ) 의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다. 날짜 = 2017 - 02 - 05 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(dataset['train'][0]['context'], padding=\"max_length\", truncation=True, max_length=510)\n",
    "tokenizer.decode(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpoTleVJjp5x"
   },
   "source": [
    "## Dense encoder (BERT) 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nrxmtmfkRVb"
   },
   "source": [
    "HuggingFace BERT를 활용하여 question encoder, passage encoder 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6b215ZfJ_EOc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup, RobertaForSequenceClassification\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-bKwkxTpoje"
   },
   "source": [
    "1) Training Dataset 준비하기 (question, passage pairs)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E_FQ1kcazxge"
   },
   "outputs": [],
   "source": [
    "# Use subset (128 example) of original training dataset \n",
    "# sample_idx = np.random.choice(range(len(dataset['train'])), 128)\n",
    "# training_dataset = dataset['train'][sample_idx]\n",
    "\n",
    "training_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NJZWx1b-613e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt', return_token_type_ids=False, max_length=510)\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt', return_token_type_ids=False, max_length=510)\n",
    "# q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "# p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bAplp66Pkayy"
   },
   "outputs": [],
   "source": [
    "# train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "#                         q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'],)\n",
    "\n",
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'],\n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMvVH1e3h99"
   },
   "source": [
    "2) BERT encoder 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW7Oc7Zd9kkm"
   },
   "source": [
    "BertEncoder 모델 정의 후, question encoder, passage encoder에 pre-trained weight 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaPreTrainedModel, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oKKkTlh_l5VL"
   },
   "outputs": [],
   "source": [
    "class RobertaEncoder(RobertaPreTrainedModel):\n",
    "# class BertEncoder(BertPreTrainedModel):  \n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.roberta = RobertaModel(config)\n",
    "    # self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.roberta(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids #roberta시 주석\n",
    "                          )\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24450,
     "status": "ok",
     "timestamp": 1616574119714,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "wnO1b30SomBP",
    "outputId": "998e38f2-0564-4956-bd43-878798158805",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "p_encoder = RobertaEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = RobertaEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "# p_encoder = RobertaForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "# q_encoder = RobertaForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  p_encoder.cuda()\n",
    "  q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# q_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Dgo8U997HD"
   },
   "source": [
    "Train function 정의 후, 두개의 encoder fine-tuning 하기 (In-batch negative 활용) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VAb7NpUc8YRo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(args, dataset, p_model, q_model):\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "  ]\n",
    "\n",
    "  optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    eps=args.adam_epsilon\n",
    "  )\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "# tt\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  # 'token_type_ids': batch[2] # roberta시 주석\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[2],\n",
    "                  'attention_mask': batch[3],\n",
    "                  # 'token_type_ids': batch[5] # roberta시 주석\n",
    "                  }\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "      \n",
    "      # print('q_outputs: ',q_outputs)\n",
    "      # print('p_outputs: ',p_outputs)\n",
    "      # print('sim_scores: ',sim_scores)\n",
    "    \n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    "\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets) \n",
    "      print(loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "  return p_model, q_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ICSJoJrUDGZ5"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96513,
     "status": "ok",
     "timestamp": 1616574191784,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "E8a7ww3WgsaZ",
    "outputId": "8f98f6cf-8a44-4e4a-928b-836a1b27a772"
   },
   "outputs": [],
   "source": [
    "p_encoder, q_encoder = tqdm(train(args, train_dataset, p_encoder, q_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.save_pretrained('../encoders/p_encoder')\n",
    "q_encoder.save_pretrained('../encoders/q_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../encoders/p_encoder were not used when initializing RobertaEncoder: ['bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaEncoder were not initialized from the model checkpoint at ../encoders/p_encoder and are newly initialized: ['encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'embeddings.position_ids', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../encoders/q_encoder were not used when initializing RobertaEncoder: ['bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaEncoder were not initialized from the model checkpoint at ../encoders/q_encoder and are newly initialized: ['encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'embeddings.position_ids', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaEncoder(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_encoder.from_pretrained('../encoders/p_encoder')\n",
    "q_encoder.from_pretrained('../encoders/q_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to('cuda')\n",
    "q_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOw-k7Ln85t"
   },
   "source": [
    "## Dense Embedding을 활용하여 passage retrieval 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96984,
     "status": "ok",
     "timestamp": 1616574192258,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "NouB9uBcTaws",
    "outputId": "f2446b6c-3008-4350-be54-140b194a1a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "루이 14세의 왕비 마리아 테래사는 어느 나라 공주인가?\n",
      "예부터 노트르담 다리는 큰 다리(Grand-pont)라고 불리면서 센 강을 가로지르는 교통 수단의 역할을 하였다. 이후 886년 노르만족이 파리 지역을 침공했을 당시 구조가 파괴되어 다시 지어졌다. 다시 지어진 다리는 밀브레 다리라고 불리기도 하였다. 1406년 대홍수 동안 다리는 다시 유실된다.\\n\\n1412년 5월 3일 샤를 6세가 부지에 다리의 골격을 새로 정비하고 최초로 노트르담이라는 이름을 하사한다. 그가 정비토록 지시한 다리의 구조는 견고한 목재를 통해 쌩마르탱 가와 다른 곳을 연결하도록 되어 있었다. 이때 다리 건축에만 7년이 소요되었으며 양옆으로는 각각 30여 가구가 있었다고 한다. 그러나 이 다리는 1499년 10월 25일 아침 9시경 지반 침하와 정비 부실로 붕괴된다.\\n\\n석재로의 다리 건축이 같은 해 시작되었지만 당분간 주민들은 연락선을 타고 센 강을 건너다녔다. 이 시기에는 아치 형으로 된 석재 다리가 지어졌으며 이탈리아 출신의 건축가이자 철학자였던 프라 지오반니가 건축을 맡았다. 그의 건축은 1507년에 완공되었으며 여전히 당시의 60여 개 벽돌과 석재는 보존되고 있다. 이후 상권의 중심으로 떠오른 다리 인근은 사람들이 모이는 곳으로 성장했다.\\n\\n1660년 노트르담 다리는 스페인 펠리페 4세의 딸이었던 마리아 테레사가 프랑스의 루이 14세의 왕비로 발탁되어 파리로 들어올 당시 최초의 다리가 되는 영예를 안기도 한다. 1646~1788년 동안 다리 인근의 가옥이 모두 도시 정비의 일환으로 파괴되었다.\\n\\n 1853년 새로운 석재 구조로 기존의 돌다리를 덮었다. 하지만 기존의 예술적 아름다움은 상당부분 경감되고 만다. 새로운 다리는 아치형으로 지어진 것이었지만 작은 흠이 있었다. 그것은 건축 보수 이후 연락선이 지나다니다 빈번히 사고가 난 것이었다. 때문에 1891년에서 1910년 사이 너무도 잦은 사고 탓에 사람들은 이곳을 악마의 다리라고 부르기도 했다고 한다. 때문에 센 강변의 유람선이나 다른 배의 운송 편의를 위해 금속으로 다시 만드는 계획이 수립된다. 이 작업은 미라보 다리를 만들었던 장 르살이 맡았다. 1919년 프랑스 공화국 대통령이었던 푸앵카레가 참석한 가운데 다리가 개통되었다. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_corpus = list(set([example['context'] for example in dataset['validation']]))#[:10]\n",
    "sample_idx = random.choice(range(len(dataset['validation'])))\n",
    "query = dataset['validation'][sample_idx]['question']\n",
    "ground_truth = dataset['validation'][sample_idx]['context']\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "  valid_corpus.append(ground_truth)\n",
    "\n",
    "print(query)\n",
    "print(ground_truth, '\\n\\n')\n",
    "\n",
    "# valid_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05D8GzFrJhHO"
   },
   "source": [
    "앞서 학습한 passage encoder, question encoder을 이용해 dense embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "ba-hH3NQOEWJ"
   },
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "  return tuple(t.cuda() for t in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97547,
     "status": "ok",
     "timestamp": 1616574192826,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "YufA_ayPJBRg",
    "outputId": "fa600009-393a-4f93-871e-bff93675d05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([235, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  p_encoder.eval()\n",
    "  q_encoder.eval()\n",
    "\n",
    "  # q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "  q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=510).to('cuda')\n",
    "  q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)\n",
    "\n",
    "  p_embs = []\n",
    "  for p in valid_corpus:\n",
    "    p = tokenizer(p, padding=\"max_length\", truncation=True, return_tensors='pt', max_length=510).to('cuda')\n",
    "    p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "    p_embs.append(p_emb)\n",
    "\n",
    "p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "\n",
    "print(p_embs.size(), q_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOHHak7WS1ko"
   },
   "source": [
    "생성된 embedding에 dot product를 수행 => Document들의 similarity ranking을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97546,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "xn5Cx5JkKZJB",
    "outputId": "eb232f4f-bdae-474d-b630-a13d0b563364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 235])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.size())\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "# print(dot_prod_scores)\n",
    "# print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2Oiv8MKVS6"
   },
   "source": [
    "Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97544,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WaStRXYdJ-wI",
    "outputId": "bbf42d40-dfae-49ff-bf12-139452b5849f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 루이 14세의 왕비 마리아 테래사는 어느 나라 공주인가? \n",
      "\n",
      "[Ground truth passage]\n",
      "예부터 노트르담 다리는 큰 다리(Grand-pont)라고 불리면서 센 강을 가로지르는 교통 수단의 역할을 하였다. 이후 886년 노르만족이 파리 지역을 침공했을 당시 구조가 파괴되어 다시 지어졌다. 다시 지어진 다리는 밀브레 다리라고 불리기도 하였다. 1406년 대홍수 동안 다리는 다시 유실된다.\\n\\n1412년 5월 3일 샤를 6세가 부지에 다리의 골격을 새로 정비하고 최초로 노트르담이라는 이름을 하사한다. 그가 정비토록 지시한 다리의 구조는 견고한 목재를 통해 쌩마르탱 가와 다른 곳을 연결하도록 되어 있었다. 이때 다리 건축에만 7년이 소요되었으며 양옆으로는 각각 30여 가구가 있었다고 한다. 그러나 이 다리는 1499년 10월 25일 아침 9시경 지반 침하와 정비 부실로 붕괴된다.\\n\\n석재로의 다리 건축이 같은 해 시작되었지만 당분간 주민들은 연락선을 타고 센 강을 건너다녔다. 이 시기에는 아치 형으로 된 석재 다리가 지어졌으며 이탈리아 출신의 건축가이자 철학자였던 프라 지오반니가 건축을 맡았다. 그의 건축은 1507년에 완공되었으며 여전히 당시의 60여 개 벽돌과 석재는 보존되고 있다. 이후 상권의 중심으로 떠오른 다리 인근은 사람들이 모이는 곳으로 성장했다.\\n\\n1660년 노트르담 다리는 스페인 펠리페 4세의 딸이었던 마리아 테레사가 프랑스의 루이 14세의 왕비로 발탁되어 파리로 들어올 당시 최초의 다리가 되는 영예를 안기도 한다. 1646~1788년 동안 다리 인근의 가옥이 모두 도시 정비의 일환으로 파괴되었다.\\n\\n 1853년 새로운 석재 구조로 기존의 돌다리를 덮었다. 하지만 기존의 예술적 아름다움은 상당부분 경감되고 만다. 새로운 다리는 아치형으로 지어진 것이었지만 작은 흠이 있었다. 그것은 건축 보수 이후 연락선이 지나다니다 빈번히 사고가 난 것이었다. 때문에 1891년에서 1910년 사이 너무도 잦은 사고 탓에 사람들은 이곳을 악마의 다리라고 부르기도 했다고 한다. 때문에 센 강변의 유람선이나 다른 배의 운송 편의를 위해 금속으로 다시 만드는 계획이 수립된다. 이 작업은 미라보 다리를 만들었던 장 르살이 맡았다. 1919년 프랑스 공화국 대통령이었던 푸앵카레가 참석한 가운데 다리가 개통되었다. \n",
      "\n",
      "Top-1 passage with score 30.2412\n",
      "예부터 노트르담 다리는 큰 다리(Grand-pont)라고 불리면서 센 강을 가로지르는 교통 수단의 역할을 하였다. 이후 886년 노르만족이 파리 지역을 침공했을 당시 구조가 파괴되어 다시 지어졌다. 다시 지어진 다리는 밀브레 다리라고 불리기도 하였다. 1406년 대홍수 동안 다리는 다시 유실된다.\\n\\n1412년 5월 3일 샤를 6세가 부지에 다리의 골격을 새로 정비하고 최초로 노트르담이라는 이름을 하사한다. 그가 정비토록 지시한 다리의 구조는 견고한 목재를 통해 쌩마르탱 가와 다른 곳을 연결하도록 되어 있었다. 이때 다리 건축에만 7년이 소요되었으며 양옆으로는 각각 30여 가구가 있었다고 한다. 그러나 이 다리는 1499년 10월 25일 아침 9시경 지반 침하와 정비 부실로 붕괴된다.\\n\\n석재로의 다리 건축이 같은 해 시작되었지만 당분간 주민들은 연락선을 타고 센 강을 건너다녔다. 이 시기에는 아치 형으로 된 석재 다리가 지어졌으며 이탈리아 출신의 건축가이자 철학자였던 프라 지오반니가 건축을 맡았다. 그의 건축은 1507년에 완공되었으며 여전히 당시의 60여 개 벽돌과 석재는 보존되고 있다. 이후 상권의 중심으로 떠오른 다리 인근은 사람들이 모이는 곳으로 성장했다.\\n\\n1660년 노트르담 다리는 스페인 펠리페 4세의 딸이었던 마리아 테레사가 프랑스의 루이 14세의 왕비로 발탁되어 파리로 들어올 당시 최초의 다리가 되는 영예를 안기도 한다. 1646~1788년 동안 다리 인근의 가옥이 모두 도시 정비의 일환으로 파괴되었다.\\n\\n 1853년 새로운 석재 구조로 기존의 돌다리를 덮었다. 하지만 기존의 예술적 아름다움은 상당부분 경감되고 만다. 새로운 다리는 아치형으로 지어진 것이었지만 작은 흠이 있었다. 그것은 건축 보수 이후 연락선이 지나다니다 빈번히 사고가 난 것이었다. 때문에 1891년에서 1910년 사이 너무도 잦은 사고 탓에 사람들은 이곳을 악마의 다리라고 부르기도 했다고 한다. 때문에 센 강변의 유람선이나 다른 배의 운송 편의를 위해 금속으로 다시 만드는 계획이 수립된다. 이 작업은 미라보 다리를 만들었던 장 르살이 맡았다. 1919년 프랑스 공화국 대통령이었던 푸앵카레가 참석한 가운데 다리가 개통되었다.\n",
      "Top-2 passage with score 30.1994\n",
      "프랑스의 십자군 무훈시는 1099년 예루살렘 왕국의 통치자가 된 고드프루아 드 부용의 전설적인 선조로 백조 기사를 등장시킨다. 고드프루아는 중세 기독교 세계에서 전설적인 인물이 되었고 그의 신화적 혈통은 중세 작가들의 인기있는 주제였다.\\n\\n《백조 기사의 탄생》(La Naissance du Chevalier au Cygne)은 백조 아이 이야기를 십자군 무훈시에 도입한 첫번째 작품이다. 텍스트는 백조 기사 어머니의 이름에 따라 1) 엘리옥스, 2) 베아트릭스, 3) 엘리옥스와 베아트릭스의 혼합, 4) 이솜베르테의 네 가지 버전으로 분류 할 수 있다. 이 가운데 이솜브레테 계열의 이야기는 프랑스어 버전에는 없고 스페인의 《첫 해외 정복》(Gran conquista de Ultramar )에만 등장한다. (가스통 파리는 그가 버전 I로 부른 십자군과 구분된 백조-아이 이야기 원형도 비슷하게 분류하였다.)\\n\\n엘리옥스는 돌로파토스 이야기에 가장 가까운 버전이지만 길을 잃은 젊은 영주를 헝가리 너머의 동방의 통치자인 로타이르 왕으로, 처녀를 엘리옥스로 바꾸어 이야기를 보다 궁정식으로 바꾸었다. 로타이르는 길을 잃고 샘 옆에 멈추어 잠들고, 그 사이 산으로 나무를 하러 온 엘리옥스가 등장한다. 한눈에 반한 로타이르는 어머니의 반대를 무릅쓰고 그녀와 결혼하고, 엘리옥스는 자신이 일곱 아이를 낳고 죽을 것이며 그 아이들 가운데 한 명이 동방의 왕이 될 것이라 예언한다.\\n\\n로타이르가 전쟁에 나간 사이 엘리옥스는 일곱 아이를 낳는다. 시어머니 마트로시유는 엘리옥스를 죽이고 하인에게 아이들을 바구니 둘에 담아 숲에 버리라고 명령하고, 로타이르에게는 엘리옥스가 뱀을 낳고서 물려 죽었다고 거짓말을 한다. 그러나 하인은 은둔자의 오두막 옆에 아이들을 놓아두었고, 아이들은 살아 남아 있다가 7년 후 루데마르라는 탐욕스러운 시종에게 발견된다. 보고를 받은 대비는 아이들의 사슬을 빼앗아 없애라고 명령하지만 탐욕에 눈이 먼 시종은 자신의 것을 챙기느라 미처 누이의 사슬을 빼앗지 못한다. 사슬을 빼앗긴 여섯 소년은 백조의 모습으로 날아가고, 이 이야기를 누이로부터 전해 들은 아버지 로타이르는 백조를 죽이지 말라는 명령을 내린다. 왕의 조카가 백조 하나를 활로 쏘자 로타이르는 금대야를 던져 화살을 막았고 백조가 살아난 대신 금대야가 부서진다. 마트로시유는 대야를 수리하라고 사슬달린 목걸이 하나를 건내주고 이로 인해 진실이 드러난다. 결국 아이들은 사람의 모습을 되찾지만 시종이 사슬을 가져간 한 명만은 백조로 남게 되어 백조 기사가 된다. \\n\\n베아트릭스 버전에서 다태아 출산은 간음의 증거로 간주되어 무고를 당한 아이들의 어머니가 처벌받는다. 이 버전에서 어머니는 복수하여 정의를 실현한다.1969 이솜브르테 버전에서 여성은 혐오스러운 결혼 생활에서 도망진 공주로 묘사된다.\n",
      "Top-3 passage with score 29.8852\n",
      "비오 11세는 이리 오랫동안 질병을 앓았으며, 1938년 11월 25일 몇 시간에 걸친 심장마비를 두 번이나 겪었다. 그는 숨을 쉬는 것을 상당히 힘들어했기 때문에 교황궁 안에만 머물러 있어야만 했다. 비오 11세는 자신이 아끼는 최고급 포도주병 두 개에 ‘2000년이 되는 해에 짐의 후임자에게’라는 라벨을 붙이는 아이디어를 생각해 냈다 나중에 교황 요한 바오로 2세가 실제로 그 포도주병을 받았는지의 여부는 알려지지 않았다. 비오 11세는 자신이 설립한 교황청 과학원에서 마지막 강론을 하였다. 그는 준비된 문서 없이 과학과 종교, 특히 가톨릭교회의 관계에 대해 강론하였다. 이것이 비오 11세가 임종하기 전에 마지막으로 한 강론인 것으로 알려져 있다. 한 젊은 사제는 교황에게 ‘Principiis obsta(애초에 막아라)’라는 고대 로마인의 속담을 인용하면서 약을 먹을 것을 중용하였으나, 교황은 웃으면서 “신부님은 다음 구절을 빠뜨리셨습니다. 애초에 막지 못하여 병이 깊어지면 어떠한 처방을 해도 이미 소용 없습니다. 저는 약을 먹고 낫기에는 이미 늦었습니다.”라고 말했다.\\n\\n1939년 1월 교황의 상태는 눈에 띄게 악화되었다. 비오 11세는 극심한 통증에 시달렸으며 걷는 것도 어려워했다. 그리고 호흡장애가 증가하면서 그는 잠자리에서 일어나려고 해도 일어날 수가 없게 되었다. 1939년 1월 7일 교황의 주치의들은 교황청 관료들에게 교황의 죽음이 머지않았다고 일러주었다. 교황청은 밀라니, 로치, 보나모네, 제멜리, 비안 등 이탈리아 전국에서 내로라하는 전문의들과 교수들로 팀을 구성해 교황의 진단을 맡겼다. 그들은 에우제니오 파첼리 추기경과 조반니 바티스타 몬티니 몬시뇰에게 기관지발작과 결합된 심부전증으로 인해 교황은 이미 절망적인 상태에 빠졌다는 안 좋은 소식을 전해주었다. 비오 11세는 마치 조만간 다시 건강을 되찾을 수 있기라도 하듯이 도메니코 타르디니와 함께 일반 알현 계획을 세웠지만, 이미 그는 정상적으로 숨을 쉴 수 없었으며 따라서 몸을 제대로 움직이거나 심지어 잠결에 돌아누울 힘도 잃어버렸다. 그가 주위 사람들에게 남긴 마지막 말은 “내 영혼은 평온하게 여러분 곁을 떠납니다.”이다. 그의 말은 명료하고 확고부동하게 전달되었다. 교황 비오 11세는 1939년 1월 10일 로마 시각으로 오전 5시 31분에 세 차례의 심장 발작으로 선종하였다. 향년 81세였다. 그의 유해는 성 베드로 대성전의 지하 묘소에 안장되었다.\n",
      "Top-4 passage with score 29.8008\n",
      "유진길과 정하상은 조선으로 돌아와서 남명혁을 비롯한 천주교 공동체들의 지도자들에게 자신들이 교황에게 보낸 편지에 대한 자초지종을 알렸다. 그 소식은 무너져가는 교회에 새로운 희망과 용기를 불어넣었다. 유진길이 집으로 돌아왔을 때, 기쁜 소식이 기다리고 있었다. 그는 아들을 얻었는데, 그가 유대철 베드로이다.\\n\\n유진길과 그의 동료들의 호소문으로 인하여 1831년 9월 9일에 교황 그레고리오 16세는 조선에 베이징에서 분리된 하나의 대목구를 설정했고 파리 외방전교회의 바르톨로메오 브뤼기에르 신부를 그곳의 초대 주교로 임명했다. 그 시작은 1826년의 그 서신이 교황의 마음을 움직인 데에서 비롯된 것이다.\\n\\n태국 방콕에서 근무하고 있던 브뤼기에르 신부는 1832년 7월 25일의 얼마 뒤에 조선의 초대 주교로 임명되었다는 소식을 들었다. 하지만, 불행하게도 그는 조선으로 향하던 중 요동에서 뇌출혈을 일으켜 1835년 10월 20일에 숨을 거두었다. 그 소식은 곧바로 조선에 퍼졌다. 유진길과 그의 동료들은 슬픔에 잠겼지만, 다른 사제들의 입국을 돕기위한 노력을 계속했다. 한편, 그는 마치 신부처럼 행동하며 저명한 많은 학자들을 비롯하여 많은 사람들을 개종시켰다. 그러나, 그는 그의 아내와 딸들은 개종시키지 못했고, 그의 아들만이 그를 따랐다. 후일에 그의 13살 된 장남 유대철은 한국 천주교의 103위 순교성인 중에 가장 어린 사람이 되었다.\n",
      "Top-5 passage with score 29.7865\n",
      "1797년 선거로 다수가 된 왕당파는 구시대로 회귀하려 하였다. 에미그레(망명자)의 친척에 대한 정치 활동 제한이 허용되었고, 기피성직자도 시민권을 되찾았다.\\n\\n총재인 바라스, 루벨, 라 루베리에르 등은 쿠데타로 정부 왕당파를 추방하기로 모의했다. 1797년 9월 4일(프랑스 혁명력 프뤽티도르 18일) 먼저 방데의 반란을 평정했던 라자르 오슈와 나폴레옹의 부하인 피에르 오주로를 파리에 초청하여, 이른바 〈프뤽티도르 18일 쿠데타〉를 일으켰다.\\n\\n쿠데타는 성공했다. 선거에서 뽑힌 198명의 의원이 당선 무효가 되었고 많은 유명 인사들이 체포되었다. 총재인 바르세레미는 남아메리카의 카옌으로 추방되었고 카르노는 망명했다. 그 후임 총재로 두에와 누샤토가 올랐다. 정부 요직은 공화파가 점유하였고, 에미그레의 친척에 대한 법률도 부활되었다. 군사 법정이 설치되었고, 망명은 유죄로 프랑스에 귀국을 명하는 판결이 내려졌다.\\n\\n기피성직자는 다시 억압되게 되었다. 수백 명이 카옌으로 보내지거나, 혹은 레 섬과 오레론 섬의 폐허에 갇혔다. 라 루베리에르 스스로 교단을 확장하고 많은 교회를 경신박애교 시설로 개조하였다. 정부는 10요일(프랑스 혁명력 참조)을 공적인 제례일로 정하고 의무적으로 업무를 쉬도록 하여, 지금까지 교회에서 행해지고 있었던 일요일 예배를 금지시켰다. 언론의 자유가 제한되고 신문은 발행 금지 처분당하여, 신문은 일제히 추방되었다.\\n\\n이전 귀족 전체를 프랑스에서 추방하는 것이 제안되었다. 그 방안은 실현되지 않았지만, 옛 귀족은 외국인 취급이 되었고 시민권을 얻기 위해서는 귀화를 해야만 했다. 또한 채권 이자의 2/3는 무효 처리됐다.\\n\\n기타 정책으로 경제 장관 라메르는 긴축 정책을 취하여, 지출을 줄이고, 각종 급여의 인하와 동결, 간접세의 부활 등의 정책을 실시했다. 또한 내무 장관 누샤토는 학교와 정부 통계 등에 주력했다.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(\"Top-%d passage with score %.4f\" % (i+1, dot_prod_scores.squeeze()[rank[i]]))\n",
    "  print(valid_corpus[rank[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBaKYpdoXDcW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MRC Practice 5 - Dense Passage Retrieval (In-batch)",
   "provenance": [
    {
     "file_id": "1c9Vr7z_LBG2l9K4lVb40pu7Kk22hXQCp",
     "timestamp": 1614240569955
    },
    {
     "file_id": "1Q7iAXm_kwF_NHfOEGdViMCiPHnqoZlXe",
     "timestamp": 1613491158162
    }
   ]
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
