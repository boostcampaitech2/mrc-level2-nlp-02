{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDdziEN_VCt"
   },
   "source": [
    "# 5강) BERT를 활용한 Dense Passage Retrieval 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NWluWk3_VCu"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5421,
     "status": "ok",
     "timestamp": 1616574100645,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "eGqFS4EEBF_Z",
    "outputId": "b5b5af1d-0d0d-4197-a717-d2fe3ca2528f"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYUkp06Y_VCv"
   },
   "source": [
    "## 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMrZa4uql_nx"
   },
   "source": [
    "KorQuAD train 데이터셋을 학습 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6098,
     "status": "ok",
     "timestamp": 1616574101330,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "4IUxepuj_VCv",
    "outputId": "6c681d71-4e21-4062-9807-1d975fc901e8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# dataset = load_dataset(\"squad_kor_v1\")\n",
    "dataset = load_from_disk('../../data/train_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtECqpB_VCx"
   },
   "source": [
    "## 토크나이저 준비 - Huggingface 제공 tokenizer 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Fu2WaqpUB8"
   },
   "source": [
    "BERT를 encoder로 사용하므로, hugginface에서 제공하는 \"bert-base-multilingual-cased\" tokenizer를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AoB8BHGDmVIK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "# model_checkpoint = \"klue/roberta-base\"\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8395,
     "status": "ok",
     "timestamp": 1616574103635,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WPxRvMjdvh4y",
    "outputId": "069dd4b9-760a-450b-ea59-096510005e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 9195,
     "status": "ok",
     "timestamp": 1616574104440,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "0U7sn3jsu44O",
    "outputId": "afb5de1c-c5d4-420c-d1f1-f69f56c44cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 미국 상의원 또는 미국 상원 ( United States Senate ) 은 양원제인 미국 의회의 상원이다. [UNK] n [UNK] n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1 / 3씩 상원의원을 새로 선출하여 연방에 보낸다. [UNK] n [UNK] n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할 ( 하원의 법안을 거부할 권한 등 ) 을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션 ( 공공건강보험기관 ) 의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다. 날짜 = 2017 - 02 - 05 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(dataset['train'][0]['context'], padding=\"max_length\", truncation=True)\n",
    "tokenizer.decode(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpoTleVJjp5x"
   },
   "source": [
    "## Dense encoder (BERT) 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nrxmtmfkRVb"
   },
   "source": [
    "HuggingFace BERT를 활용하여 question encoder, passage encoder 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6b215ZfJ_EOc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-bKwkxTpoje"
   },
   "source": [
    "1) Training Dataset 준비하기 (question, passage pairs)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E_FQ1kcazxge"
   },
   "outputs": [],
   "source": [
    "# Use subset (128 example) of original training dataset \n",
    "# sample_idx = np.random.choice(range(len(dataset['train'])), 128)\n",
    "# training_dataset = dataset['train'][sample_idx]\n",
    "\n",
    "training_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NJZWx1b-613e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "# q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt', return_token_type_ids=False)\n",
    "# p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt', return_token_type_ids=False)\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bAplp66Pkayy"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMvVH1e3h99"
   },
   "source": [
    "2) BERT encoder 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW7Oc7Zd9kkm"
   },
   "source": [
    "BertEncoder 모델 정의 후, question encoder, passage encoder에 pre-trained weight 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaPreTrainedModel, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oKKkTlh_l5VL"
   },
   "outputs": [],
   "source": [
    "# class BertEncoder(RobertaPreTrainedModel):\n",
    "class BertEncoder(BertPreTrainedModel):  \n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    # self.bert = RobertaModel(config)\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids #roberta시 주석\n",
    "                          )\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24450,
     "status": "ok",
     "timestamp": 1616574119714,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "wnO1b30SomBP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "998e38f2-0564-4956-bd43-878798158805",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "# p_encoder = BertEncoder.from_pretrained('../encoders/p_encoder')\n",
    "# q_encoder = BertEncoder.from_pretrained('../encoders/q_encoder')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  p_encoder.cuda()\n",
    "  q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in iter(p_encoder.named_parameters()):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Dgo8U997HD"
   },
   "source": [
    "Train function 정의 후, 두개의 encoder fine-tuning 하기 (In-batch negative 활용) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VAb7NpUc8YRo"
   },
   "outputs": [],
   "source": [
    "def train(args, dataset, p_model, q_model):\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "  ]\n",
    "\n",
    "  optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    eps=args.adam_epsilon\n",
    "  )\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "# tt\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2] # roberta시 주석\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5] # roberta시 주석\n",
    "                  }\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "      \n",
    "      # print('q_outputs: ',q_outputs)\n",
    "      # print('p_outputs: ',p_outputs)\n",
    "      # print('sim_scores: ',sim_scores)\n",
    "        \n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    "\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "      \n",
    "      \n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    "      # print(loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "  return p_model, q_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ICSJoJrUDGZ5"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96513,
     "status": "ok",
     "timestamp": 1616574191784,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "E8a7ww3WgsaZ",
    "outputId": "8f98f6cf-8a44-4e4a-928b-836a1b27a772"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 988/988 [12:56<00:00,  1.27it/s]\n",
      "Iteration: 100%|██████████| 988/988 [13:04<00:00,  1.26it/s]\n",
      "Epoch: 100%|██████████| 2/2 [26:00<00:00, 780.27s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 6887.20it/s]\n"
     ]
    }
   ],
   "source": [
    "p_encoder, q_encoder = tqdm(train(args, train_dataset, p_encoder, q_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.save_pretrained('../encoders/p_encoder')\n",
    "q_encoder.save_pretrained('../encoders/q_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder = BertEncoder.from_pretrained('../encoders/p_encoder_neg').to('cuda')\n",
    "q_encoder = BertEncoder.from_pretrained('../encoders/q_encoder_neg').to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOw-k7Ln85t"
   },
   "source": [
    "## Dense Embedding을 활용하여 passage retrieval 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96984,
     "status": "ok",
     "timestamp": 1616574192258,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "NouB9uBcTaws",
    "outputId": "f2446b6c-3008-4350-be54-140b194a1a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 남부 지방에서 급격하게 영국산 제품이 많이 팔린 계기가 된 조치는?\n",
      "통상금지법(Embargo Act of 1807)은 1807년에 미국 의회에서 통과된 법률이며 , 나폴레옹 전쟁을 치루고 있는 영국과 프랑스에 대항하여 미국의 무역을 금지한 것이었다\\n\\n이 통상금지(이하, 엠바고)는 미국의 상인과 화물선이 유럽의 해군에게 금수품으로 나포하면서, 미국의 중립 위반에 대한 보복 조치를 취하게 되자 이 법안을 시행하게 되었다. 영국 해군은 특히 수천 명의 미국 선원들을 자신들의 함대에 복역하게 하는 강제 징발 조치를 감행하였다. 유럽에 대한 통제권을 위해 필사적으로 다투었던 영국과 프랑스는 우발적인 사고이고, 그들의 생존을 위해 필수적인 조치였다고 합리화했다. 미국인들은 체사피크 레오퍼드 사건을 중립을 지키는 미국의 조치에 대한 영국의 어처구니 없는 위반 사례라고 보았다 외교적 모욕이라고 이해를 하고, 유럽 열강에 의해 이러한 작전들을 지지하는 보증되지 않은 공식적인 명령서는 미국의 선전포고에 대한 근거로서 널리 인정되고 있다. 노한 군중들은 냉소적으로 ‘엠바고’(Embargo)라는 글자를 거꾸로 ‘나 좀 잡아줘’(O Grab Me)라고 바꿔 불렀다. \\n\\n토머스 제퍼슨 대통령은 이러한 분노한 대중들의 적의를 등에 업고, 보복에 대한 대중적 지지에 힘을 싣고 있었다. 그는 의회에 군사적으로 물리적 조치보다는 경제 전쟁으로 응전해야 한다고 권했다. 그리하여 통상금지법이 1807년 12월 22일 서명되었다. 이 급작스런 조치에 대한 기대 효과(교전국 사이의 경제적 곤란 )는 영국과 프랑스를 힘들게 하여, 그들로 하여금 미국 상선을 방해를 하지못하도록 강제하고, 미국의 중립을 존중하며, 강제 징발 정책을 끝내는 것이었다. 이 엠바고는 강압적인 조치로서는 그다지 효과를 보지 못한 것으로 밝혀졌으며, 외교적으로도, 경제적으로 모두 실패한 조치였다. 이 법이 막상 시행되고 나서, 미국 경제와 미국 국민들에게 파괴적인 부담을 주었다. \\n\\n미국 상인들에 의한 바다와 내륙에서의 무역제한 회피, 법망의 허점은 유럽을 목표로 했던 엠바고의 영향을 급격히 감소시켰다. 영국 상인들은 엠바고 때문에 미국 배들에 의해 버려진 항로를 잘 이용해 먹었다 수입 금지 조치로 인해 촉발된 물자부족 현상으로 미국 남부에서 영국산 제품에 대한 수요가 치솟았다 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_corpus = list(set([example['context'] for example in dataset['validation']]))#[:10]\n",
    "sample_idx = random.choice(range(len(dataset['validation'])))\n",
    "query = dataset['validation'][sample_idx]['question']\n",
    "ground_truth = dataset['validation'][sample_idx]['context']\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "  valid_corpus.append(ground_truth)\n",
    "\n",
    "print(query)\n",
    "print(ground_truth, '\\n\\n')\n",
    "\n",
    "# valid_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05D8GzFrJhHO"
   },
   "source": [
    "앞서 학습한 passage encoder, question encoder을 이용해 dense embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ba-hH3NQOEWJ"
   },
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "  return tuple(t.cuda() for t in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97547,
     "status": "ok",
     "timestamp": 1616574192826,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "YufA_ayPJBRg",
    "outputId": "fa600009-393a-4f93-871e-bff93675d05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([235, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  p_encoder.eval()\n",
    "  q_encoder.eval()\n",
    "\n",
    "  q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "  # q_seqs_val = tokenizer(['미국의 대통령은 누구인가?'], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "  q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)\n",
    "\n",
    "  p_embs = []\n",
    "  for p in valid_corpus:\n",
    "    p = tokenizer(p, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "    p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "    p_embs.append(p_emb)\n",
    "\n",
    "p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "\n",
    "print(p_embs.size(), q_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOHHak7WS1ko"
   },
   "source": [
    "생성된 embedding에 dot product를 수행 => Document들의 similarity ranking을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97546,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "xn5Cx5JkKZJB",
    "outputId": "eb232f4f-bdae-474d-b630-a13d0b563364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 235])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.size())\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "# print(dot_prod_scores)\n",
    "# print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2Oiv8MKVS6"
   },
   "source": [
    "Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97544,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WaStRXYdJ-wI",
    "outputId": "bbf42d40-dfae-49ff-bf12-139452b5849f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 미국 남부 지방에서 급격하게 영국산 제품이 많이 팔린 계기가 된 조치는? \n",
      "\n",
      "[Ground truth passage]\n",
      "통상금지법(Embargo Act of 1807)은 1807년에 미국 의회에서 통과된 법률이며 , 나폴레옹 전쟁을 치루고 있는 영국과 프랑스에 대항하여 미국의 무역을 금지한 것이었다\\n\\n이 통상금지(이하, 엠바고)는 미국의 상인과 화물선이 유럽의 해군에게 금수품으로 나포하면서, 미국의 중립 위반에 대한 보복 조치를 취하게 되자 이 법안을 시행하게 되었다. 영국 해군은 특히 수천 명의 미국 선원들을 자신들의 함대에 복역하게 하는 강제 징발 조치를 감행하였다. 유럽에 대한 통제권을 위해 필사적으로 다투었던 영국과 프랑스는 우발적인 사고이고, 그들의 생존을 위해 필수적인 조치였다고 합리화했다. 미국인들은 체사피크 레오퍼드 사건을 중립을 지키는 미국의 조치에 대한 영국의 어처구니 없는 위반 사례라고 보았다 외교적 모욕이라고 이해를 하고, 유럽 열강에 의해 이러한 작전들을 지지하는 보증되지 않은 공식적인 명령서는 미국의 선전포고에 대한 근거로서 널리 인정되고 있다. 노한 군중들은 냉소적으로 ‘엠바고’(Embargo)라는 글자를 거꾸로 ‘나 좀 잡아줘’(O Grab Me)라고 바꿔 불렀다. \\n\\n토머스 제퍼슨 대통령은 이러한 분노한 대중들의 적의를 등에 업고, 보복에 대한 대중적 지지에 힘을 싣고 있었다. 그는 의회에 군사적으로 물리적 조치보다는 경제 전쟁으로 응전해야 한다고 권했다. 그리하여 통상금지법이 1807년 12월 22일 서명되었다. 이 급작스런 조치에 대한 기대 효과(교전국 사이의 경제적 곤란 )는 영국과 프랑스를 힘들게 하여, 그들로 하여금 미국 상선을 방해를 하지못하도록 강제하고, 미국의 중립을 존중하며, 강제 징발 정책을 끝내는 것이었다. 이 엠바고는 강압적인 조치로서는 그다지 효과를 보지 못한 것으로 밝혀졌으며, 외교적으로도, 경제적으로 모두 실패한 조치였다. 이 법이 막상 시행되고 나서, 미국 경제와 미국 국민들에게 파괴적인 부담을 주었다. \\n\\n미국 상인들에 의한 바다와 내륙에서의 무역제한 회피, 법망의 허점은 유럽을 목표로 했던 엠바고의 영향을 급격히 감소시켰다. 영국 상인들은 엠바고 때문에 미국 배들에 의해 버려진 항로를 잘 이용해 먹었다 수입 금지 조치로 인해 촉발된 물자부족 현상으로 미국 남부에서 영국산 제품에 대한 수요가 치솟았다 \n",
      "\n",
      "Top-1 passage with score 139.8865\n",
      "통상금지법(Embargo Act of 1807)은 1807년에 미국 의회에서 통과된 법률이며 , 나폴레옹 전쟁을 치루고 있는 영국과 프랑스에 대항하여 미국의 무역을 금지한 것이었다\\n\\n이 통상금지(이하, 엠바고)는 미국의 상인과 화물선이 유럽의 해군에게 금수품으로 나포하면서, 미국의 중립 위반에 대한 보복 조치를 취하게 되자 이 법안을 시행하게 되었다. 영국 해군은 특히 수천 명의 미국 선원들을 자신들의 함대에 복역하게 하는 강제 징발 조치를 감행하였다. 유럽에 대한 통제권을 위해 필사적으로 다투었던 영국과 프랑스는 우발적인 사고이고, 그들의 생존을 위해 필수적인 조치였다고 합리화했다. 미국인들은 체사피크 레오퍼드 사건을 중립을 지키는 미국의 조치에 대한 영국의 어처구니 없는 위반 사례라고 보았다 외교적 모욕이라고 이해를 하고, 유럽 열강에 의해 이러한 작전들을 지지하는 보증되지 않은 공식적인 명령서는 미국의 선전포고에 대한 근거로서 널리 인정되고 있다. 노한 군중들은 냉소적으로 ‘엠바고’(Embargo)라는 글자를 거꾸로 ‘나 좀 잡아줘’(O Grab Me)라고 바꿔 불렀다. \\n\\n토머스 제퍼슨 대통령은 이러한 분노한 대중들의 적의를 등에 업고, 보복에 대한 대중적 지지에 힘을 싣고 있었다. 그는 의회에 군사적으로 물리적 조치보다는 경제 전쟁으로 응전해야 한다고 권했다. 그리하여 통상금지법이 1807년 12월 22일 서명되었다. 이 급작스런 조치에 대한 기대 효과(교전국 사이의 경제적 곤란 )는 영국과 프랑스를 힘들게 하여, 그들로 하여금 미국 상선을 방해를 하지못하도록 강제하고, 미국의 중립을 존중하며, 강제 징발 정책을 끝내는 것이었다. 이 엠바고는 강압적인 조치로서는 그다지 효과를 보지 못한 것으로 밝혀졌으며, 외교적으로도, 경제적으로 모두 실패한 조치였다. 이 법이 막상 시행되고 나서, 미국 경제와 미국 국민들에게 파괴적인 부담을 주었다. \\n\\n미국 상인들에 의한 바다와 내륙에서의 무역제한 회피, 법망의 허점은 유럽을 목표로 했던 엠바고의 영향을 급격히 감소시켰다. 영국 상인들은 엠바고 때문에 미국 배들에 의해 버려진 항로를 잘 이용해 먹었다 수입 금지 조치로 인해 촉발된 물자부족 현상으로 미국 남부에서 영국산 제품에 대한 수요가 치솟았다\n",
      "Top-2 passage with score 138.5299\n",
      "7년 전쟁(1756년 – 1763년)을 겪은 대영 제국은 무거운 부채에 허덕이고 있었다. 새로 획득 한 땅에 대한 비용 분담을 하고자 그레이트브리튼 의회(이하 \"본국 의회\")는 북미 식민지에 세금을 과세하기로 결정했다. 이전에 〈항해법〉을 제정하여 만든 과세는 제국의 무역 통제를 하기 위한 수단에 불과했다. 그러나 1764년 설탕법은 이전보다 세입 증가라는 구체적인 목표를 앞세워 식민지에 세금을 부과하고자 했다. 북미 식민지의 사람들이 ‘설탕법’에 반대한 것은 처음에는 경제적인 이유였지만, 곧 헌법상의 문제가 포함되어 있다는 것을 알게 되었다. \\n\\n헌법에 따르면 “영국 신민은 본국 의회에서 대표자의 동의없이 세금을 부과받을 수 없다”고 되어 있었다. 식민지는 본국 의회 의원을 선출하지 않았기 때문에 식민지의 많은 사람들은 자신들에게 세금을 부과하려는 본국 의회의 시도는 동의에 의해서만 과세를 허용하는 것을 강조한 헌법 이념에 위배되는 것이라고 생각했다. 이에 대해 본국 의원 중 일부는 “사실상 대표 파견”이라는 논리를 가지고 대항했다. 즉, 선출된 의원은 없지만, 식민지의 의견은 실제로 본국 의회에 반영되고 있다는 것이었다. 이 논제는 설탕법이 통과되었을 때는 작은 논제에 불과했으나, 1765년 〈인지세법〉 통과되었을 때는 주요 논점이 되었다. ‘인지세법’으로 식민지에서 격렬한 비판이 들끓자 이듬해 본국 의회는 부득이 이 법을 철폐했다.\\n\\n‘인지세법’을 둘러싼 논란의 이면에는 세금과 대의제보다 더 근본적인 문제가 숨어 있었다. 즉 본국 의회가 식민지에 대한 주권을 행사할 수 있는가에 대한 의문점이었다. 이것은 대한 답변으로 본국 의회는 1766년에 인지세법을 철폐하는 동시에 선언법을 제정하여 본국 의회가 식민지에 대한 입법을 “어떤 경우에도 행할 수” 있도록 하였다.\n",
      "Top-3 passage with score 136.9488\n",
      "제2차 세계 대전 후에 와그너는 노동, 사회 보장, 완전 고용 (취업 기회들이 풍부하고 실업률이 6 퍼센트 아래 있던 경제 상태), 주택 공급, 국제 경제 원조, 팔레스타인에 유대인 국가의 창조와 북동부에서 세인트로렌스 수로의 경제 개발을 포함한 광범위한 문제들에 다시 한번 연루되었다. 와그너는 공공 주택 공급과 도시 재개발을 흥행하는 입법과 유대인의 국가 이스라엘이 1948년 현실이 된 성공을 가졌다. 해리 트루먼 대통령의 강한 성원과 함께 와그너는 또한 국내 건강보험과 민권을 위한 입법을 흥행하기도 하였다. 하지만 보수적 의회는 와그너의 제안들을 통과시키는 데 더욱 어렵게 만들었다. 의회가 트루먼의 거부에 태프트-하틀리 법령을 통과시켰을 때 주요 패배가 왔다. 법령은 국내 노동 관계 법령을 수정하였고, 이전의 10년간 세월에 와그너가 노동력 달성을 도운 권력을 줄였다.\\n\\n자신의 건강 상실과 함께 와그너는 1949년 6월 28일 상원으로부터 조용히 사임하였다. 자신의 퇴직에 그는 인권을 위한 싸움에서 패배보다 더욱 많은 입법적 승리들을 간주할 수 있었다는 것을 자랑스럽게 선언하였다. 자신의 공헌의 세월 동안 와그너는 진보적인 정치의 상승을 반영하였고 이민자들의 정치력을 증가시켰다.\n",
      "Top-4 passage with score 135.3889\n",
      "제2차 세계 대전이 일어나는 동안에 아이오와 주에서 생산된 옥수수와 돼지고기를 포함한 미국 농산품들을 위한 요청이 늘어났다. 결과로서 아이오와 주 농부들의 소득이 재빠르게 상승하였다.\\n\\n1945년과 1960년대 후반 사이에 수백개의 새로운 산업들이 아이오와 주로 옮겨져 들어왔다. 그 대부분은 식품 혹은 금속 가공업이나 기계 제조업이었다. 아이오와 주는 그때부터 기초의 농장 경제에서 공업-농업 경제로 변화하였다. 그 동안 근대의 농기구 사용의 증가와 작은 농장들을 더 커진 농장들로 합병이 농장의 고용을 감소하였다. 많은 아이오와 주민들이 새롭고 확장된 산업들에서 일하러 시골 지역들에서 도시들로 이주하였다. 1960년, 인구 조사국은 처음으로 더 많은 아이오와 주민들이 시골 지역들보다 도시 지역들에 산다고 보고하였다. 조사국은 1950년에 48 퍼센트와 비교하며, 주민들의 53 퍼센트가 도시와 타운들에 사는 것을 보여주었다.\\n\\n술 판매의 논쟁이 1950년대 동안에 다시 올랐다. 아이오와 주의 입법부는 술집에서 알콜 음료들이 내도록 허락하는 일부 결의를 거절하였다. 1963년, 입법부는 마시는 술의 판매를 적법화하였다. 그러나 각각의 아이오와 주의 카운티들에서 투표자들은 자신들의 지역들에서 술의 판매를 허락하느냐를 결정할 수 있다.\n",
      "Top-5 passage with score 134.4723\n",
      "1919년부터 1933년까지 독일은 제 1차 세계대전 이후 세워진 바이마르 공화국이 통치했다. 바이마르 공화국은 이원집정부제와 민주주의에 기반한 공화국이었다. 하지만 바이마르 공화국은 성립 직후부터 수많은 위기에 직면하게 된다. 초인플레이션과 정치적 혼란, 정당들의 분열과 1차 세계대전의 승전국들과의 껄끄러운 관계 등이 끊임없이 바이마르 공화국의 목을 잡았던 것이다. 특히 1919년에 맺어진 베르사유 조약에서, 당시 패전국이었던 독일은 엄청난 양의 배상금을 물어주어야했는데, 바이마르 정부는 이 배상금을 주기 위하여 대규모로 돈을 찍어냈다. 정부가 지나치게 통화 유통량을 증가시키자 시중에서는 당연히 대규모 인플레이션이 일어났고, 이 때문에 모든 상품 가격들이 상승하고 독일은 경제적인 대혼란에 빠져들었다. 게다가 이러한 노력에도 불구하고 바이마르 공화국이 프랑스에게 배상금을 갚는 데에 실패하자, 프랑스 군대는 루르에 있는 독일의 공장 지대를 강제로 점령하였다. 이는 독일 대중들의 격분을 불러왔고, 바이마르 공화국은 갈수록 위태로워져만 갔다.\\n\\n국가사회주의 독일 노동자당(나치당)은 1920년에 창당되었다. 1년 전에 창당되었던 독일 노동자당을 이어 만들어졌으며, 당시 독일에서 활하던 극우 정당 들 중 하나였다. 나치당 강령에는 바이마르 공화국의 해체, 베르사유 조약 파기, 급진적인 반유대주의, 반공주의 등이 들어있었다. 이들은 국민들에게 강력한 중앙 정부를 약속했으며, 주거 문제를 안정시키겠다고 공언했으며 인종에 기반한 정책을 펼칠 것이라고 말했다. 또한 막대한 부를 축적하고 있던 유대인들과 같은 인종들을 쓸어내어 세계의 '인종적 순수성'을 지켜낼 것이는 망언을 하는 등 과격하고 급진적인 정책들을 쏟아내었다. 나치당은 돌격대를 창설하여 자신들에게 반대하는 사람들에게 폭행을 가하거나 죽였고, 거리에서 유대인들을 폭행하거나 상대 정당의 모임들에 쳐들어가며 공포 분위기를 조장하였다.\\n\\n1929년 10월 24일, 미국의 주식 시장이 대폭락하자 이는 미국의 달러에 깊이 의존하고 있던 독일에도 막대한 영향을 미쳤다. 수 백만 명이 실업자로 전락하였으며, 주요 은행들이 연이어 파산한 것이다. 히틀러와 나치당은 이러한 경제적 혼란을 바탕으로 정권을 잡고자 하였다. 그들은 경제 재건과 일자리를 약속하였고, 강력한 정부를 바탕으로 국가를 다시 세울 것이라고 말했다. 유권자들은 나치당이 독일을 부활시킬 수 있다고 믿었고, 1932년에 치러진 선거에서 나치당은 37.4%의 득표율로 230석을 가져가며 의회에서 가장 많은 의석을 차지하였다.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(\"Top-%d passage with score %.4f\" % (i+1, dot_prod_scores.squeeze()[rank[i]]))\n",
    "  print(valid_corpus[rank[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBaKYpdoXDcW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MRC Practice 5 - Dense Passage Retrieval (In-batch)",
   "provenance": [
    {
     "file_id": "1c9Vr7z_LBG2l9K4lVb40pu7Kk22hXQCp",
     "timestamp": 1614240569955
    },
    {
     "file_id": "1Q7iAXm_kwF_NHfOEGdViMCiPHnqoZlXe",
     "timestamp": 1613491158162
    }
   ]
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
