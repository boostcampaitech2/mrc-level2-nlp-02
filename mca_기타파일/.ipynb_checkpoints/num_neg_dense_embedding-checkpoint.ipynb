{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==1.13.3 -q\n",
    "# !pip install transformers==4.11.3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    ")\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../../data/train_dataset')['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?',\n",
       " '스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?',\n",
       " '촌락에서 운영 위원 후보자 이름을 쓰기위해 사용된 것은?',\n",
       " '로타이르가 백조를 구하기 위해 사용한 것은?',\n",
       " '의견을 자유롭게 나누는 것은 조직 내 어떤 관계에서 가능한가?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['question'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.search(index='wiki_documents', size=2, query={'match':{'text':dataset['question'][0]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 11,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 10000, 'relation': 'gte'},\n",
       "  'max_score': 21.199265,\n",
       "  'hits': [{'_index': 'wiki_documents',\n",
       "    '_type': '_doc',\n",
       "    '_id': '38855',\n",
       "    '_score': 21.199265,\n",
       "    '_source': {'title': '경남은행 (1970~2014년 기업)',\n",
       "     'text': '1968년 회사 설립을 발기하고 1970년 창립총회를 거쳐 업무를 시작하였으며 1972년 증권거래소에 주식을 상장하였다. 1997년 IMF 구제금융사건으로 인한 후유증으로 국제결제은행 기준 자기자본비율이 미달되자 2000년 11월 금융감독원의 경영개선요구를 받았고 2000년 12월 부실금융기관으로 지정되어 독자생존 불가 판정을 받았다. 2001년 3월 대형화 및 겸업화를 통한 국제경쟁력 강화를 위하여 예금보험공사가 공적자금을 투입하여 설립된 우리금융그룹의 자회사로 편입되었다. 2014년 5월 우리금융그룹으로부터 분리되어 KNB금융지주로 설립하였고 2014년 8월 KNB금융지주에 흡수합병되어 폐업하였다. 2014년 10월 BS금융지주의 자회사로 편입되었고 2015년 3월 BNK경남은행으로 상호를 변경하였다.'}},\n",
       "   {'_index': 'wiki_documents',\n",
       "    '_type': '_doc',\n",
       "    '_id': '31694',\n",
       "    '_score': 19.99009,\n",
       "    '_source': {'title': '앨런 클라인',\n",
       "     'text': '앨런 클라인(Allen Klein, 1931년 12월 18일 2009년 7월 4일)은 미국의 사업가, 음악 출판인, 작가 대표인, 영화 제작자, 음반 레이블 경영자다. 거친 성격과 공격적이며 혁신적인 협상 방식으로 유명하며 이들은 음반 아티스트에게 돌아가는 보상금의 업계 표준을 높게 설정하도록 했다. ABKCO 뮤직 앤 레코드 인코퍼레이티드를 설립했다. 클라인은 그전까지 힘겨운 회사계약에 일상적으로 희생자가 된 음반 아티스트의 소득 잠재력에 혁명을 가져왔다.  1950년대 말의 원 히트 로커빌리 버디 녹스와 지미 보웬을 도와 처음 거대 금전적 성공과 계약상 우발적 소득을 거두게 되었고, 뒤에 샘 쿡의 매니저를 수행해 초기 성공을 증대했다. 이후 많은 아티스트의 매니저가 되었고, 여기에는 비틀즈와 롤링 스톤스도 포함된다. 그럼으로써 그는 당대 음악업계에서 가장 영향력 있는 인물 중 한 명으로 자리매김했다.p=178\\n\\n사업 매니저가 보통 재정적 조언과 고객의 소득 최대화를 하는 것과 달리, 클라인은 자기가 \"사고파는 합의(buy/sell agreements)\"라고 부르는 것을 제공했다. 클라인이 소유한 회사는 그의 고객과 음반 레이블간의 중재자 일, 음반을 제조하는 일, 그것을 음반 레이블에 판매하는 일, 고객에게 로열티 및 선금 지불하는 일을 했다. 클라인은 고객의 수익을 크게 높였고 본인도 부유해졌으나, 때때로 고객의 상식밖의 일을 했다.pp=133-135 (롤링 스톤스가 1965년에 데카로부터 지급받은 125만 달러의 경우 회사로 예금되었고, 클라인이 제공하고 작은 글씨로 작성된 계약서에서는 클라인이 이를 20년간 공개할 필요가 없다고 명시되어 있다.)  클라인이 비틀즈와 롤링 스톤스에게 관여한 일은 수년간의 소송전으로 이어졌고, 특히 롤링 스톤스는 클라인을 로열티 미지급, 자신들 곡의 출판 권리 강탈, 5년간의 세금 지불 방관(이는 1971년 프랑스로의 \"망명\"을 이끌었다.)에 대한 혐의로 기소했다.pp=199-223\\n\\nIRS의 다년간의 추적 끝에 클라인은 1972년 소득 신고서에 허위진술을 한 경범죄 혐의로 유죄판결을 받았고, 1980년 두 달간 감옥에서 복역했다.pp=252-253 알츠하이머 투병 끝에 2009년 7월 4일 세상을 떠났다. 향년 77세.'}}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.6.0].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrieval:\n",
    "  def __init__(self, args, train_dataset, valid_dataset, num_neg, tokenizer, p_encoder, q_encoder):\n",
    "\n",
    "    self.args = args\n",
    "    self.train_dataset = train_dataset\n",
    "    self.valid_dataset = valid_dataset\n",
    "    self.num_neg = num_neg\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.p_encoder = p_encoder\n",
    "    self.q_encoder = q_encoder\n",
    "    \n",
    "    self.train_answer_index = []\n",
    "    self.valid_answer_index = []\n",
    "    \n",
    "    self.prepare_in_batch_negative(train_dataset, num_neg, tokenizer, is_train=True)\n",
    "    self.prepare_in_batch_negative(valid_dataset, num_neg, tokenizer, is_train=False)\n",
    "    \n",
    "    \n",
    "\n",
    "  def prepare_in_batch_negative(self, dataset, num_neg, tokenizer, is_train):\n",
    "    \n",
    "    corpus = np.array(list(set([example for example in dataset['context']])))\n",
    "    p_with_neg = []\n",
    "\n",
    "    for c, q in zip(dataset['context'], dataset['question']):\n",
    "      if is_train:\n",
    "        p_with_neg.append(c)\n",
    "        neg_count = 0\n",
    "\n",
    "        responses = es.search(index='wiki_documents', size=num_neg*2, query={'match':{'text':q}})\n",
    "        for res in responses['hits']['hits']:\n",
    "          if c != res['_source']['text'] and neg_count < num_neg:\n",
    "            neg_count += 1\n",
    "            p_with_neg.append(res['_source']['text'])\n",
    "          if not neg_count < num_neg:\n",
    "            break\n",
    "        while neg_count < num_neg:\n",
    "          neg_idxs = np.random.randint(len(corpus), size=num_neg-num_count)\n",
    "\n",
    "          if c not in corpus[neg_idxs]:\n",
    "            p_neg = corpus[neg_idxs].tolist()\n",
    "\n",
    "            p_with_neg.extend(p_neg)\n",
    "            break\n",
    "        \n",
    "        # 정답 섞기\n",
    "        number_for_swap = random.randint(1, num_neg+1)\n",
    "        p_with_neg[-(num_neg+1)], p_with_neg[-number_for_swap] = p_with_neg[-number_for_swap], p_with_neg[-(num_neg+1)]\n",
    "        self.train_answer_index.append(num_neg+1-number_for_swap)\n",
    "    \n",
    "      else:\n",
    "        while True:\n",
    "          neg_idxs = np.random.randint(len(corpus), size=num_neg)\n",
    "\n",
    "          if c not in corpus[neg_idxs]:\n",
    "            p_neg = corpus[neg_idxs].tolist()\n",
    "\n",
    "            p_with_neg.append(c)\n",
    "            p_with_neg.extend(p_neg)\n",
    "            # print(p_with_neg)\n",
    "            break\n",
    "            \n",
    "        number_for_swap = random.randint(1, num_neg+1)\n",
    "        p_with_neg[-(num_neg+1)], p_with_neg[-number_for_swap] = p_with_neg[-number_for_swap], p_with_neg[-(num_neg+1)]\n",
    "        self.valid_answer_index.append(num_neg+1-number_for_swap)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    q_seqs = tokenizer(\n",
    "      dataset['question'],\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_tensors = 'pt',\n",
    "      # return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    p_seqs = tokenizer(\n",
    "      p_with_neg,\n",
    "      padding = 'max_length',\n",
    "      truncation=True,\n",
    "      return_tensors = 'pt',\n",
    "      # return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    max_len = p_seqs['input_ids'].size(-1)\n",
    "    p_seqs['input_ids'] = p_seqs['input_ids'].view(-1, num_neg+1, max_len)\n",
    "    p_seqs['attention_mask'] = p_seqs['attention_mask'].view(-1, num_neg+1, max_len)\n",
    "    p_seqs['token_type_ids'] = p_seqs['token_type_ids'].view(-1, num_neg+1, max_len)\n",
    "\n",
    " \n",
    "\n",
    "    if is_train:\n",
    "      train_dataset = TensorDataset(\n",
    "        p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'],\n",
    "        \n",
    "      )\n",
    "      self.train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size = self.args.per_device_train_batch_size\n",
    "      )\n",
    "    else:\n",
    "      valid_dataset = TensorDataset(\n",
    "        p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'],\n",
    "      )\n",
    "      self.valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size = self.args.per_device_train_batch_size\n",
    "      )\n",
    "\n",
    "    valid_seqs = tokenizer(\n",
    "      dataset['context'],\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    passage_dataset = TensorDataset(\n",
    "      valid_seqs['input_ids'],\n",
    "      valid_seqs['attention_mask'],\n",
    "      valid_seqs['token_type_ids'],\n",
    "    )\n",
    "    \n",
    "    if is_train:\n",
    "      self.passage_dataloader = DataLoader(\n",
    "        passage_dataset,\n",
    "        batch_size = self.args.per_device_train_batch_size\n",
    "      )\n",
    "    \n",
    "\n",
    "  def train(self, args=None):\n",
    "    if args is None:\n",
    "      args = self.args\n",
    "    batch_size = args.per_device_train_batch_size\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "      {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "      {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "      {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "      {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "      optimizer_grouped_parameters,\n",
    "      lr=args.learning_rate,\n",
    "      eps=args.adam_epsilon\n",
    "    )\n",
    "    t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps = args.warmup_steps,\n",
    "      num_training_steps = t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    self.p_encoder.zero_grad()\n",
    "    self.q_encoder.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_iterator = tqdm(range(int(args.num_train_epochs)), desc='Epoch')\n",
    "\n",
    "    for _ in train_iterator:\n",
    "      valid_check_period = 100\n",
    "      count_iteration = 0\n",
    "      total_loss = 0\n",
    "      print('train_iterator')\n",
    "\n",
    "      with tqdm(self.train_dataloader, unit='batch') as tepoch:\n",
    "        print('with tqdm')\n",
    "        for i, batch in enumerate(tepoch):\n",
    "          # print('batch')\n",
    "          self.p_encoder.train()\n",
    "          self.q_encoder.train()\n",
    "\n",
    "          # targets = torch.zeros(batch_size).long()\n",
    "          targets = torch.tensor(self.train_answer_index[i*batch_size : i*batch_size+batch_size])\n",
    "          targets = targets.to(args.device)\n",
    "          \n",
    "\n",
    "          p_inputs = {\n",
    "            \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "            \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "            \"token_type_ids\": batch[2].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "          }\n",
    "\n",
    "          q_inputs = {\n",
    "            \"input_ids\": batch[3].to(args.device),\n",
    "            \"attention_mask\": batch[4].to(args.device),\n",
    "            \"token_type_ids\" : batch[5].to(args.device),\n",
    "          }\n",
    "\n",
    "          p_outputs = self.p_encoder(**p_inputs)\n",
    "          q_outputs = self.q_encoder(**q_inputs)\n",
    "          \n",
    "          # print(p_outputs.shape, q_outputs.shape)\n",
    "\n",
    "          # 기존의것 p_outputs = p_outputs.view(batch_size, -1, self.num_neg+1)\n",
    "          # 내가시도 q_outputs = torch.transpose(q_outputs.view(batch_size, -1, 1),1,2)\n",
    "            \n",
    "          # 틀림 q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "        \n",
    "          p_outputs = p_outputs.view(batch_size, self.num_neg + 1, -1).transpose(1, 2)\n",
    "          q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "          \n",
    "          # print(p_outputs.shape, q_outputs.shape)\n",
    "\n",
    "          sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()\n",
    "          sim_scores = sim_scores.view(batch_size, -1)\n",
    "          sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "          loss = F.nll_loss(sim_scores, targets)\n",
    "          tepoch.set_postfix(loss=f'{str(loss.item())}')\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "          self.p_encoder.zero_grad()\n",
    "          self.q_encoder.zero_grad()\n",
    "\n",
    "          global_step += 1\n",
    "\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "          del p_inputs, q_inputs\n",
    "\n",
    "          # print('loss:', loss)\n",
    "          total_loss += loss\n",
    "          count_iteration += 1\n",
    "          # print(count_iteration)\n",
    "\n",
    "          if count_iteration == valid_check_period:\n",
    "            count_iteration = 0\n",
    "            print('total loss: ', total_loss)\n",
    "            total_loss = 0\n",
    "            valid_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "              self.p_encoder.eval()\n",
    "              self.q_encoder.eval()\n",
    "\n",
    "              with tqdm(self.valid_dataloader, unit='batch') as tepoch_val:\n",
    "          \n",
    "                for j, batch_val in enumerate(tepoch_val):\n",
    "\n",
    "                  # targets = torch.zeros(batch_size).long()\n",
    "                  targets = torch.tensor(self.valid_answer_index[j*batch_size : j*batch_size+batch_size])\n",
    "                  targets = targets.to(args.device)\n",
    "                  \n",
    "\n",
    "                  p_inputs = {\n",
    "                    \"input_ids\": batch_val[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                    \"attention_mask\": batch_val[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                    \"token_type_ids\": batch_val[2].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                  }\n",
    "\n",
    "                  q_inputs = {\n",
    "                    \"input_ids\": batch_val[3].to(args.device),\n",
    "                    \"attention_mask\": batch_val[4].to(args.device),\n",
    "                    \"token_type_ids\": batch_val[5].to(args.device),\n",
    "                  }\n",
    "\n",
    "                  p_outputs = self.p_encoder(**p_inputs)\n",
    "                  q_outputs = self.q_encoder(**q_inputs)\n",
    "                  # print('val: ',p_outputs.shape, q_outputs.shape)\n",
    "                \n",
    "                  # p_outputs = p_outputs.view(batch_size, -1, self.num_neg+1)\n",
    "                  # q_outputs = torch.transpose(q_outputs.view(batch_size, -1, 1),1,2)\n",
    "                  # q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "                  \n",
    "                \n",
    "                  p_outputs = p_outputs.view(batch_size, self.num_neg + 1, -1).transpose(1, 2)\n",
    "                  q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "                  # print('val: ',p_outputs.shape, q_outputs.shape)\n",
    "                  \n",
    "                  sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()\n",
    "                  sim_scores = sim_scores.view(batch_size, -1)\n",
    "                  sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                  loss = F.nll_loss(sim_scores, targets)\n",
    "                  tepoch_val.set_postfix(loss=f'{str(loss.item())}')\n",
    "\n",
    "                  torch.cuda.empty_cache()\n",
    "\n",
    "                  del p_inputs, q_inputs\n",
    "\n",
    "                  valid_loss += loss\n",
    "              \n",
    "              print('valid_loss: ', valid_loss)\n",
    "\n",
    "\n",
    "\n",
    "  def get_relevant_doc(self, query, k=1, args=None, p_encoder=None, q_encoder=None):\n",
    "    if args is None:\n",
    "      args = self.args\n",
    "    \n",
    "    if p_encoder is None:\n",
    "      p_encoder = self.p_encoder\n",
    "    \n",
    "    if q_encoder is None:\n",
    "      q_encoder = self.q_encoder\n",
    "\n",
    "    with torch.no_grad():\n",
    "      p_encoder.eval()\n",
    "      q_encoder.eval()\n",
    "\n",
    "      q_seqs_val = self.tokenizer(\n",
    "        [query],\n",
    "        padding = 'max_length',\n",
    "        truncation= True,\n",
    "        return_tensors = 'pt',\n",
    "        # return_token_type_ids=False,\n",
    "      ).to(args.device)\n",
    "      q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "\n",
    "      p_embs = []\n",
    "      for batch in self.passage_dataloader:\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        p_inputs = {\n",
    "          'input_ids': batch[0],\n",
    "          'attention_mask': batch[1],\n",
    "          'token_type_ids': batch[2]\n",
    "        }\n",
    "        p_emb = p_encoder(**p_inputs).to('cpu')\n",
    "        p_embs.append(p_emb)\n",
    "    \n",
    "    p_embs = torch.stack(\n",
    "      p_embs, dim = 0\n",
    "    ).view(len(self.passage_dataloader.dataset), -1)\n",
    "\n",
    "    dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "    rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "    return rank[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(RobertaPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.roberta = RobertaModel(config)\n",
    "    self.init_weights()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask=None):\n",
    "    outputs = self.roberta(\n",
    "      input_ids,\n",
    "      attention_mask = attention_mask\n",
    "    )\n",
    "\n",
    "    pooled_output = outputs[1]\n",
    "    return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertPreTrainedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5297/724379711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertPreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "    outputs = self.bert(\n",
    "      input_ids,\n",
    "      attention_mask = attention_mask,\n",
    "      token_type_ids = token_type_ids\n",
    "    )\n",
    "\n",
    "    pooled_output = outputs[1]\n",
    "    return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5297/3572012176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/train_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/train_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# num_sample = 1500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# sample_idx = np.random.choice(range(len(train_dataset)),100)# len(train_dataset))#num_sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_disk' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = load_from_disk('../../data/train_dataset')['train']\n",
    "valid_dataset = load_from_disk('../../data/train_dataset')['validation']\n",
    "\n",
    "# num_sample = 1500\n",
    "# sample_idx = np.random.choice(range(len(train_dataset)),100)# len(train_dataset))#num_sample)\n",
    "# sample_idx = np.random.choice(range(len(train_dataset)), len(train_dataset))#num_sample)\n",
    "# train_dataset = train_dataset[sample_idx]\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = 'dense_retrieval',\n",
    "  evaluation_strategy = 'epoch',\n",
    "  learning_rate=1e-5,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=4,\n",
    "  num_train_epochs=3,\n",
    "  weight_decay=0.01\n",
    ")\n",
    "\n",
    "# model_checkpoint = 'klue/roberta-small'\n",
    "model_checkpoint = 'klue/bert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# p_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "# q_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = DenseRetrieval(\n",
    "  args=args,\n",
    "  train_dataset=train_dataset,\n",
    "  valid_dataset=valid_dataset,\n",
    "  # num_neg=12,\n",
    "  num_neg=5,\n",
    "  tokenizer=tokenizer,\n",
    "  p_encoder=p_encoder,\n",
    "  q_encoder=q_encoder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5745256954584d0a81126321f31bf664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iterator\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143d8cee8a9148199619afa0ab3d42b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with tqdm\n",
      "total loss:  tensor(385.2856, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925ec67fc86340dfb81bedb99221ecec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(113.4799, device='cuda:0')\n",
      "total loss:  tensor(209.5696, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5051724ccb437099e7c6d570bf714e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(110.2308, device='cuda:0')\n",
      "total loss:  tensor(196.6562, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df1ee41aa464eca988e0fdc791c419c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(109.4243, device='cuda:0')\n",
      "total loss:  tensor(194.7516, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3956dd925803413294d8e7a029741486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.0769, device='cuda:0')\n",
      "total loss:  tensor(192.7015, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd487cb99c1434fb9942d9aa2217642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(109.1514, device='cuda:0')\n",
      "total loss:  tensor(186.5000, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72004c253dac4ad0bda50a444ee921f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.3030, device='cuda:0')\n",
      "total loss:  tensor(188.7613, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8617a5fa28c4c1486c890b5c608ddf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.6411, device='cuda:0')\n",
      "total loss:  tensor(187.3733, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2f605e76ec4beb984075909c71662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.8266, device='cuda:0')\n",
      "total loss:  tensor(182.2780, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc3877ae9b8436aaa6f91e58adf2502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.8745, device='cuda:0')\n",
      "train_iterator\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d78f610ed40492fa0de04073f231c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with tqdm\n",
      "total loss:  tensor(182.4828, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0087ebea7bbe4977911934d2d6c89b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.6109, device='cuda:0')\n",
      "total loss:  tensor(182.6456, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff453213e12b4e07a1e6fc38b4e17149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss:  tensor(107.6735, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"해바라기는 무슨꽃일까?\"\n",
    "# query = '대한민국의 대통령은 누구인가?'\n",
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[Search Query] {query}\\n\")\n",
    "\n",
    "indices = results.tolist()\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"Top-{i + 1}th Passage (Index {idx})\")\n",
    "    pprint(retriever.dataset[\"context\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(retriever.train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "print(\"sim_scores1:  tensor([[ 2.0670, -1.8882, -0.4755],\\n        [ 1.1081, -0.2872, -0.5352],\\n        [ 1.2624, -0.2268, -1.5149],\\n        [ 2.0739, -1.5962, -0.9872]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 2.0670, -1.8882, -0.4755],\\n        [ 1.1081, -0.2872, -0.5352],\\n        [ 1.2624, -0.2268, -1.5149],\\n        [ 2.0739, -1.5962, -0.9872]], device='cuda:0', grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[-0.0933, -4.0485, -2.6358],\\n        [-0.3654, -1.7607, -2.0087],\\n        [-0.2529, -1.7421, -3.0301],\\n        [-0.0698, -3.7400, -3.1309]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 18.9575, -13.0442,  -6.3324],\\n        [ 18.9219, -13.7115,  -7.6625],\\n        [ 18.6899, -14.1833,  -6.6999],\\n        [ 19.7926, -14.9210,  -6.0559]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 18.9575, -13.0442,  -6.3324],\\n        [ 18.9219, -13.7115,  -7.6625],\\n        [ 18.6899, -14.1833,  -6.6999],\\n        [ 19.7926, -14.9210,  -6.0559]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -32.0017, -25.2900],\\n        [  0.0000, -32.6333, -26.5844],\\n        [  0.0000, -32.8732, -25.3898],\\n        [  0.0000, -34.7136, -25.8485]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.7016, -15.9050,  -7.9630],\\n        [ 18.6390, -16.8143,  -6.2301],\\n        [ 18.5716, -15.4357,  -5.1185],\\n        [ 18.6988, -16.4790,  -6.3037]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.7016, -15.9050,  -7.9630],\\n        [ 18.6390, -16.8143,  -6.2301],\\n        [ 18.5716, -15.4357,  -5.1185],\\n        [ 18.6988, -16.4790,  -6.3037]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -35.6066, -27.6647],\\n        [  0.0000, -35.4534, -24.8691],\\n        [  0.0000, -34.0072, -23.6900],\\n        [  0.0000, -35.1778, -25.0025]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.3020, -16.3367,  -6.9249],\\n        [ 18.0863, -15.6936,  -6.4714],\\n        [ 20.8831, -15.9750,  -6.4057],\\n        [ 17.5353, -15.9932,  -5.3757]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.3020, -16.3367,  -6.9249],\\n        [ 18.0863, -15.6936,  -6.4714],\\n        [ 20.8831, -15.9750,  -6.4057],\\n        [ 17.5353, -15.9932,  -5.3757]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -35.6387, -26.2269],\\n        [  0.0000, -33.7799, -24.5577],\\n        [  0.0000, -36.8581, -27.2888],\\n        [  0.0000, -33.5285, -22.9110]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.4612, -18.5311,  -7.0692],\\n        [ 17.4514, -16.1776,  -5.9871],\\n        [ 17.8875, -18.1035,  -5.6846],\\n        [ 19.6938, -18.1141,  -7.2004]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.4612, -18.5311,  -7.0692],\\n        [ 17.4514, -16.1776,  -5.9871],\\n        [ 17.8875, -18.1035,  -5.6846],\\n        [ 19.6938, -18.1141,  -7.2004]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -37.9922, -26.5303],\\n        [  0.0000, -33.6291, -23.4386],\\n        [  0.0000, -35.9910, -23.5721],\\n        [  0.0000, -37.8079, -26.8942]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.0263, -17.2546,  -7.3807],\\n        [ 19.7196, -16.0824,  -5.2096],\\n        [ 18.5708, -17.6870,  -6.0913],\\n        [ 16.7673, -17.2788,  -5.5635]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.0263, -17.2546,  -7.3807],\\n        [ 19.7196, -16.0824,  -5.2096],\\n        [ 18.5708, -17.6870,  -6.0913],\\n        [ 16.7673, -17.2788,  -5.5635]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -38.2809, -28.4070],\\n        [  0.0000, -35.8020, -24.9292],\\n        [  0.0000, -36.2579, -24.6622],\\n        [  0.0000, -34.0461, -22.3308]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 18.3619, -17.5218,  -6.9513],\\n        [ 17.7334, -18.8895,  -6.5805],\\n        [ 19.0399, -18.0639,  -6.1909],\\n        [ 16.6032, -16.8530,  -7.2458]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 18.3619, -17.5218,  -6.9513],\\n        [ 17.7334, -18.8895,  -6.5805],\\n        [ 19.0399, -18.0639,  -6.1909],\\n        [ 16.6032, -16.8530,  -7.2458]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -35.8837, -25.3132],\\n        [  0.0000, -36.6229, -24.3139],\\n        [  0.0000, -37.1038, -25.2308],\\n        [  0.0000, -33.4562, -23.8490]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.6403, -19.4150,  -8.1556],\\n        [ 20.7628, -20.4260,  -6.9605],\\n        [ 20.4725, -19.5822,  -6.1974],\\n        [ 20.1756, -19.9032,  -7.3601]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.6403, -19.4150,  -8.1556],\\n        [ 20.7628, -20.4260,  -6.9605],\\n        [ 20.4725, -19.5822,  -6.1974],\\n        [ 20.1756, -19.9032,  -7.3601]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -39.0552, -27.7959],\\n        [  0.0000, -41.1888, -27.7232],\\n        [  0.0000, -40.0548, -26.6699],\\n        [  0.0000, -40.0787, -27.5357]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.6039, -20.3562,  -7.5317],\\n        [ 18.2517, -18.4895,  -7.2396],\\n        [ 19.2696, -18.7270,  -5.3625],\\n        [ 18.6063, -20.9600,  -7.1433]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.6039, -20.3562,  -7.5317],\\n        [ 18.2517, -18.4895,  -7.2396],\\n        [ 19.2696, -18.7270,  -5.3625],\\n        [ 18.6063, -20.9600,  -7.1433]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -40.9601, -28.1357],\\n        [  0.0000, -36.7412, -25.4912],\\n        [  0.0000, -37.9966, -24.6322],\\n        [  0.0000, -39.5663, -25.7496]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.1420, -18.6040,  -6.0182],\\n        [ 19.4384, -20.3461,  -7.5667],\\n        [ 19.9583, -20.1868,  -5.9409],\\n        [ 21.3782, -20.0411,  -7.1013]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.1420, -18.6040,  -6.0182],\\n        [ 19.4384, -20.3461,  -7.5667],\\n        [ 19.9583, -20.1868,  -5.9409],\\n        [ 21.3782, -20.0411,  -7.1013]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -37.7459, -25.1601],\\n        [  0.0000, -39.7845, -27.0051],\\n        [  0.0000, -40.1451, -25.8991],\\n        [  0.0000, -41.4194, -28.4796]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.6004, -20.2478,  -7.4371],\\n        [ 21.7364, -21.1459,  -8.1965],\\n        [ 19.3092, -19.3821,  -6.4901],\\n        [ 21.4933, -19.5955,  -7.5651]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.6004, -20.2478,  -7.4371],\\n        [ 21.7364, -21.1459,  -8.1965],\\n        [ 19.3092, -19.3821,  -6.4901],\\n        [ 21.4933, -19.5955,  -7.5651]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -39.8482, -27.0375],\\n        [  0.0000, -42.8823, -29.9330],\\n        [  0.0000, -38.6913, -25.7992],\\n        [  0.0000, -41.0887, -29.0584]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.7627, -18.8483,  -7.5668],\\n        [ 20.6127, -20.5880,  -7.3777],\\n        [ 17.6649, -19.9797,  -6.5389],\\n        [ 20.9623, -21.4547,  -7.2272]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.7627, -18.8483,  -7.5668],\\n        [ 20.6127, -20.5880,  -7.3777],\\n        [ 17.6649, -19.9797,  -6.5389],\\n        [ 20.9623, -21.4547,  -7.2272]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -39.6110, -28.3295],\\n        [  0.0000, -41.2007, -27.9905],\\n        [  0.0000, -37.6446, -24.2039],\\n        [  0.0000, -42.4171, -28.1895]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.2274, -21.3646,  -7.2595],\\n        [ 20.4602, -19.8530,  -7.7108],\\n        [ 21.4854, -20.9368,  -7.7451],\\n        [ 21.9013, -20.5813,  -7.5967]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.2274, -21.3646,  -7.2595],\\n        [ 20.4602, -19.8530,  -7.7108],\\n        [ 21.4854, -20.9368,  -7.7451],\\n        [ 21.9013, -20.5813,  -7.5967]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.5920, -28.4869],\\n        [  0.0000, -40.3132, -28.1710],\\n        [  0.0000, -42.4222, -29.2305],\\n        [  0.0000, -42.4826, -29.4980]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.1489, -22.7956,  -6.4710],\\n        [ 20.8912, -21.5647,  -7.8131],\\n        [ 21.0385, -21.7423,  -6.8502],\\n        [ 20.4188, -21.9876,  -7.7137]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.1489, -22.7956,  -6.4710],\\n        [ 20.8912, -21.5647,  -7.8131],\\n        [ 21.0385, -21.7423,  -6.8502],\\n        [ 20.4188, -21.9876,  -7.7137]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.9445, -26.6199],\\n        [  0.0000, -42.4559, -28.7044],\\n        [  0.0000, -42.7808, -27.8887],\\n        [  0.0000, -42.4063, -28.1324]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.3652, -20.7907,  -7.3486],\\n        [ 20.2258, -21.4324,  -8.1898],\\n        [ 22.0525, -22.6062,  -8.8879],\\n        [ 19.7080, -20.6045,  -6.6445]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.3652, -20.7907,  -7.3486],\\n        [ 20.2258, -21.4324,  -8.1898],\\n        [ 22.0525, -22.6062,  -8.8879],\\n        [ 19.7080, -20.6045,  -6.6445]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -41.1559, -27.7138],\\n        [  0.0000, -41.6581, -28.4155],\\n        [  0.0000, -44.6587, -30.9405],\\n        [  0.0000, -40.3125, -26.3525]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 22.4861, -23.1768,  -8.7276],\\n        [ 20.8697, -19.7706,  -7.4301],\\n        [ 22.5807, -20.7921,  -7.6226],\\n        [ 20.5334, -20.4921,  -8.2179]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 22.4861, -23.1768,  -8.7276],\\n        [ 20.8697, -19.7706,  -7.4301],\\n        [ 22.5807, -20.7921,  -7.6226],\\n        [ 20.5334, -20.4921,  -8.2179]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -45.6629, -31.2138],\\n        [  0.0000, -40.6404, -28.2998],\\n        [  0.0000, -43.3728, -30.2033],\\n        [  0.0000, -41.0255, -28.7513]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 19.9769, -22.5745,  -8.8266],\\n        [ 21.8273, -20.9343,  -7.7861],\\n        [ 20.9987, -21.2951,  -8.9782],\\n        [ 20.3307, -21.7390,  -9.4534]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 19.9769, -22.5745,  -8.8266],\\n        [ 21.8273, -20.9343,  -7.7861],\\n        [ 20.9987, -21.2951,  -8.9782],\\n        [ 20.3307, -21.7390,  -9.4534]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.5514, -28.8035],\\n        [  0.0000, -42.7615, -29.6134],\\n        [  0.0000, -42.2938, -29.9769],\\n        [  0.0000, -42.0697, -29.7841]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 22.1752, -21.7726,  -8.8740],\\n        [ 21.3944, -20.5539,  -8.4125],\\n        [ 21.2713, -22.4065,  -8.4852],\\n        [ 21.8581, -20.9206,  -8.7656]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 22.1752, -21.7726,  -8.8740],\\n        [ 21.3944, -20.5539,  -8.4125],\\n        [ 21.2713, -22.4065,  -8.4852],\\n        [ 21.8581, -20.9206,  -8.7656]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.9478, -31.0493],\\n        [  0.0000, -41.9482, -29.8069],\\n        [  0.0000, -43.6779, -29.7565],\\n        [  0.0000, -42.7787, -30.6237]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 20.8886, -22.0446,  -8.5062],\\n        [ 22.2826, -22.1557,  -7.3194],\\n        [ 21.3736, -22.4655,  -7.9215],\\n        [ 22.0496, -22.2193,  -8.4336]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 20.8886, -22.0446,  -8.5062],\\n        [ 22.2826, -22.1557,  -7.3194],\\n        [ 21.3736, -22.4655,  -7.9215],\\n        [ 22.0496, -22.2193,  -8.4336]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -42.9332, -29.3947],\\n        [  0.0000, -44.4383, -29.6019],\\n        [  0.0000, -43.8391, -29.2951],\\n        [  0.0000, -44.2689, -30.4832]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.3588, -22.2370,  -6.8472],\\n        [ 19.6908, -21.9729,  -6.7540],\\n        [ 22.0681, -22.8626,  -8.4110],\\n        [ 23.0115, -23.2496,  -8.9440]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.3588, -22.2370,  -6.8472],\\n        [ 19.6908, -21.9729,  -6.7540],\\n        [ 22.0681, -22.8626,  -8.4110],\\n        [ 23.0115, -23.2496,  -8.9440]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.5958, -28.2060],\\n        [  0.0000, -41.6637, -26.4449],\\n        [  0.0000, -44.9307, -30.4791],\\n        [  0.0000, -46.2611, -31.9556]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 23.3233, -21.0999,  -8.8561],\\n        [ 20.8003, -21.8411,  -7.2726],\\n        [ 20.2321, -21.5142,  -6.7270],\\n        [ 21.0954, -24.0800,  -8.5347]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 23.3233, -21.0999,  -8.8561],\\n        [ 20.8003, -21.8411,  -7.2726],\\n        [ 20.2321, -21.5142,  -6.7270],\\n        [ 21.0954, -24.0800,  -8.5347]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -44.4232, -32.1794],\\n        [  0.0000, -42.6414, -28.0729],\\n        [  0.0000, -41.7463, -26.9591],\\n        [  0.0000, -45.1754, -29.6301]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.5006, -22.2052,  -8.2979],\\n        [ 21.1089, -22.8062,  -7.6997],\\n        [ 20.6536, -21.2667,  -8.6762],\\n        [ 21.8066, -23.0844,  -8.1231]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.5006, -22.2052,  -8.2979],\\n        [ 21.1089, -22.8062,  -7.6997],\\n        [ 20.6536, -21.2667,  -8.6762],\\n        [ 21.8066, -23.0844,  -8.1231]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.7057, -29.7985],\\n        [  0.0000, -43.9151, -28.8086],\\n        [  0.0000, -41.9203, -29.3298],\\n        [  0.0000, -44.8910, -29.9297]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 18.4972, -22.1835,  -6.9772],\\n        [ 21.6914, -21.1846,  -9.2879],\\n        [ 20.9040, -22.9383,  -8.7836],\\n        [ 21.8061, -23.1066,  -8.4577]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 18.4972, -22.1835,  -6.9772],\\n        [ 21.6914, -21.1846,  -9.2879],\\n        [ 20.9040, -22.9383,  -8.7836],\\n        [ 21.8061, -23.1066,  -8.4577]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -40.6807, -25.4745],\\n        [  0.0000, -42.8761, -30.9793],\\n        [  0.0000, -43.8423, -29.6877],\\n        [  0.0000, -44.9127, -30.2638]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 21.5885, -22.1243,  -7.6789],\\n        [ 21.6998, -22.9157,  -7.4915],\\n        [ 21.5375, -20.8942,  -7.2593],\\n        [ 22.6453, -23.8574,  -7.7890]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 21.5885, -22.1243,  -7.6789],\\n        [ 21.6998, -22.9157,  -7.4915],\\n        [ 21.5375, -20.8942,  -7.2593],\\n        [ 22.6453, -23.8574,  -7.7890]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.7128, -29.2674],\\n        [  0.0000, -44.6155, -29.1913],\\n        [  0.0000, -42.4317, -28.7968],\\n        [  0.0000, -46.5027, -30.4342]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nsim_scores1:  tensor([[ 22.3031, -21.2729,  -8.4275],\\n        [ 22.1473, -21.8015,  -9.0392],\\n        [ 20.1393, -20.6704,  -8.0232],\\n        [ 18.1470, -19.8829,  -7.3621]], device='cuda:0',\\n       grad_fn=<SqueezeBackward0>)\\nsim_scores2:  tensor([[ 22.3031, -21.2729,  -8.4275],\\n        [ 22.1473, -21.8015,  -9.0392],\\n        [ 20.1393, -20.6704,  -8.0232],\\n        [ 18.1470, -19.8829,  -7.3621]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nsim_scores3:  tensor([[  0.0000, -43.5760, -30.7306],\\n        [  0.0000, -43.9488, -31.1865],\\n        [  0.0000, -40.8096, -28.1624],\\n        [  0.0000, -38.0299, -25.5091]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\nloss: tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\\nsim_scores: tensor([[  0.0000, -43.5760, -30.7306],\\n        [  0.0000, -43.9488, -31.1865],\\n        [  0.0000, -40.8096, -28.1624],\\n        [  0.0000, -38.0299, -25.5091]], device='cuda:0',\\n       grad_fn=<LogSoftmaxBackward>)\\ntargets: tensor([0, 0, 0, 0], device='cuda:0')\\np_outputs: tensor([[[-0.2828,  0.2189,  0.3590],\\n         [-0.4554, -0.1614,  0.1250],\\n         [ 0.0780,  0.0511, -0.1944],\\n         ...,\\n         [ 0.4404,  0.2783,  0.3426],\\n         [-0.3670,  0.0518, -0.1551],\\n         [ 0.6381, -0.3617, -0.2321]],\\n\\n        [[-0.2635,  0.3162,  0.3221],\\n         [-0.3746, -0.2633,  0.0335],\\n         [ 0.2125,  0.0111, -0.2120],\\n         ...,\\n         [ 0.5239,  0.1418,  0.3787],\\n         [-0.3427,  0.0350, -0.0782],\\n         [ 0.5831, -0.3120, -0.3704]],\\n\\n        [[-0.1935,  0.2067,  0.2202],\\n         [-0.3663, -0.2757,  0.0735],\\n         [ 0.0890,  0.0329, -0.2128],\\n         ...,\\n         [ 0.4545,  0.2334,  0.3461],\\n         [-0.2655,  0.0806, -0.2142],\\n         [ 0.5106, -0.3753, -0.3272]],\\n\\n        [[-0.3044,  0.1907,  0.2350],\\n         [-0.4767, -0.2779,  0.0664],\\n         [-0.1426,  0.2462, -0.2211],\\n         ...,\\n         [ 0.4860,  0.3116,  0.3844],\\n         [-0.4297, -0.1095, -0.1866],\\n         [ 0.5486, -0.2870, -0.4021]]], device='cuda:0',\\n       grad_fn=<ViewBackward>)\\nq_outputs: tensor([[[ 0.1808,  0.1537,  0.0327,  ...,  0.2288, -0.3302,  0.5529]],\\n\\n        [[ 0.0656,  0.2280,  0.0654,  ...,  0.1543, -0.4032,  0.6550]],\\n\\n        [[ 0.0651,  0.2539, -0.0007,  ...,  0.1649, -0.3408,  0.5014]],\\n\\n        [[ 0.1571,  0.2728,  0.1050,  ...,  0.1525, -0.4152,  0.5329]]\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
